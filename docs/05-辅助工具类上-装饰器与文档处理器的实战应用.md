# ç¬¬05ç« ï¼šè¾…åŠ©å·¥å…·ç±»(ä¸Š) - è£…é¥°å™¨ä¸æ–‡æ¡£å¤„ç†å™¨çš„å®æˆ˜åº”ç”¨

æœ¬ç« å®ç°ä¸¤ä¸ªé‡è¦çš„è¾…åŠ©æ¨¡å—ï¼šè£…é¥°å™¨å·¥å…·å’Œæ–‡æ¡£å¤„ç†å™¨ã€‚

## Part 1: è£…é¥°å™¨å·¥å…·ç±»

**æ–‡ä»¶**ï¼š`utils/decorators.py`ï¼ˆ510è¡Œï¼‰

æä¾›3ä¸ªè£…é¥°å™¨ï¼š
- `@error_handler`ï¼šå¼‚å¸¸æ•è·å’Œå¤„ç†
- `@log_execution`ï¼šæ‰§è¡Œæ—¥å¿—è®°å½•
- `@performance_monitor`ï¼šæ€§èƒ½ç›‘æ§

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹å®Œæ•´ä»£ç </summary>

```python
import functools
import logging
import time
import traceback
from typing import Callable, Any, Optional
import streamlit as st
from config.settings import Settings

logger = logging.getLogger(__name__)

def error_handler(
    func_name: str = None,
    show_in_ui: bool = True,
    log_level: str = "ERROR",
    return_on_error: Any = None,
    error_message: str = None
):
    """é”™è¯¯å¤„ç†è£…é¥°å™¨
    
    æ–¹æ³•ç”¨é€”ï¼šä¸ºå‡½æ•°æä¾›ç»Ÿä¸€çš„å¼‚å¸¸æ•è·å’Œå¤„ç†æœºåˆ¶ï¼Œé˜²æ­¢ç¨‹åºå› å¼‚å¸¸è€Œå´©æºƒï¼Œ
    åŒæ—¶æä¾›å‹å¥½çš„é”™è¯¯æç¤ºå’Œæ—¥å¿—è®°å½•åŠŸèƒ½
    
    å‚æ•°è§£é‡Šï¼š
        func_name (str, å¯é€‰): å‡½æ•°æ˜¾ç¤ºåç§°ï¼Œç”¨äºæ—¥å¿—å’ŒUIæ˜¾ç¤ºï¼ŒNoneåˆ™ä½¿ç”¨å®é™…å‡½æ•°å
        show_in_ui (bool, å¯é€‰): æ˜¯å¦åœ¨Streamlitç•Œé¢æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯ï¼Œé»˜è®¤True
        log_level (str, å¯é€‰): æ—¥å¿—çº§åˆ«ï¼Œæ”¯æŒ"ERROR", "WARNING", "INFO", "DEBUG"ï¼Œé»˜è®¤"ERROR"
        return_on_error (Any, å¯é€‰): å‘ç”Ÿé”™è¯¯æ—¶è¿”å›çš„é»˜è®¤å€¼ï¼Œé»˜è®¤None
        error_message (str, å¯é€‰): è‡ªå®šä¹‰é”™è¯¯æ¶ˆæ¯å‰ç¼€ï¼ŒNoneåˆ™ä½¿ç”¨é»˜è®¤æ¶ˆæ¯
        
    è¿”å›å€¼ï¼š
        Callable: è£…é¥°å™¨å‡½æ•°ï¼Œè¿”å›åŒ…è£…åçš„å‡½æ•°
        
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        # åŸºæœ¬ç”¨æ³• - æ•è·å¼‚å¸¸å¹¶è¿”å›None
        >>> @error_handler
        >>> def divide(a, b):
        >>>     return a / b
        >>> 
        >>> result = divide(10, 2)   # æ­£å¸¸æ‰§è¡Œï¼Œè¿”å›5.0
        >>> result = divide(10, 0)   # æ•è·å¼‚å¸¸ï¼Œè¿”å›None
        
        # é«˜çº§ç”¨æ³• - è‡ªå®šä¹‰é”™è¯¯å¤„ç†
        >>> @error_handler(
        >>>     func_name="å®‰å…¨é™¤æ³•å™¨",
        >>>     return_on_error=-1,
        >>>     error_message="é™¤æ³•è®¡ç®—å¤±è´¥",
        >>>     show_in_ui=True,
        >>>     log_level="WARNING"
        >>> )
        >>> def safe_divide(a, b):
        >>>     return a / b
        >>> 
        >>> result = safe_divide(10, 0)  # è¿”å›-1ï¼Œè®°å½•è­¦å‘Šæ—¥å¿—ï¼ŒUIæ˜¾ç¤ºé”™è¯¯
    """
    def decorator(func: Callable) -> Callable:
        """é”™è¯¯å¤„ç†è£…é¥°å™¨çš„å†…éƒ¨è£…é¥°å™¨å‡½æ•°
        
        æ–¹æ³•ç”¨é€”ï¼šæ¥æ”¶è¢«è£…é¥°çš„å‡½æ•°ï¼Œè¿”å›åŒ…è£…åçš„å‡½æ•°ï¼Œåœ¨åŒ…è£…å‡½æ•°ä¸­æ·»åŠ å¼‚å¸¸æ•è·å’Œå¤„ç†é€»è¾‘
        
        å‚æ•°è§£é‡Šï¼š
            func (Callable): è¢«è£…é¥°çš„åŸå§‹å‡½æ•°
            
        è¿”å›å€¼ï¼š
            Callable: åŒ…è£…åçš„å‡½æ•°ï¼Œå…·æœ‰å¼‚å¸¸å¤„ç†åŠŸèƒ½
            
        ä½¿ç”¨ç¤ºä¾‹ï¼š
            >>> decorated_func = decorator(original_func)
            >>> result = decorated_func(*args, **kwargs)  # å®‰å…¨æ‰§è¡Œï¼Œå¼‚å¸¸è¢«æ•è·
        """
        @functools.wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            """é”™è¯¯å¤„ç†åŒ…è£…å‡½æ•°
            
            æ–¹æ³•ç”¨é€”ï¼šåŒ…è£…åŸå§‹å‡½æ•°ï¼Œåœ¨æ‰§è¡Œæ—¶æ•è·å¼‚å¸¸å¹¶è¿›è¡Œå¤„ç†ï¼Œ
            è®°å½•æ—¥å¿—ï¼Œæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯ï¼Œè¿”å›é»˜è®¤å€¼
            
            å‚æ•°è§£é‡Šï¼š
                *args: ä¼ é€’ç»™åŸå‡½æ•°çš„ä½ç½®å‚æ•°
                **kwargs: ä¼ é€’ç»™åŸå‡½æ•°çš„å…³é”®å­—å‚æ•°
                
            è¿”å›å€¼ï¼š
                Any: æ­£å¸¸æ‰§è¡Œæ—¶è¿”å›åŸå‡½æ•°ç»“æœï¼Œå¼‚å¸¸æ—¶è¿”å›return_on_erroræŒ‡å®šçš„é»˜è®¤å€¼
                
            ä½¿ç”¨ç¤ºä¾‹ï¼š
                >>> result = wrapper(10, 5)        # æ­£å¸¸æ‰§è¡Œï¼Œè¿”å›åŸå‡½æ•°ç»“æœ
                >>> result = wrapper(10, 0)        # æ•è·å¼‚å¸¸ï¼Œè¿”å›é»˜è®¤å€¼
                >>> result = wrapper("invalid")  # æ•è·å¼‚å¸¸ï¼Œè¿”å›é»˜è®¤å€¼
                
            å¼‚å¸¸å¤„ç†ï¼š
                æ•è·æ‰€æœ‰ExceptionåŠå…¶å­ç±»ï¼Œè®°å½•æ—¥å¿—ï¼Œæ˜¾ç¤ºUIé”™è¯¯ï¼ˆå¦‚å¯ç”¨ï¼‰ï¼Œ
                è¿”å›é»˜è®¤å€¼ï¼Œä¸ä¼šé‡æ–°æŠ›å‡ºå¼‚å¸¸
            """
            try:
                return func(*args, **kwargs)
            except Exception as e:
                # è·å–å‡½æ•°åç§°
                actual_func_name = func_name or func.__name__

                # æ„å»ºé”™è¯¯ä¿¡æ¯
                error_msg = error_message or f"å‡½æ•° '{actual_func_name}' æ‰§è¡Œå¤±è´¥"
                full_error_msg = f"{error_msg}: {str(e)}"

                # è®°å½•æ—¥å¿—
                log_func = getattr(logger, log_level.lower(), logger.error)
                log_func(full_error_msg)

                # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
                logger.debug(f"é”™è¯¯è¯¦æƒ…:\n{traceback.format_exc()}")

                # åœ¨UIä¸­æ˜¾ç¤ºé”™è¯¯ï¼ˆå¦‚æœä½¿ç”¨Streamlitä¸”å¤„äºStreamlitç¯å¢ƒä¸­ï¼‰
                if show_in_ui and hasattr(st, 'error'):
                    try:
                        # æ£€æŸ¥æ˜¯å¦åœ¨Streamlitç¯å¢ƒä¸­è¿è¡Œ
                        if st._is_running_with_streamlit:
                            st.error(f"âŒ {full_error_msg}")
                            
                            # æ˜¾ç¤ºè¯¦ç»†é”™è¯¯ï¼ˆåœ¨å¼€å‘æ¨¡å¼ä¸‹ï¼‰
                            if logger.level <= logging.DEBUG:
                                with st.expander("ğŸ” æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯"):
                                    st.code(traceback.format_exc())
                    except (AttributeError, RuntimeError):
                        # ä¸åœ¨Streamlitç¯å¢ƒä¸­ï¼Œå¿½ç•¥UIæ˜¾ç¤º
                        pass

                # è¿”å›é”™è¯¯æ—¶çš„é»˜è®¤å€¼
                return return_on_error

        return wrapper
    return decorator

def log_execution(
    func_name: str = None,
    log_level: str = "INFO",
    log_args: bool = False,
    log_result: bool = False,
    log_time: bool = True
):
    """æ‰§è¡Œæ—¥å¿—è£…é¥°å™¨
    
    æ–¹æ³•ç”¨é€”ï¼šä¸ºå‡½æ•°æä¾›è¯¦ç»†çš„æ‰§è¡Œæ—¥å¿—è®°å½•ï¼ŒåŒ…æ‹¬å¼€å§‹æ—¶é—´ã€æ‰§è¡Œæ—¶é—´ã€å‚æ•°å’Œè¿”å›å€¼ï¼Œ
    å¸®åŠ©å¼€å‘è€…è¿½è¸ªå‡½æ•°æ‰§è¡Œè¿‡ç¨‹å’Œè°ƒè¯•é—®é¢˜
    
    å‚æ•°è§£é‡Šï¼š
        func_name (str, å¯é€‰): å‡½æ•°åç§°ï¼Œç”¨äºæ—¥å¿—è®°å½•ï¼ŒNoneåˆ™ä½¿ç”¨å®é™…å‡½æ•°å
        log_level (str, å¯é€‰): æ—¥å¿—çº§åˆ«ï¼Œæ”¯æŒ"INFO", "DEBUG", "WARNING", "ERROR"ï¼Œé»˜è®¤"INFO"
        log_args (bool, å¯é€‰): æ˜¯å¦è®°å½•å‡½æ•°å‚æ•°ï¼Œé»˜è®¤False
        log_result (bool, å¯é€‰): æ˜¯å¦è®°å½•å‡½æ•°è¿”å›å€¼ï¼Œé»˜è®¤False
        log_time (bool, å¯é€‰): æ˜¯å¦è®°å½•æ‰§è¡Œæ—¶é—´ï¼Œé»˜è®¤True
        
    è¿”å›å€¼ï¼š
        Callable: è£…é¥°å™¨å‡½æ•°ï¼Œè¿”å›åŒ…è£…åçš„å‡½æ•°
        
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        # åŸºæœ¬ç”¨æ³• - è®°å½•æ‰§è¡Œæ—¶é—´å’ŒçŠ¶æ€
        >>> @log_execution
        >>> def calculate_sum(a, b):
        >>>     return a + b
        >>> 
        >>> result = calculate_sum(5, 3)  # è®°å½•ï¼šå¼€å§‹æ‰§è¡Œã€æ‰§è¡Œå®Œæˆã€è€—æ—¶
        
        # é«˜çº§ç”¨æ³• - è®°å½•è¯¦ç»†ä¿¡æ¯
        >>> @log_execution(
        >>>     func_name="æ•°æ®å¤„ç†å‡½æ•°",
        >>>     log_args=True,
        >>>     log_result=True,
        >>>     log_level="DEBUG",
        >>>     log_time=True
        >>> )
        >>> def process_data(items):
        >>>     return [item.upper() for item in items]
        >>> 
        >>> # è®°å½•ï¼šå¼€å§‹æ‰§è¡Œã€å‚æ•°ã€è¿”å›å€¼ã€æ‰§è¡Œæ—¶é—´
        >>> result = process_data(['hello', 'world'])
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            """æ‰§è¡Œæ—¥å¿—åŒ…è£…å‡½æ•°
            
            æ–¹æ³•ç”¨é€”ï¼šåŒ…è£…åŸå§‹å‡½æ•°ï¼Œåœ¨æ‰§è¡Œå‰åè®°å½•è¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯ï¼Œ
            åŒ…æ‹¬å¼€å§‹æ‰§è¡Œã€å‚æ•°ã€æ‰§è¡Œæ—¶é—´ã€è¿”å›å€¼ç­‰
            
            å‚æ•°è§£é‡Šï¼š
                *args: ä¼ é€’ç»™åŸå‡½æ•°çš„ä½ç½®å‚æ•°
                **kwargs: ä¼ é€’ç»™åŸå‡½æ•°çš„å…³é”®å­—å‚æ•°
                
            è¿”å›å€¼ï¼š
                Any: åŸå‡½æ•°çš„è¿”å›å€¼ï¼Œä¿æŒåŸå‡½æ•°çš„è¡Œä¸ºä¸å˜
                
            ä½¿ç”¨ç¤ºä¾‹ï¼š
                >>> result = wrapper(10, 20, key="value")  # è®°å½•è¯¦ç»†æ‰§è¡Œæ—¥å¿—å¹¶è¿”å›ç»“æœ
                >>> # æ—¥å¿—è¾“å‡ºï¼šå¼€å§‹æ‰§è¡Œå‡½æ•°ã€å‚æ•°ã€æ‰§è¡Œå®Œæˆã€è¿”å›å€¼ã€è€—æ—¶
                
            å¼‚å¸¸å¤„ç†ï¼š
                å¦‚æœåŸå‡½æ•°æŠ›å‡ºå¼‚å¸¸ï¼Œä¼šè®°å½•å¼‚å¸¸ä¿¡æ¯å¹¶é‡æ–°æŠ›å‡ºï¼Œ
                ä¿æŒå¼‚å¸¸ä¼ æ’­é“¾ä¸å˜ï¼Œä¾¿äºä¸Šå±‚å¤„ç†
            """
            # è·å–å‡½æ•°åç§°
            actual_func_name = func_name or func.__name__

            # è·å–æ—¥å¿—å‡½æ•°
            log_func = getattr(logger, log_level.lower(), logger.info)

            try:
                # è®°å½•å‡½æ•°å¼€å§‹æ‰§è¡Œ
                start_time = time.time()
                log_func(f"å¼€å§‹æ‰§è¡Œå‡½æ•°: {actual_func_name}")

                # è®°å½•å‚æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if log_args:
                    args_str = str(args) if args else ""
                    kwargs_str = str(kwargs) if kwargs else ""
                    log_func(f"å‡½æ•°å‚æ•° - args: {args_str}, kwargs: {kwargs_str}")

                # æ‰§è¡Œå‡½æ•°
                result = func(*args, **kwargs)

                # è®°å½•æ‰§è¡Œæ—¶é—´ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if log_time:
                    execution_time = time.time() - start_time
                    log_func(f"å‡½æ•°æ‰§è¡Œå®Œæˆ: {actual_func_name} (è€—æ—¶: {execution_time:.3f}ç§’)")
                else:
                    log_func(f"å‡½æ•°æ‰§è¡Œå®Œæˆ: {actual_func_name}")

                # è®°å½•è¿”å›å€¼ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if log_result:
                    result_str = str(result) if result is not None else "None"
                    # é™åˆ¶ç»“æœå­—ç¬¦ä¸²é•¿åº¦
                    if len(result_str) > 500:
                        result_str = result_str[:500] + "..."
                    log_func(f"å‡½æ•°è¿”å›å€¼: {result_str}")

                return result

            except Exception as e:
                # è®°å½•å¼‚å¸¸ä¿¡æ¯
                execution_time = time.time() - start_time if log_time else 0
                error_msg = f"å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {actual_func_name}"
                if log_time:
                    error_msg += f" (è€—æ—¶: {execution_time:.3f}ç§’)"
                error_msg += f" - {str(e)}"

                logger.error(error_msg)
                logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")

                # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚å¤„ç†
                raise

        return wrapper
    return decorator

def performance_monitor(
    func_name: str = None,
    warning_threshold: float = 1.0,
    error_threshold: float = 5.0
):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨
    
    æ–¹æ³•ç”¨é€”ï¼šç›‘æ§å‡½æ•°çš„æ‰§è¡Œæ—¶é—´ï¼Œæ ¹æ®è®¾å®šçš„æ€§èƒ½é˜ˆå€¼è®°å½•ä¸åŒçº§åˆ«çš„æ—¥å¿—ï¼Œ
    å¸®åŠ©å¼€å‘è€…åŠæ—¶å‘ç°æ€§èƒ½ç“¶é¢ˆå’Œæ…¢æŸ¥è¯¢é—®é¢˜
    
    å‚æ•°è§£é‡Šï¼š
        func_name (str, å¯é€‰): å‡½æ•°åç§°ï¼Œç”¨äºæ—¥å¿—è®°å½•ï¼ŒNoneåˆ™ä½¿ç”¨å®é™…å‡½æ•°å
        warning_threshold (float, å¯é€‰): è­¦å‘Šé˜ˆå€¼ï¼ˆç§’ï¼‰ï¼Œè¶…è¿‡æ­¤æ—¶é—´è®°å½•è­¦å‘Šæ—¥å¿—ï¼Œé»˜è®¤1.0ç§’
        error_threshold (float, å¯é€‰): é”™è¯¯é˜ˆå€¼ï¼ˆç§’ï¼‰ï¼Œè¶…è¿‡æ­¤æ—¶é—´è®°å½•é”™è¯¯æ—¥å¿—ï¼Œé»˜è®¤5.0ç§’
        
    è¿”å›å€¼ï¼š
        Callable: è£…é¥°å™¨å‡½æ•°ï¼Œè¿”å›åŒ…è£…åçš„å‡½æ•°
        
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        # åŸºæœ¬ç”¨æ³• - ä½¿ç”¨é»˜è®¤é˜ˆå€¼ç›‘æ§
        >>> @performance_monitor
        >>> def slow_function():
        >>>     time.sleep(0.5)
        >>>     return "å®Œæˆ"
        >>> 
        >>> result = slow_function()  # è®°å½•ï¼šæ€§èƒ½æ­£å¸¸ (è€—æ—¶: 0.500ç§’)
        
        # é«˜çº§ç”¨æ³• - è‡ªå®šä¹‰æ€§èƒ½é˜ˆå€¼
        >>> @performance_monitor(
        >>>     func_name="æ•°æ®åº“æŸ¥è¯¢",
        >>>     warning_threshold=0.1,    # 100msè­¦å‘Š
        >>>     error_threshold=0.5       # 500msé”™è¯¯
        >>> )
        >>> def query_database(sql):
        >>>     # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
        >>>     time.sleep(0.2)
        >>>     return f"æŸ¥è¯¢ç»“æœ: {sql}"
        >>> 
        >>> result = query_database("SELECT * FROM users")
        >>> # è®°å½•ï¼šæ€§èƒ½è­¦å‘Š - å‡½æ•°æ‰§è¡Œè¾ƒæ…¢: æ•°æ®åº“æŸ¥è¯¢ (è€—æ—¶: 0.200ç§’)
    """
    def decorator(func: Callable) -> Callable:
            """æ€§èƒ½ç›‘æ§è£…é¥°å™¨çš„å†…éƒ¨è£…é¥°å™¨å‡½æ•°
            
            æ–¹æ³•ç”¨é€”ï¼šæ¥æ”¶è¢«è£…é¥°çš„å‡½æ•°ï¼Œè¿”å›åŒ…è£…åçš„å‡½æ•°ï¼Œåœ¨åŒ…è£…å‡½æ•°ä¸­
            æ·»åŠ æ€§èƒ½ç›‘æ§å’Œæ—¶é—´è®°å½•åŠŸèƒ½
            
            å‚æ•°è§£é‡Šï¼š
                func (Callable): è¢«è£…é¥°çš„åŸå§‹å‡½æ•°
                
            è¿”å›å€¼ï¼š
                Callable: åŒ…è£…åçš„å‡½æ•°ï¼Œå…·æœ‰æ€§èƒ½ç›‘æ§åŠŸèƒ½
                
            ä½¿ç”¨ç¤ºä¾‹ï¼š
                >>> decorated_func = decorator(database_query_func)
                >>> result = decorated_func(sql_query)  # ç›‘æ§æ‰§è¡Œæ€§èƒ½å¹¶è¿”å›ç»“æœ
            """
            @functools.wraps(func)
            def wrapper(*args, **kwargs) -> Any:
                """æ€§èƒ½ç›‘æ§åŒ…è£…å‡½æ•°
                
                æ–¹æ³•ç”¨é€”ï¼šåŒ…è£…åŸå§‹å‡½æ•°ï¼Œåœ¨æ‰§è¡Œå‰åè®°å½•æ€§èƒ½ä¿¡æ¯ï¼Œ
                æ ¹æ®æ‰§è¡Œæ—¶é—´ä¸é˜ˆå€¼çš„æ¯”è¾ƒè®°å½•ä¸åŒçº§åˆ«çš„æ—¥å¿—
                
                å‚æ•°è§£é‡Šï¼š
                    *args: ä¼ é€’ç»™åŸå‡½æ•°çš„ä½ç½®å‚æ•°
                    **kwargs: ä¼ é€’ç»™åŸå‡½æ•°çš„å…³é”®å­—å‚æ•°
                    
                è¿”å›å€¼ï¼š
                    Any: åŸå‡½æ•°çš„è¿”å›å€¼ï¼Œä¿æŒåŸå‡½æ•°çš„è¡Œä¸ºä¸å˜
                    
                ä½¿ç”¨ç¤ºä¾‹ï¼š
                    >>> result = wrapper("query", timeout=30)  # ç›‘æ§æ‰§è¡Œæ€§èƒ½å¹¶è¿”å›ç»“æœ
                    >>> # æ—¥å¿—è¾“å‡ºï¼šæ€§èƒ½æ­£å¸¸/è­¦å‘Š/å‘Šè­¦ - å‡½æ•°æ‰§è¡Œå®Œæˆ/è¾ƒæ…¢/è¿‡æ…¢: å‡½æ•°å (è€—æ—¶: x.xxxç§’)
                    
                æ€§èƒ½åˆ†çº§ï¼š
                    - æ­£å¸¸ï¼šæ‰§è¡Œæ—¶é—´ < warning_thresholdï¼Œè®°å½•INFOçº§åˆ«æ—¥å¿—
                    - è­¦å‘Šï¼šwarning_threshold â‰¤ æ‰§è¡Œæ—¶é—´ < error_thresholdï¼Œè®°å½•WARNINGçº§åˆ«æ—¥å¿—  
                    - å‘Šè­¦ï¼šæ‰§è¡Œæ—¶é—´ â‰¥ error_thresholdï¼Œè®°å½•ERRORçº§åˆ«æ—¥å¿—
                    
                å¼‚å¸¸å¤„ç†ï¼š
                    å¦‚æœåŸå‡½æ•°æŠ›å‡ºå¼‚å¸¸ï¼Œä¼šè®°å½•å¼‚å¸¸ä¿¡æ¯å’Œæ‰§è¡Œæ—¶é—´ï¼Œç„¶åé‡æ–°æŠ›å‡ºå¼‚å¸¸
                """
                actual_func_name = func_name or func.__name__
                start_time = time.time()

                try:
                    # æ‰§è¡Œå‡½æ•°
                    result = func(*args, **kwargs)

                    # è®¡ç®—æ‰§è¡Œæ—¶é—´
                    execution_time = time.time() - start_time

                    # æ ¹æ®æ‰§è¡Œæ—¶é—´è®°å½•ä¸åŒçº§åˆ«çš„æ—¥å¿—
                    if execution_time >= error_threshold:
                        logger.error(f"æ€§èƒ½å‘Šè­¦ - å‡½æ•°æ‰§è¡Œè¿‡æ…¢: {actual_func_name} (è€—æ—¶: {execution_time:.3f}ç§’)")
                    elif execution_time >= warning_threshold:
                        logger.warning(f"æ€§èƒ½è­¦å‘Š - å‡½æ•°æ‰§è¡Œè¾ƒæ…¢: {actual_func_name} (è€—æ—¶: {execution_time:.3f}ç§’)")
                    else:
                        logger.info(f"æ€§èƒ½æ­£å¸¸ - å‡½æ•°æ‰§è¡Œå®Œæˆ: {actual_func_name} (è€—æ—¶: {execution_time:.3f}ç§’)")

                    return result

                except Exception as e:
                    execution_time = time.time() - start_time
                    logger.error(f"æ€§èƒ½ç›‘æ§ - å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {actual_func_name} (è€—æ—¶: {execution_time:.3f}ç§’) - {str(e)}")
                    raise

            return wrapper
    return decorator


if __name__ == "__main__":
    """è£…é¥°å™¨æµ‹è¯•ä»£ç 
    
    æµ‹è¯•å†…å®¹ï¼š
        1. @error_handler - å¼‚å¸¸å¤„ç†è£…é¥°å™¨
        2. @log_execution - æ‰§è¡Œæ—¥å¿—è£…é¥°å™¨  
        3. @performance_monitor - æ€§èƒ½ç›‘æ§è£…é¥°å™¨
        4. ç»„åˆè£…é¥°å™¨ä½¿ç”¨
        5. è£…é¥°å™¨å‚æ•°åŒ–ä½¿ç”¨
    
    æµ‹è¯•è¾“å‡ºï¼š
        - æ§åˆ¶å°æ—¥å¿—è¾“å‡º
        - å¼‚å¸¸å¤„ç†æ¼”ç¤º
        - æ€§èƒ½ç›‘æ§ç»“æœ
    """
    
    import time
    # é…ç½®æ—¥å¿—å¤„ç†å™¨ä»¥åœ¨æ§åˆ¶å°æ˜¾ç¤ºæ—¥å¿—
    console_handler = logging.StreamHandler() # åˆ›å»ºä¸€ä¸ª æ§åˆ¶å°æ—¥å¿—å¤„ç†å™¨ï¼Œå°†æ—¥å¿—ä¿¡æ¯è¾“å‡ºåˆ° ç»ˆç«¯/æ§åˆ¶å°
    console_handler.setLevel(logging.DEBUG) # è®¾ç½®å¤„ç†å™¨çš„ æ—¥å¿—çº§åˆ«ä¸ºDEBUGã€‚å‘Šè¯‰å¤„ç†å™¨"æ‰€æœ‰çº§åˆ«ä¸ºDEBUGåŠä»¥ä¸Šçš„æ—¥å¿—éƒ½è¦å¤„ç†"ï¼Œçº§åˆ«é¡ºåº ï¼šDEBUG < INFO < WARNING < ERROR < CRITICAL
    formatter = logging.Formatter('%(levelname)s - %(message)s') # åˆ›å»º æ—¥å¿—æ ¼å¼å™¨ã€‚è¾“å‡ºç¤ºä¾‹ ï¼š DEBUG - å¼€å§‹æ‰§è¡Œå‡½æ•°: æ•°æ®å¤„ç†å‡½æ•°
    console_handler.setFormatter(formatter) # å‘Šè¯‰å¤„ç†å™¨"æŒ‰ç…§æˆ‘å®šä¹‰çš„æ ¼å¼æ¥æ˜¾ç¤ºæ—¥å¿—"
    logger.addHandler(console_handler) # è®©loggerçŸ¥é“"æˆ‘æœ‰ä¸€ä¸ªæ§åˆ¶å°å¤„ç†å™¨ï¼Œå¯ä»¥æŠŠæ—¥å¿—è¾“å‡ºåˆ°å±å¹•"
    logger.setLevel(logging.DEBUG) # è®¾ç½®loggerçš„ å…¨å±€æ—¥å¿—çº§åˆ«ä¸ºDEBUG
    
    # æµ‹è¯• @error_handler è£…é¥°å™¨
    print("=== æµ‹è¯• @error_handler è£…é¥°å™¨ ===")
    
    @error_handler(func_name="é™¤æ³•è®¡ç®—å™¨:divide_numbers", return_on_error=-1)
    def divide_numbers(a, b):
        """é™¤æ³•å‡½æ•° - æµ‹è¯•å¼‚å¸¸å¤„ç†"""
        return a / b
    
    # æ­£å¸¸è°ƒç”¨
    result = divide_numbers(10, 2)
    print(f"10 Ã· 2 = {result}")
    
    # å¼‚å¸¸è°ƒç”¨ï¼ˆä¼šè¢«æ•è·å¹¶è®°å½•ï¼‰
    result = divide_numbers(10, 0)  # é™¤é›¶å¼‚å¸¸è¢«æ•è·
    print(f"10 Ã· 0 = {result}")
    
    print()
    
    # æµ‹è¯• @log_execution è£…é¥°å™¨
    print("=== æµ‹è¯• @log_execution è£…é¥°å™¨ ===")
    
    @log_execution(
        func_name="æ•°æ®å¤„ç†å‡½æ•°",
        log_args=True,
        log_result=True,
        log_level="DEBUG",
        log_time=True
    )
    def process_data(items):
        """æ•°æ®å¤„ç†å‡½æ•° - æµ‹è¯•æ‰§è¡Œæ—¥å¿—"""
        return [item.upper() for item in items]
    
    result = process_data(['hello', 'world', 'python'])
    print(f"å¤„ç†ç»“æœ: {result}")
    
    print()
    
    # æµ‹è¯• @performance_monitor è£…é¥°å™¨
    print("=== æµ‹è¯• @performance_monitor è£…é¥°å™¨ ===")
    
    @performance_monitor(
        func_name="æ…¢é€Ÿå‡½æ•°",
        warning_threshold=0.1,  # 100ms è­¦å‘Š
        error_threshold=0.3     # 300ms é”™è¯¯
    )
    def slow_function(delay):
        """æ…¢é€Ÿå‡½æ•° - æµ‹è¯•æ€§èƒ½ç›‘æ§"""
        time.sleep(delay)
        return f"å»¶è¿Ÿäº† {delay} ç§’"
    
    # æ­£å¸¸æ€§èƒ½
    result = slow_function(0.05)  # 50ms - æ­£å¸¸
    print(f"ç»“æœ: {result}")
    
    # è­¦å‘Šæ€§èƒ½
    result = slow_function(0.15)  # 150ms - è­¦å‘Š
    print(f"ç»“æœ: {result}")
    
    # é”™è¯¯æ€§èƒ½
    result = slow_function(0.35)  # 350ms - é”™è¯¯
    print(f"ç»“æœ: {result}")
    
    print()
    
    # æµ‹è¯•ç»„åˆè£…é¥°å™¨
    print("=== æµ‹è¯•ç»„åˆè£…é¥°å™¨ ===")
    
    @error_handler()
    @log_execution(
        func_name="ç»„åˆå‡½æ•°",
        log_args=True,
        log_result=True
    )
    @performance_monitor(
        func_name="ç»„åˆå‡½æ•°",
        warning_threshold=0.1,
        error_threshold=0.5
    )
    def combined_function(x, y):
        """ç»„åˆè£…é¥°å™¨å‡½æ•° - åŒæ—¶å…·æœ‰å¼‚å¸¸å¤„ç†ã€æ—¥å¿—è®°å½•ã€æ€§èƒ½ç›‘æ§"""
        time.sleep(0.05)  # 50ms å»¶è¿Ÿ
        return x * y + 100
    
    result = combined_function(5, 8)
    print(f"ç»„åˆå‡½æ•°ç»“æœ: {result}")
    
    print()
    
    # æµ‹è¯•è£…é¥°å™¨ä¸å¸¦å‚æ•°
    print("=== æµ‹è¯•è£…é¥°å™¨ä¸å¸¦å‚æ•° ===")
    
    @error_handler()
    def simple_error_func():
        """ç®€å•é”™è¯¯å‡½æ•°"""
        raise ValueError("æµ‹è¯•å¼‚å¸¸")
    
    @log_execution()
    def simple_log_func(name):
        """ç®€å•æ—¥å¿—å‡½æ•°"""
        return f"Hello, {name}!"
    
    @performance_monitor()
    def simple_perf_func():
        """ç®€å•æ€§èƒ½å‡½æ•°"""
        time.sleep(0.02)
        return "å¿«é€Ÿå®Œæˆ"
    
    # æµ‹è¯•ç®€å•é”™è¯¯å¤„ç†
    try:
        simple_error_func()
    except Exception as e:
        print(f"æ•è·åˆ°å¼‚å¸¸: {e}")
    
    # æµ‹è¯•ç®€å•æ—¥å¿—
    result = simple_log_func("Python")
    print(f"ç®€å•æ—¥å¿—ç»“æœ: {result}")
    
    # æµ‹è¯•ç®€å•æ€§èƒ½
    result = simple_perf_func()
    print(f"ç®€å•æ€§èƒ½ç»“æœ: {result}")
    
    print("\n=== æ‰€æœ‰æµ‹è¯•å®Œæˆ ===")```

</details>

## Part 2: æ–‡æ¡£å¤„ç†å™¨

**æ–‡ä»¶**ï¼š`utils/document_processor.py`ï¼ˆ565è¡Œï¼‰

æ ¸å¿ƒåŠŸèƒ½ï¼š
- æ”¯æŒPDF/TXT/MD/DOCXå¤šæ ¼å¼
- æ–‡æ¡£ç¼“å­˜æœºåˆ¶
- æ™ºèƒ½åˆ†å—ç­–ç•¥
- æ‰¹é‡å¤„ç†

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹å®Œæ•´ä»£ç </summary>

```python
import os
import hashlib
import logging
from typing import List, Dict, Optional, Any
from pathlib import Path
import streamlit as st
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredWordDocumentLoader
from config.settings import Settings

logger = logging.getLogger(__name__)

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ç±»"""

    def __init__(self):
        self.settings = Settings()
        self.cache_dir = self.settings.DATA_DIR / "document_cache"
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def _get_file_hash(self, file_content: bytes) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        return hashlib.md5(file_content).hexdigest()

    def process_uploaded_file(self, uploaded_file) -> List[Document]:
        """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆæ”¯æŒ UploadedFile å¯¹è±¡å’Œæ–‡ä»¶è·¯å¾„ï¼‰"""
        try:
            # å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥
            if hasattr(uploaded_file, 'size') and hasattr(uploaded_file, 'read'):
                # Streamlit UploadedFile å¯¹è±¡
                if uploaded_file.size > self.settings.MAX_FILE_SIZE:
                    raise ValueError(f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶: {uploaded_file.size} > {self.settings.MAX_FILE_SIZE}")
                file_content = uploaded_file.read()
                file_name = uploaded_file.name
            else:
                # æ–‡ä»¶è·¯å¾„
                file_path = Path(uploaded_file)
                if not file_path.exists():
                    raise ValueError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                file_size = file_path.stat().st_size
                if file_size > self.settings.MAX_FILE_SIZE:
                    raise ValueError(f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶: {file_size} > {self.settings.MAX_FILE_SIZE}")
                with open(file_path, 'rb') as f:
                    file_content = f.read()
                file_name = file_path.name
            
            file_type = Path(file_name).suffix.lower()

            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            if file_type not in self.settings.SUPPORTED_FILE_TYPES:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")

            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
            file_hash = self._get_file_hash(file_content)
            cache_path = self._get_cache_path(file_hash, file_name)

            # å°è¯•ä»ç¼“å­˜åŠ è½½
            cached_documents = self._load_from_cache(cache_path)
            if cached_documents is not None:
                return cached_documents

            # å¤„ç†æ–‡ä»¶
            documents = self._process_file_content(file_content, file_name, file_type)

            # ä¿å­˜åˆ°ç¼“å­˜
            self._save_to_cache(cache_path, documents)

            logger.info(f"å¤„ç†æ–‡ä»¶æˆåŠŸ: {file_name}, æ–‡æ¡£æ•°é‡: {len(documents)}")
            return documents

        except Exception as e:
            logger.error(f"å¤„ç†ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {str(e)}")
            raise

    def _process_file_content(self, file_content: bytes, file_name: str, file_type: str) -> List[Document]:
        """å¤„ç†æ–‡ä»¶å†…å®¹"""
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            temp_dir = self.settings.DATA_DIR / "temp"
            temp_dir.mkdir(parents=True, exist_ok=True)
            temp_path = temp_dir / file_name

            # å†™å…¥ä¸´æ—¶æ–‡ä»¶
            with open(temp_path, 'wb') as f:
                f.write(file_content)

            try:
                # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
                if file_type == '.pdf':
                    documents = self._load_pdf(temp_path)
                elif file_type == '.txt':
                    documents = self._load_text(temp_path)
                elif file_type == '.md':
                    documents = self._load_markdown(temp_path)
                elif file_type == '.docx':
                    documents = self._load_word(temp_path)
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")

                # æ·»åŠ å…ƒæ•°æ®
                for i, doc in enumerate(documents):
                    doc.metadata.update({
                        'source': file_name,
                        'file_type': file_type,
                        'chunk_index': i,
                        'total_chunks': len(documents),
                        'processing_timestamp': str(Path(temp_path).stat().st_mtime)
                    })

                return documents

            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_path.exists():
                    temp_path.unlink()

        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å†…å®¹å¤±è´¥: {str(e)}")
            raise

    def _load_pdf(self, file_path: Path) -> List[Document]:
        """åŠ è½½PDFæ–‡ä»¶"""
        try:
            loader = PyPDFLoader(str(file_path))
            documents = loader.load()

            # æ·»åŠ é¡µç ä¿¡æ¯
            for i, doc in enumerate(documents):
                if 'page' not in doc.metadata:
                    doc.metadata['page'] = i + 1

            logger.info(f"åŠ è½½PDFæˆåŠŸ: {file_path.name}, é¡µæ•°: {len(documents)}")
            return documents

        except Exception as e:
            logger.error(f"åŠ è½½PDFå¤±è´¥: {str(e)}")
            raise
            
    def _load_text(self, file_path: Path) -> List[Document]:
        """åŠ è½½æ–‡æœ¬æ–‡ä»¶"""
        try:
            loader = TextLoader(str(file_path), encoding='utf-8')
            documents = loader.load()
            
            logger.info(f"åŠ è½½æ–‡æœ¬æ–‡ä»¶æˆåŠŸ: {file_path.name}")
            return documents
            
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {str(e)}")
            raise
    
    def _load_markdown(self, file_path: Path) -> List[Document]:
        """åŠ è½½Markdownæ–‡ä»¶"""
        try:
            # Markdownæ–‡ä»¶ä¹Ÿä½¿ç”¨æ–‡æœ¬åŠ è½½å™¨
            loader = TextLoader(str(file_path), encoding='utf-8')
            documents = loader.load()
            
            # æ·»åŠ æ–‡ä»¶ç±»å‹æ ‡è¯†
            for doc in documents:
                doc.metadata['file_type'] = '.md'
            
            logger.info(f"åŠ è½½Markdownæ–‡ä»¶æˆåŠŸ: {file_path.name}")
            return documents
            
        except Exception as e:
            logger.error(f"åŠ è½½Markdownæ–‡ä»¶å¤±è´¥: {str(e)}")
            raise
    
    def _load_word(self, file_path: Path) -> List[Document]:
        """åŠ è½½Wordæ–‡æ¡£"""
        try:
            loader = UnstructuredWordDocumentLoader(str(file_path))
            documents = loader.load()
            
            logger.info(f"åŠ è½½Wordæ–‡æ¡£æˆåŠŸ: {file_path.name}")
            return documents
            
        except Exception as e:
            logger.error(f"åŠ è½½Wordæ–‡æ¡£å¤±è´¥: {str(e)}")
            raise           


    def split_documents(self, documents: List[Document], chunk_size: int = None, chunk_overlap: int = None) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£"""
        try:
            chunk_size = chunk_size or self.settings.CHUNK_SIZE
            chunk_overlap = chunk_overlap or self.settings.CHUNK_OVERLAP
            
            logger.info(f"åˆ†å‰²æ–‡æ¡£ï¼Œchunk_size: {chunk_size}, chunk_overlap: {chunk_overlap}")
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                length_function=len,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
            )
            
            split_docs = text_splitter.split_documents(documents)
            
            # æ›´æ–°å…ƒæ•°æ®
            for i, doc in enumerate(split_docs):
                doc.metadata['chunk_index'] = i
                doc.metadata['chunk_size'] = len(doc.page_content)
                doc.metadata['total_chunks'] = len(split_docs)
            
            logger.info(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œç‰‡æ®µæ•°é‡: {len(split_docs)}")
            return split_docs
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ†å‰²å¤±è´¥: {str(e)}")
            return documents    

    def _get_cache_path(self, file_hash: str, file_name: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{file_hash}_{file_name}.json"
    
    def _load_from_cache(self, cache_path: Path) -> Optional[List[Document]]:
        """ä»ç¼“å­˜åŠ è½½æ–‡æ¡£"""
        try:
            if cache_path.exists() and self.settings.CACHE_ENABLED:
                import json
                with open(cache_path, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                import time
                current_time = time.time()
                cache_time = cache_data.get('timestamp', 0)
                
                if current_time - cache_time < self.settings.CACHE_EXPIRE_TIME:
                    # é‡å»ºDocumentå¯¹è±¡
                    documents = []
                    for doc_data in cache_data.get('documents', []):
                        doc = Document(
                            page_content=doc_data['page_content'],
                            metadata=doc_data['metadata']
                        )
                        documents.append(doc)
                    
                    logger.info(f"ä»ç¼“å­˜åŠ è½½æ–‡æ¡£æˆåŠŸ: {len(documents)} ä¸ªæ–‡æ¡£")
                    return documents
                else:
                    logger.info("ç¼“å­˜å·²è¿‡æœŸ")
                    
        except Exception as e:
            logger.error(f"ä»ç¼“å­˜åŠ è½½å¤±è´¥: {str(e)}")
        
        return None
    
    def _save_to_cache(self, cache_path: Path, documents: List[Document]):
        """ä¿å­˜æ–‡æ¡£åˆ°ç¼“å­˜"""
        try:
            if not self.settings.CACHE_ENABLED:
                return
                
            import json
            import time
            
            cache_data = {
                'timestamp': time.time(),
                'documents': [
                    {
                        'page_content': doc.page_content,
                        'metadata': doc.metadata
                    }
                    for doc in documents
                ]
            }
            
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ä¿å­˜åˆ°ç¼“å­˜æˆåŠŸ: {cache_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ°ç¼“å­˜å¤±è´¥: {str(e)}")
            
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        try:
            cache_files = list(self.cache_dir.glob("*.json"))
            total_size = sum(f.stat().st_size for f in cache_files)
            
            stats = {
                'cache_enabled': self.settings.CACHE_ENABLED,
                'cache_expire_time': self.settings.CACHE_EXPIRE_TIME,
                'cache_files_count': len(cache_files),
                'cache_total_size_bytes': total_size,
                'cache_total_size_mb': round(total_size / (1024 * 1024), 2)
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {'error': str(e)}
    
    def clear_cache(self) -> bool:
        """æ¸…ç©ºç¼“å­˜"""
        try:
            cache_files = list(self.cache_dir.glob("*.json"))
            deleted_count = 0
            
            for cache_file in cache_files:
                try:
                    cache_file.unlink()
                    deleted_count += 1
                except Exception as e:
                    logger.error(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {cache_file}, é”™è¯¯: {str(e)}")
            
            logger.info(f"æ¸…ç©ºç¼“å­˜æˆåŠŸï¼Œåˆ é™¤æ–‡ä»¶æ•°: {deleted_count}")
            return True
            
        except Exception as e:
            logger.error(f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {str(e)}")
            return False                

    def process_documents_batch(self, uploaded_files: List) -> Dict[str, Any]:
        """æ‰¹é‡å¤„ç†æ–‡æ¡£"""
        try:
            results = {
                'total_files': len(uploaded_files),
                'processed_files': 0,
                'failed_files': 0,
                'total_documents': 0,
                'errors': []
            }
            
            all_documents = []
            
            for file in uploaded_files:
                try:
                    documents = self.process_uploaded_file(file)
                    all_documents.extend(documents)
                    results['processed_files'] += 1
                    results['total_documents'] += len(documents)
                    
                    logger.info(f"å¤„ç†æ–‡ä»¶æˆåŠŸ: {file.name}")
                    
                except Exception as e:
                    results['failed_files'] += 1
                    error_info = {
                        'file_name': file.name,
                        'error': str(e)
                    }
                    results['errors'].append(error_info)
                    logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {file.name}, é”™è¯¯: {str(e)}")
            
            results['all_documents'] = all_documents
            return results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†æ–‡æ¡£å¤±è´¥: {str(e)}")
            raise


def test_document_processor():
    """æµ‹è¯•æ–‡æ¡£å¤„ç†å™¨æ ¸å¿ƒåŠŸèƒ½"""
    import logging
    import time
    from pathlib import Path
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    
    print("=" * 60)
    print("ğŸ“š æ–‡æ¡£å¤„ç†å™¨æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–è®¾ç½®å’Œå¤„ç†å™¨
        from config.settings import Settings
        settings = Settings()
        processor = DocumentProcessor()
        
        # æµ‹è¯•æ–‡ä»¶è·¯å¾„
        test_files_dir = Path("files")
        if not test_files_dir.exists():
            print(f"âŒ æµ‹è¯•ç›®å½•ä¸å­˜åœ¨: {test_files_dir}")
            return
        
        # è·å–æµ‹è¯•æ–‡ä»¶
        test_files = list(test_files_dir.glob("*"))
        if not test_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•æ–‡ä»¶")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶:")
        for i, file in enumerate(test_files, 1):
            file_size = file.stat().st_size / 1024
            print(f"   {i}. {file.name} ({file_size:.1f} KB)")
        
        print("\n" + "=" * 40)
        print("ğŸ”§ å¼€å§‹åŠŸèƒ½æµ‹è¯•...")
        print("=" * 40)
        
        # 1. æµ‹è¯•å•ä¸ªæ–‡ä»¶å¤„ç†
        print("\nğŸ“„ 1. æµ‹è¯•å•ä¸ªæ–‡ä»¶å¤„ç†")
        print("-" * 30)
        
        test_file = test_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶
        print(f"å¤„ç†æ–‡ä»¶: {test_file.name}")
        
        try:
            documents = processor.process_uploaded_file(test_file)
            print(f"âœ… æˆåŠŸå¤„ç†ï¼æå–äº† {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            
            if documents:
                # æ˜¾ç¤ºç¬¬ä¸€ä¸ªç‰‡æ®µçš„ä¿¡æ¯
                first_doc = documents[0]
                content_preview = first_doc.page_content[:100] + "..." if len(first_doc.page_content) > 100 else first_doc.page_content
                print(f"   ç¬¬ä¸€ä¸ªç‰‡æ®µé¢„è§ˆ: {content_preview}")
                print(f"   ç‰‡æ®µé•¿åº¦: {len(first_doc.page_content)} å­—ç¬¦")
                print(f"   å…ƒæ•°æ®: {first_doc.metadata}")
                
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
        
        # 2. æµ‹è¯•æ–‡æ¡£åˆ†å‰²
        print("\nâœ‚ï¸ 2. æµ‹è¯•æ–‡æ¡£åˆ†å‰²åŠŸèƒ½")
        print("-" * 30)
        
        if 'documents' in locals() and documents:
            try:
                # æµ‹è¯•ä¸åŒçš„åˆ†å‰²å‚æ•°
                test_params = [
                    (500, 50),
                    (1000, 100)
                ]
                
                for chunk_size, overlap in test_params:
                    split_docs = processor.split_documents(documents, chunk_size, overlap)
                    print(f"   chunk_size={chunk_size}, overlap={overlap}: {len(split_docs)} ä¸ªç‰‡æ®µ")
                    
                    if split_docs:
                        avg_size = sum(len(doc.page_content) for doc in split_docs) / len(split_docs)
                        print(f"   å¹³å‡ç‰‡æ®µå¤§å°: {avg_size:.0f} å­—ç¬¦")
                        
            except Exception as e:
                print(f"âŒ åˆ†å‰²å¤±è´¥: {str(e)}")
                import traceback
                traceback.print_exc()
        
        # 3. æµ‹è¯•æ‰¹é‡å¤„ç†
        print("\nğŸ“¦ 3. æµ‹è¯•æ‰¹é‡å¤„ç†")
        print("-" * 30)
        
        try:
            # æ¨¡æ‹ŸStreamlitçš„UploadedFileå¯¹è±¡
            class MockUploadedFile:
                def __init__(self, file_path):
                    self.name = file_path.name
                    self.type = self._get_file_type(file_path)
                    self.size = file_path.stat().st_size
                    self._file_path = file_path
                    
                def _get_file_type(self, file_path):
                    suffix = file_path.suffix.lower()
                    type_map = {
                        '.pdf': 'application/pdf',
                        '.txt': 'text/plain',
                        '.md': 'text/markdown',
                        '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
                    }
                    return type_map.get(suffix, 'application/octet-stream')
                
                def read(self):
                    return self._file_path.read_bytes()
            
            # åˆ›å»ºæ¨¡æ‹Ÿä¸Šä¼ æ–‡ä»¶åˆ—è¡¨
            mock_files = [MockUploadedFile(f) for f in test_files]
            
            print(f"æ‰¹é‡å¤„ç† {len(mock_files)} ä¸ªæ–‡ä»¶...")
            batch_results = processor.process_documents_batch(mock_files)
            
            print(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆï¼")
            print(f"   æ€»æ–‡ä»¶æ•°: {batch_results['total_files']}")
            print(f"   æˆåŠŸå¤„ç†: {batch_results['processed_files']}")
            print(f"   å¤„ç†å¤±è´¥: {batch_results['failed_files']}")
            print(f"   æ€»æ–‡æ¡£ç‰‡æ®µ: {batch_results['total_documents']}")
            
            # æ˜¾ç¤ºå¤±è´¥æ–‡ä»¶ä¿¡æ¯
            if batch_results['errors']:
                print("\n   å¤±è´¥çš„æ–‡ä»¶:")
                for error in batch_results['errors']:
                    print(f"   - {error['file_name']}: {error['error']}")
                    
        except Exception as e:
            print(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
        
        # 4. æµ‹è¯•ç¼“å­˜åŠŸèƒ½
        print("\nğŸ’¾ 4. æµ‹è¯•ç¼“å­˜åŠŸèƒ½")
        print("-" * 30)
        
        try:
            # è·å–ç¼“å­˜ç»Ÿè®¡
            cache_stats = processor.get_cache_stats()
            print(f"âœ… ç¼“å­˜ç»Ÿè®¡:")
            print(f"   ç¼“å­˜å¯ç”¨: {cache_stats['cache_enabled']}")
            print(f"   ç¼“å­˜æ–‡ä»¶æ•°: {cache_stats['cache_files_count']}")
            print(f"   ç¼“å­˜å¤§å°: {cache_stats['cache_total_size_mb']} MB")
            print(f"   è¿‡æœŸæ—¶é—´: {cache_stats['cache_expire_time']} ç§’")
            
            # æµ‹è¯•å†æ¬¡å¤„ç†ç›¸åŒæ–‡ä»¶ï¼ˆåº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰
            print(f"\n   å†æ¬¡å¤„ç†ç›¸åŒæ–‡ä»¶æµ‹è¯•ç¼“å­˜...")
            start_time = time.time()
            cached_docs = processor.process_uploaded_file(test_files[0])
            cache_time = time.time() - start_time
            
            print(f"   ç¼“å­˜å¤„ç†æ—¶é—´: {cache_time:.3f} ç§’")
            print(f"   ç¼“å­˜æ–‡æ¡£æ•°: {len(cached_docs)}")
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜æµ‹è¯•å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
        
        # 5. æµ‹è¯•æ–‡ä»¶ç±»å‹è¯†åˆ«
        print("\nğŸ” 5. æµ‹è¯•æ–‡ä»¶ç±»å‹å¤„ç†")
        print("-" * 30)
        
        supported_types = ['.pdf', '.txt', '.md', '.docx']
        success_count = 0
        
        for file_path in test_files:
            file_ext = file_path.suffix.lower()
            if file_ext in supported_types:
                print(f"   {file_path.name} ({file_ext}): ", end="")
                try:
                    docs = processor.process_uploaded_file(file_path)
                    print(f"âœ… æˆåŠŸ ({len(docs)} ç‰‡æ®µ)")
                    success_count += 1
                except Exception as e:
                    print(f"âŒ å¤±è´¥ ({str(e)})")
        
        print(f"\n   æˆåŠŸå¤„ç†: {success_count}/{len(test_files)} ä¸ªæ–‡ä»¶")
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
        print("=" * 60)
        
        # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
        if 'batch_results' in locals():
            print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
            print(f"   å¤„ç†æ–‡ä»¶ç±»å‹: {len(set(f.suffix for f in test_files))}")
            print(f"   æ€»æ–‡æ¡£ç‰‡æ®µ: {batch_results['total_documents']}")
            if batch_results['total_files'] > 0:
                success_rate = (batch_results['processed_files']/batch_results['total_files']*100)
                print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_document_processor()```

</details>

## æœ¬ç« æ€»ç»“

âœ… å®ç°äº†å¼‚å¸¸å¤„ç†ã€æ—¥å¿—è®°å½•ã€æ€§èƒ½ç›‘æ§è£…é¥°å™¨
âœ… å®ç°äº†å¤šæ ¼å¼æ–‡æ¡£å¤„ç†å™¨
âœ… æ”¯æŒæ–‡æ¡£ç¼“å­˜å’Œæ‰¹é‡å¤„ç†

**ä¸‹èŠ‚é¢„å‘Š**ï¼šç¬¬06ç« å®ç°å¤©æ°”æœåŠ¡ã€èŠå¤©å†å²å’ŒUIç»„ä»¶ã€‚
