# åŒæ¨¡å¼LLMé›†æˆ - æœ¬åœ°ä¸äº‘ç«¯çš„çµæ´»åˆ‡æ¢

> **æœ¬è®²ç›®æ ‡**ï¼šå®ç°Ollamaæœ¬åœ°æ¨¡å‹ä¸é˜¿é‡Œäº‘ç™¾ç‚¼åœ¨çº¿APIçš„ç»Ÿä¸€æ¥å£å’Œæ— ç¼åˆ‡æ¢

## ä¸€ã€ä¸ºä»€ä¹ˆéœ€è¦åŒæ¨¡å¼æ”¯æŒï¼Ÿ

åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æˆæœ¬ã€æ€§èƒ½ã€éšç§ä¹‹é—´åšæƒè¡¡ï¼š

**åœºæ™¯å¯¹æ¯”**ï¼š

| åœºæ™¯ | æ¨èæ¨¡å¼ | åŸå›  |
|------|---------|------|
| **å¼€å‘è°ƒè¯•** | Ollamaæœ¬åœ° | æ— APIè´¹ç”¨ã€å“åº”å¿«ã€å¯ç¦»çº¿ |
| **ç”Ÿäº§ç¯å¢ƒ** | åœ¨çº¿API | æœåŠ¡ç¨³å®šã€æ¨¡å‹æœ€æ–°ã€æ— éœ€ç»´æŠ¤ |
| **æ•æ„Ÿæ•°æ®** | Ollamaæœ¬åœ° | æ•°æ®ä¸å‡ºæœ¬åœ°ã€ç¬¦åˆåˆè§„è¦æ±‚ |
| **é«˜å¹¶å‘** | åœ¨çº¿API | äº‘ç«¯å¼¹æ€§æ‰©å®¹ã€æ— ç¡¬ä»¶é™åˆ¶ |
| **æ¼”ç¤ºDemo** | Ollamaæœ¬åœ° | ä¸ä¾èµ–ç½‘ç»œã€æˆæœ¬å¯æ§ |

**åŒæ¨¡å¼çš„ä»·å€¼**ï¼š
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å¼€å‘é˜¶æ®µ    â”‚  åˆ‡æ¢   â”‚ ç”Ÿäº§é˜¶æ®µ     â”‚
â”‚ Ollamaæœ¬åœ°  â”‚ â”€â”€â”€â”€â”€â”€> â”‚ é˜¿é‡Œäº‘ç™¾ç‚¼   â”‚
â”‚ å…è´¹æµ‹è¯•    â”‚         â”‚ ç¨³å®šæœåŠ¡     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†‘                        â†“
      â””â”€â”€â”€â”€â”€â”€ ä¸€é”®å›é€€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

åªéœ€ä¿®æ”¹ç¯å¢ƒå˜é‡`LLM_PROVIDER=ollama/online`ï¼Œæ— éœ€æ”¹åŠ¨ä»£ç ï¼

## äºŒã€ç»Ÿä¸€æ¥å£è®¾è®¡æ¶æ„

æˆ‘ä»¬çš„`llm_client.py`ï¼ˆ216è¡Œï¼‰å®ç°äº†ä¸¤å±‚æŠ½è±¡ï¼š

```mermaid
graph TD
    A[åº”ç”¨å±‚] --> B[UnifiedLLMClient ç»Ÿä¸€æ¥å£]
    A --> C[UnifiedEmbeddingClient ç»Ÿä¸€æ¥å£]
    B --> D{LLM_PROVIDER?}
    D -->|ollama| E[Ollama LLM]
    D -->|online| F[ChatOpenAI]
    C --> G{LLM_PROVIDER?}
    G -->|ollama| H[OllamaEmbeddings]
    G -->|online| I[OpenAIEmbeddings]
    E --> J[æœ¬åœ° Ollama æœåŠ¡]
    F --> K[é˜¿é‡Œäº‘ç™¾ç‚¼ API]
    H --> J
    I --> K
```

**å…³é”®è®¾è®¡æ€æƒ³**ï¼š
1. **ç»Ÿä¸€æ¥å£**ï¼šä¸Šå±‚ä»£ç ä¸å…³å¿ƒåº•å±‚æ˜¯Ollamaè¿˜æ˜¯åœ¨çº¿API
2. **é…ç½®é©±åŠ¨**ï¼šé€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ï¼Œæ— éœ€ä¿®æ”¹ä»£ç 
3. **å…¼å®¹LangChain**ï¼šè¿”å›æ ‡å‡†çš„`BaseLLM`å¯¹è±¡ï¼Œå¯ç”¨äºAgent
4. **åŒæ¨¡å‹æ”¯æŒ**ï¼šLLMå’ŒEmbeddingéƒ½æ”¯æŒåŒæ¨¡å¼

## ä¸‰ã€ä»£ç å®ç°è¯¦è§£

æˆ‘ä»¬å°†216è¡Œä»£ç æ‹†åˆ†æˆ5ä¸ªéƒ¨åˆ†è®²è§£ã€‚

### ç¬¬ä¸€éƒ¨åˆ†ï¼šå®¢æˆ·ç«¯åˆå§‹åŒ–é€»è¾‘

**ä»£ç æ–‡ä»¶ï¼š** `study-agentic-rag/03-smart-qa-application/services/llm_client.py`

è¿™éƒ¨åˆ†å®šä¹‰äº†`UnifiedLLMClient`ç±»ï¼Œå®ç°è‡ªåŠ¨é€‰æ‹©LLMæä¾›å•†ã€‚

<details>
<summary>ç‚¹å‡»å±•å¼€ä»£ç </summary>

```python
"""
ç»Ÿä¸€çš„ LLM å®¢æˆ·ç«¯åŒ…è£…ç±»
æ”¯æŒ Ollama æœ¬åœ°æ¨¡å‹å’Œåœ¨çº¿ API (é˜¿é‡Œäº‘ç™¾ç‚¼)
"""

import logging
import os
from typing import List, Optional, Any, Dict
from langchain.llms.base import BaseLLM
from langchain.embeddings.base import Embeddings
try:
    # å°è¯•ä½¿ç”¨æ–°çš„å¯¼å…¥æ–¹å¼
    from langchain_community.llms import Ollama
    from langchain_community.chat_models import ChatOpenAI
except ImportError:
    # å›é€€åˆ°æ—§çš„å¯¼å…¥æ–¹å¼
    from langchain.llms import Ollama
    from langchain.chat_models import ChatOpenAI
from langchain.schema import BaseMessage, HumanMessage, AIMessage, SystemMessage
from config.settings import Settings

# å±è”½LangChainå¼ƒç”¨è­¦å‘Š
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning, module="langchain")
warnings.filterwarnings("ignore", message=".*was deprecated.*LangChain.*")

logger = logging.getLogger(__name__)


class UnifiedLLMClient:
    """ç»Ÿä¸€çš„ LLM å®¢æˆ·ç«¯ï¼Œæ”¯æŒ Ollama å’Œåœ¨çº¿ API"""

    def __init__(
        self,
        model_name: str,
        temperature: float = 0.7,
        max_tokens: int = 2048,
        **kwargs
    ):
        """
        åˆå§‹åŒ– LLM å®¢æˆ·ç«¯

        Args:
            model_name: æ¨¡å‹åç§°
            temperature: æ¸©åº¦ç³»æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
        """
        self.settings = Settings()
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.provider = self.settings.LLM_PROVIDER
        self.llm = self._initialize_llm(**kwargs)

    def _initialize_llm(self, **kwargs) -> BaseLLM:
        """æ ¹æ®é…ç½®åˆå§‹åŒ– LLM"""
        try:
            if self.provider == "ollama":
                return self._initialize_ollama(**kwargs)
            else:  # online
                return self._initialize_online_api(**kwargs)
        except Exception as e:
            logger.error(f"åˆå§‹åŒ– LLM å¤±è´¥: {str(e)}")
            raise
```

</details>

**ä¸ºä»€ä¹ˆè¿™ä¹ˆå†™ï¼Ÿ**

1. **ä¸ºä»€ä¹ˆä½¿ç”¨`**kwargs`ï¼Ÿ**
   ```python
   def __init__(self, model_name: str, temperature: float = 0.7, **kwargs):
   ```
   - ä¸åŒçš„LLMå¯èƒ½æœ‰ä¸åŒçš„å‚æ•°ï¼ˆå¦‚Ollamaçš„`num_ctx`ï¼ŒOpenAIçš„`top_p`ï¼‰
   - `**kwargs`è®©è°ƒç”¨æ–¹å¯ä»¥ä¼ é€’é¢å¤–å‚æ•°
   - ç»Ÿä¸€æ¥å£çš„åŒæ—¶ä¿æŒçµæ´»æ€§

2. **ä¸ºä»€ä¹ˆåœ¨`__init__`ä¸­ç«‹å³åˆå§‹åŒ–LLMï¼Ÿ**
   ```python
   self.provider = self.settings.LLM_PROVIDER
   self.llm = self._initialize_llm(**kwargs)
   ```
   - æå‰éªŒè¯é…ç½®æ˜¯å¦æ­£ç¡®ï¼ˆå¦‚APIå¯†é’¥ã€æ¨¡å‹åç§°ï¼‰
   - åˆå§‹åŒ–å¤±è´¥ä¼šç«‹å³æŠ›å‡ºå¼‚å¸¸ï¼Œè€Œä¸æ˜¯åœ¨é¦–æ¬¡è°ƒç”¨æ—¶
   - éµå¾ª"å¿«é€Ÿå¤±è´¥"åŸåˆ™

3. **ä¸ºä»€ä¹ˆç”¨`if self.provider == "ollama"`åˆ¤æ–­ï¼Ÿ**
   ```python
   if self.provider == "ollama":
       return self._initialize_ollama(**kwargs)
   else:  # online
       return self._initialize_online_api(**kwargs)
   ```
   - ç®€å•æ¸…æ™°ï¼Œåªæœ‰ä¸¤ç§æ¨¡å¼
   - é»˜è®¤èµ°`online`åˆ†æ”¯ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒ
   - å¦‚æœæœªæ¥æ”¯æŒæ›´å¤šæä¾›å•†ï¼Œå¯ä»¥æ”¹ä¸º`if-elif-else`

### ç¬¬äºŒéƒ¨åˆ†ï¼šOllamaæœ¬åœ°æ¨¡å¼åˆå§‹åŒ–

**ä»£ç æ–‡ä»¶ï¼š** `study-agentic-rag/03-smart-qa-application/services/llm_client.py`

è¿™éƒ¨åˆ†åˆå§‹åŒ–Ollamaæœ¬åœ°æ¨¡å‹ã€‚

<details>
<summary>ç‚¹å‡»å±•å¼€ä»£ç </summary>

```python
    def _initialize_ollama(self, **kwargs) -> Ollama:
        """åˆå§‹åŒ– Ollama æœ¬åœ°æ¨¡å‹"""
        logger.info(f"åˆå§‹åŒ– Ollama æ¨¡å‹: {self.model_name}")

        llm = Ollama(
            base_url=self.settings.OLLAMA_BASE_URL,
            model=self.model_name,
            temperature=self.temperature,
            num_predict=self.max_tokens,
            **kwargs
        )

        logger.info("Ollama æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        return llm   
```

</details>

**ä¸ºä»€ä¹ˆè¿™ä¹ˆå†™ï¼Ÿ**

1. **ä¸ºä»€ä¹ˆç”¨`num_predict`è€Œä¸æ˜¯`max_tokens`ï¼Ÿ**
   ```python
   num_predict=self.max_tokens
   ```
   - Ollamaçš„å‚æ•°åæ˜¯`num_predict`
   - OpenAIçš„å‚æ•°åæ˜¯`max_tokens`
   - æˆ‘ä»¬å¯¹å¤–ç»Ÿä¸€ä½¿ç”¨`max_tokens`ï¼Œå†…éƒ¨åšæ˜ å°„

2. **ä¸ºä»€ä¹ˆè¦è®°å½•æ—¥å¿—ï¼Ÿ**
   ```python
   logger.info(f"åˆå§‹åŒ– Ollama æ¨¡å‹: {self.model_name}")
   logger.info("Ollama æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
   ```
   - è°ƒè¯•æ—¶çŸ¥é“ä½¿ç”¨äº†å“ªä¸ªæ¨¡å‹
   - ç”Ÿäº§ç¯å¢ƒæ’æŸ¥é—®é¢˜æ—¶æœ‰æ®å¯æŸ¥
   - åŒºåˆ†åˆå§‹åŒ–å¤±è´¥çš„ä½ç½®

3. **ä¸ºä»€ä¹ˆä¼ é€’`**kwargs`ï¼Ÿ**
   - Ollamaæ”¯æŒå¾ˆå¤šè‡ªå®šä¹‰å‚æ•°ï¼ˆå¦‚`num_ctx`ä¸Šä¸‹æ–‡é•¿åº¦ã€`num_gpu`GPUå±‚æ•°ï¼‰
   - ç”¨æˆ·å¯ä»¥æ ¹æ®ç¡¬ä»¶é…ç½®è°ƒä¼˜
   - ç¤ºä¾‹ï¼š
     ```python
     client = UnifiedLLMClient(
         model_name="qwen2.5:7b",
         num_ctx=8192,  # é€šè¿‡kwargsä¼ é€’
         num_gpu=1      # é€šè¿‡kwargsä¼ é€’
     )
     ```

### ç¬¬ä¸‰éƒ¨åˆ†ï¼šåœ¨çº¿APIæ¨¡å¼åˆå§‹åŒ–ï¼ˆ69-83è¡Œï¼‰

**ä»£ç æ–‡ä»¶ï¼š** `study-agentic-rag/03-smart-qa-application/services/llm_client.py`

è¿™éƒ¨åˆ†åˆå§‹åŒ–é˜¿é‡Œäº‘ç™¾ç‚¼åœ¨çº¿APIï¼ˆå…¼å®¹OpenAIæ¥å£ï¼‰ã€‚

<details>
<summary>ç‚¹å‡»å±•å¼€ä»£ç </summary>

```python
    def _initialize_online_api(self, **kwargs) -> ChatOpenAI:
        """åˆå§‹åŒ–åœ¨çº¿ APIï¼ˆé˜¿é‡Œäº‘ç™¾ç‚¼ï¼Œå…¼å®¹ OpenAI æ¥å£ï¼‰"""
        logger.info(f"åˆå§‹åŒ–åœ¨çº¿ API æ¨¡å‹: {self.model_name}")

        # å°è¯•ä½¿ç”¨æ–°çš„ langchain-openai åŒ…ï¼Œå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ°æ—§çš„
        try:
            from langchain_openai import ChatOpenAI as NewChatOpenAI
            llm = NewChatOpenAI(
                openai_api_base=self.settings.ONLINE_BASE_URL,
                openai_api_key=self.settings.ONLINE_API_KEY,
                model_name=self.model_name,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                **kwargs
            )
        except ImportError:
            # å›é€€åˆ°æ—§çš„å®ç°
            llm = ChatOpenAI(
                openai_api_base=self.settings.ONLINE_BASE_URL,
                openai_api_key=self.settings.ONLINE_API_KEY,
                model_name=self.model_name,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                **kwargs
            )

        logger.info("åœ¨çº¿ API æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        return llm
```

</details>

**ä¸ºä»€ä¹ˆè¿™ä¹ˆå†™ï¼Ÿ**

1. **ä¸ºä»€ä¹ˆç”¨`ChatOpenAI`è€Œä¸æ˜¯`OpenAI`ï¼Ÿ**
   
   ```python
   llm = ChatOpenAI(...)
   ```
   - `ChatOpenAI`æ”¯æŒå¯¹è¯æ ¼å¼ï¼ˆ`[{"role": "user", "content": "..."}]`ï¼‰
   - ç°ä»£LLMéƒ½æ˜¯å¯¹è¯æ¨¡å‹ï¼Œä¸æ˜¯è¡¥å…¨æ¨¡å‹
- LangChainæ¨èä½¿ç”¨`ChatOpenAI`
  
2. **ä¸ºä»€ä¹ˆç”¨`openai_api_base`ï¼Ÿ**
   ```python
   openai_api_base=self.settings.ONLINE_BASE_URL
   ```
   - é˜¿é‡Œäº‘ç™¾ç‚¼å…¼å®¹OpenAIæ¥å£
   - åªéœ€ä¿®æ”¹`api_base`æŒ‡å‘ç™¾ç‚¼çš„URL
   - æ— éœ€ä¿®æ”¹ä»£ç å°±èƒ½åˆ‡æ¢åˆ°å…¶ä»–å…¼å®¹OpenAIçš„æœåŠ¡ï¼ˆå¦‚Azureã€Cloudflareï¼‰

3. **ä¸ºä»€ä¹ˆ`max_tokens`ä¸éœ€è¦æ˜ å°„ï¼Ÿ**
   - OpenAIæ¥å£çš„å‚æ•°åå°±æ˜¯`max_tokens`
   - ç›´æ¥ä¼ é€’å³å¯
   - ä¸Ollamaçš„`num_predict`å½¢æˆå¯¹æ¯”
   
4. æ™ºèƒ½ç‰ˆæœ¬å…¼å®¹
   - ä¼˜å…ˆä½¿ç”¨æ–°ç‰ˆæœ¬ ï¼š langchain_openai.ChatOpenAI
   - è‡ªåŠ¨å›é€€æœºåˆ¶ ï¼šå¦‚æœæ–°åŒ…æœªå®‰è£…ï¼Œè‡ªåŠ¨ä½¿ç”¨æ—§ç‰ˆæœ¬ langchain.llms.ChatOpenAI
   - é›¶é…ç½®åˆ‡æ¢ ï¼šæ— éœ€æ‰‹åŠ¨ä¿®æ”¹ä»£ç ï¼Œè‡ªåŠ¨é€‚åº”ç¯å¢ƒ

### ç¬¬å››éƒ¨åˆ†ï¼šç»Ÿä¸€è°ƒç”¨æ¥å£ï¼ˆ85-128è¡Œï¼‰

**ä»£ç æ–‡ä»¶ï¼š** `study-agentic-rag/03-smart-qa-application/services/llm_client.py`

è¿™éƒ¨åˆ†æä¾›ç»Ÿä¸€çš„è°ƒç”¨æ–¹æ³•ï¼Œå±è”½åº•å±‚å·®å¼‚ã€‚

<details>
<summary>ç‚¹å‡»å±•å¼€ä»£ç </summary>

```python
    def invoke(self, prompt: str) -> str:
        """
        è°ƒç”¨ LLM ç”Ÿæˆå›ç­”

        Args:
            prompt: æç¤ºè¯

        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        try:
            if self.provider == "ollama":
                # Ollama ä½¿ç”¨å­—ç¬¦ä¸²æç¤º
                response = self.llm.invoke(prompt)
                return response
            else:  # online
                # åœ¨çº¿ API ä½¿ç”¨æ¶ˆæ¯æ ¼å¼
                messages = [HumanMessage(content=prompt)]
                response = self.llm.invoke(messages)
                return response.content if hasattr(response, 'content') else str(response)

        except Exception as e:
            logger.error(f"LLM è°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    def get_llm(self) -> BaseLLM:
        """è·å–åº•å±‚ LLM å¯¹è±¡ï¼ˆç”¨äº LangChain Agentï¼‰"""
        return self.llm

    def get_provider_info(self) -> Dict[str, str]:
        """è·å–æä¾›å•†ä¿¡æ¯"""
        info = {
            "provider": self.provider,
            "model": self.model_name,
            "temperature": str(self.temperature),
            "max_tokens": str(self.max_tokens)
        }

        if self.provider == "ollama":
            info["base_url"] = self.settings.OLLAMA_BASE_URL
        else:
            info["base_url"] = self.settings.ONLINE_BASE_URL

        return info
```

</details>

**ä¸ºä»€ä¹ˆè¿™ä¹ˆå†™ï¼Ÿ**

1. **ä¸ºä»€ä¹ˆ`invoke()`æ–¹æ³•éœ€è¦åŒºåˆ†æä¾›å•†ï¼Ÿ**
   
   ```python
   if self.provider == "ollama":
       response = self.llm.invoke(prompt)  # å­—ç¬¦ä¸²
   else:  # online
       messages = [HumanMessage(content=prompt)]
       response = self.llm.invoke(messages)  # æ¶ˆæ¯åˆ—è¡¨
   ```
   - **Ollama**ï¼šæ¥å—å­—ç¬¦ä¸²æç¤ºè¯
   - **ChatOpenAI**ï¼šæ¥å—æ¶ˆæ¯åˆ—è¡¨ï¼ˆ`[HumanMessage(...)]`ï¼‰
- ä¸åŒçš„LangChainåŒ…è£…ç±»ï¼Œè°ƒç”¨æ–¹å¼ä¸åŒ
  
2. **ä¸ºä»€ä¹ˆåœ¨çº¿APIéœ€è¦`response.content`ï¼Ÿ**
   ```python
   return response.content if hasattr(response, 'content') else str(response)
   ```
   - `ChatOpenAI`è¿”å›çš„æ˜¯`AIMessage`å¯¹è±¡
   - éœ€è¦è®¿é—®`.content`å±æ€§è·å–æ–‡æœ¬
   - Ollamaç›´æ¥è¿”å›å­—ç¬¦ä¸²
   - `hasattr`é˜²å¾¡æ€§ç¼–ç¨‹ï¼Œé¿å…å±æ€§ä¸å­˜åœ¨

3. **ä¸ºä»€ä¹ˆæä¾›`get_llm()`æ–¹æ³•ï¼Ÿ**
   ```python
   def get_llm(self) -> BaseLLM:
       return self.llm
   ```
   - LangChainçš„Agentéœ€è¦åŸç”Ÿçš„LLMå¯¹è±¡
   - ä¸èƒ½ç›´æ¥ä¼ `UnifiedLLMClient`
   - ç¤ºä¾‹ï¼š
     ```python
     client = UnifiedLLMClient(model_name="qwen2.5:7b")
     agent = initialize_agent(tools, client.get_llm())
     ```

### ç¬¬äº”éƒ¨åˆ†ï¼šåµŒå…¥æ¨¡å‹å®¢æˆ·ç«¯ï¼ˆ131-216è¡Œï¼‰

**ä»£ç æ–‡ä»¶ï¼š** `study-agentic-rag/03-smart-qa-application/services/llm_client.py`

è¿™éƒ¨åˆ†å®ç°äº†ç»Ÿä¸€çš„åµŒå…¥æ¨¡å‹å®¢æˆ·ç«¯ï¼Œæ”¯æŒåŒæ¨¡å¼ã€‚

<details>
<summary>ç‚¹å‡»å±•å¼€ä»£ç </summary>

```python
class AliyunEmbeddingWrapper(Embeddings):
    """é˜¿é‡Œäº‘ç™¾ç‚¼åµŒå…¥æ¨¡å‹çš„ LangChain å…¼å®¹åŒ…è£…å™¨"""
    
    def __init__(self, openai_client, model_name):
        self.openai_client = openai_client
        self.model_name = model_name
        self.embedding_model_name = model_name
    
    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥å•ä¸ªæŸ¥è¯¢æ–‡æœ¬"""
        response = self.openai_client.embeddings.create(
            model=self.model_name,
            input=text,
            encoding_format="float"
        )
        return response.data[0].embedding
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """åµŒå…¥å¤šä¸ªæ–‡æ¡£"""
        response = self.openai_client.embeddings.create(
            model=self.model_name,
            input=texts,
            encoding_format="float"
        )
        return [data.embedding for data in response.data]


class UnifiedEmbeddingClient:
    """ç»Ÿä¸€çš„åµŒå…¥æ¨¡å‹å®¢æˆ·ç«¯"""

    def __init__(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å®¢æˆ·ç«¯"""
        self.settings = Settings()
        # ä¼˜å…ˆè¯»å–æœ€æ–°ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿è¿è¡ŒæœŸåˆ‡æ¢ç”Ÿæ•ˆ
        self.provider = os.getenv("LLM_PROVIDER", self.settings.LLM_PROVIDER)
        self.embedding_model = self.settings.get_embedding_model()
        self.embeddings = self._initialize_embeddings()

    def _initialize_embeddings(self):
        """æ ¹æ®é…ç½®åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            if self.provider == "ollama":
                return self._initialize_ollama_embeddings()
            else:  # online
                return self._initialize_online_embeddings()
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å¤±è´¥: {str(e)}")
            raise

    def _initialize_ollama_embeddings(self):
        """åˆå§‹åŒ– Ollama åµŒå…¥æ¨¡å‹"""
        from langchain_community.embeddings import OllamaEmbeddings

        logger.info(f"åˆå§‹åŒ– Ollama åµŒå…¥æ¨¡å‹: {self.embedding_model}")

        embeddings = OllamaEmbeddings(
            base_url=self.settings.OLLAMA_BASE_URL,
            model=self.embedding_model
        )

        logger.info("Ollama åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        return embeddings

    def _initialize_online_embeddings(self):
        """åˆå§‹åŒ–åœ¨çº¿ API åµŒå…¥æ¨¡å‹ï¼ˆé˜¿é‡Œäº‘ç™¾ç‚¼ï¼‰"""
        try:
            from openai import OpenAI
        except ImportError:
            logger.error("openai åŒ…æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨åœ¨çº¿åµŒå…¥æ¨¡å‹")
            raise ImportError("è¯·å®‰è£… openai åŒ…: pip install openai")

        logger.info(f"åˆå§‹åŒ–åœ¨çº¿åµŒå…¥æ¨¡å‹: {self.embedding_model}")

        # ä½¿ç”¨åŸç”Ÿ OpenAI å®¢æˆ·ç«¯ï¼Œä½†åŒ…è£…æˆ LangChain å…¼å®¹æ¥å£
        openai_client = OpenAI(
            api_key=self.settings.ONLINE_API_KEY,
            base_url=self.settings.ONLINE_BASE_URL
        )
        
        # è¿”å›å…¼å®¹ LangChain æ¥å£çš„åŒ…è£…å™¨
        logger.info("åœ¨çº¿åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        return AliyunEmbeddingWrapper(openai_client, self.embedding_model)

    def embed_query(self, text: str) -> List[float]:
        """
        åµŒå…¥å•ä¸ªæŸ¥è¯¢æ–‡æœ¬

        Args:
            text: æŸ¥è¯¢æ–‡æœ¬

        Returns:
            åµŒå…¥å‘é‡
        """
        try:
            return self.embeddings.embed_query(text)
        except Exception as e:
            logger.error(f"æŸ¥è¯¢åµŒå…¥å¤±è´¥: {str(e)}")
            raise

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        åµŒå…¥å¤šä¸ªæ–‡æ¡£

        Args:
            texts: æ–‡æ¡£åˆ—è¡¨

        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        try:
            return self.embeddings.embed_documents(texts)
        except Exception as e:
            logger.error(f"æ–‡æ¡£åµŒå…¥å¤±è´¥: {str(e)}")
            raise

    def get_embeddings(self):
        """è·å–åµŒå…¥æ¨¡å‹å¯¹è±¡"""
        return self.embeddings
```

</details>

**ä¸ºä»€ä¹ˆè¿™ä¹ˆå†™ï¼Ÿ**

>
> ## ğŸ¯ ä»£ç æ€»æ‹¬
>
> è¿™æ®µä»£ç æ˜¯ä¸€ä¸ª**æ™ºèƒ½åµŒå…¥æ¨¡å‹å®¢æˆ·ç«¯**ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**ç”¨ä¸€å¥—æ¥å£ï¼Œé€‚é…å¤šç§åµŒå…¥æ¨¡å‹**ã€‚
>
> ### æ ¸å¿ƒæ¶æ„
> ```
> UnifiedEmbeddingClient (ç»Ÿä¸€å…¥å£)
> â”œâ”€â”€ Ollama æ¨¡å¼ â†’ ç›´æ¥è°ƒç”¨ LangChain çš„ OllamaEmbeddings
> â””â”€â”€ Online æ¨¡å¼ â†’ é˜¿é‡Œäº‘ç™¾ç‚¼ â†’ AliyunEmbeddingWrapper (å…¼å®¹åŒ…è£…å™¨)
> ```
>
> ## ğŸ” å…³é”®è®¾è®¡
>
> ### 1ï¸âƒ£ **AliyunEmbeddingWrapper** - å…¼å®¹ç¥å™¨
> - **ä½œç”¨**ï¼šè®©åŸç”Ÿ OpenAI å®¢æˆ·ç«¯"ä¼ªè£…"æˆ LangChain æ¥å£
> - **åŸç†**ï¼šåŒ…è£…é˜¿é‡Œäº‘ç™¾ç‚¼ APIï¼Œæä¾› `embed_query()` å’Œ `embed_documents()` æ–¹æ³•
> - **å¥½å¤„**ï¼šä¸Šå±‚ä»£ç æ— éœ€æ”¹åŠ¨ï¼Œæ— ç¼åˆ‡æ¢
>
> ### 2ï¸âƒ£ **UnifiedEmbeddingClient** - ç»Ÿä¸€è°ƒåº¦å™¨
> - **æ™ºèƒ½è·¯ç”±**ï¼šæ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹© Ollama æˆ– Online æ¨¡å¼
> - **å¼‚å¸¸å¤„ç†**ï¼šç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
> - **æ¥å£æ ‡å‡†åŒ–**ï¼šæ— è®ºåº•å±‚ç”¨ä»€ä¹ˆï¼Œå¯¹å¤–æ¥å£ä¿æŒä¸€è‡´   
>

### ç¬¬å…­éƒ¨åˆ†ï¼šåŠŸèƒ½æµ‹è¯•

1ï¼‰æµ‹è¯•ä»£ç 

```python
if __name__ == "__main__":
    """ç®€å•çš„æµ‹è¯•ä»£ç ï¼ŒéªŒè¯ LLM å®¢æˆ·ç«¯æ ¸å¿ƒåŠŸèƒ½ï¼ˆä»…æµ‹è¯• online æ¨¡å¼ï¼‰"""
    import os
    
    print("=== å¼€å§‹æµ‹è¯• LLM å®¢æˆ·ç«¯ (Online æ¨¡å¼) ===")
    
    # æµ‹è¯•é…ç½®åŠ è½½
    try:
        settings = Settings()
        print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ - æä¾›å•†: {settings.LLM_PROVIDER}")
        
        # è·å–å½“å‰æ¨¡å‹åç§°
        current_model = settings.get_default_model()
        print(f"   æ¨¡å‹: {current_model}")
        print(f"   åµŒå…¥æ¨¡å‹: {settings.get_embedding_model()}")
        
        # ç¡®ä¿æ˜¯åœ¨ online æ¨¡å¼ä¸‹æµ‹è¯•
        if settings.LLM_PROVIDER != "online":
            print(f"âš ï¸  å½“å‰é…ç½®ä¸º {settings.LLM_PROVIDER} æ¨¡å¼ï¼Œä»…æµ‹è¯• online æ¨¡å¼ç›¸å…³åŠŸèƒ½")
            print("   è·³è¿‡ Ollama ç›¸å…³æµ‹è¯•")
            print("\n=== æµ‹è¯•å®Œæˆ ===")
            exit(0)
            
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        exit(1)
    
    # æµ‹è¯• UnifiedLLMClient (ä»… online æ¨¡å¼)
    try:
        llm_client = UnifiedLLMClient(
            model_name=current_model,
            temperature=0.1,
            max_tokens=100
        )
        print(f"âœ… LLM å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ - æä¾›å•†: {llm_client.provider}")
        
        # è·å–æä¾›å•†ä¿¡æ¯
        info = llm_client.get_provider_info()
        print(f"   æä¾›å•†ä¿¡æ¯: {info}")
        
    except Exception as e:
        print(f"âŒ LLM å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        exit(1)
    
    # æµ‹è¯• UnifiedEmbeddingClient (ä»… online æ¨¡å¼)
    try:
        embedding_client = UnifiedEmbeddingClient()
        print(f"âœ… åµŒå…¥å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ - æä¾›å•†: {embedding_client.provider}")
        print(f"   åµŒå…¥æ¨¡å‹: {embedding_client.embedding_model}")
        
    except Exception as e:
        print(f"âŒ åµŒå…¥å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        exit(1)
    
    # æµ‹è¯•åµŒå…¥æ¨¡å‹ï¼ˆä»… online æ¨¡å¼ï¼‰
    try:
        print("æ­£åœ¨æµ‹è¯•åµŒå…¥æ¨¡å‹...")
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
        embeddings = embedding_client.embed_query(test_text)
        print(f"âœ… åµŒå…¥æ¨¡å‹æµ‹è¯•æˆåŠŸ")
        print(f"   åµŒå…¥ç»´åº¦: {len(embeddings)}")
        
    except Exception as e:
        # ç®€åŒ–é”™è¯¯ä¿¡æ¯
        error_msg = str(e)
        if "400" in error_msg:
            print(f"âš ï¸  åµŒå…¥æ¨¡å‹æµ‹è¯•å¤±è´¥ï¼ˆAPI æ ¼å¼é—®é¢˜ï¼‰")
            print("   æç¤ºï¼šé˜¿é‡Œäº‘ç™¾ç‚¼åµŒå…¥æ¨¡å‹å¯èƒ½éœ€è¦ç‰¹æ®Šé…ç½®")
        elif "API key" in error_msg or "authentication" in error_msg.lower():
            print(f"âš ï¸  åµŒå…¥æ¨¡å‹æµ‹è¯•å¤±è´¥ï¼ˆAPI å¯†é’¥é—®é¢˜ï¼‰")
        else:
            print(f"âš ï¸  åµŒå…¥æ¨¡å‹æµ‹è¯•å¤±è´¥: {error_msg[:50]}...")
    
    # æµ‹è¯• LLM è°ƒç”¨ï¼ˆä»… online æ¨¡å¼ï¼‰
    try:
        print("æ­£åœ¨æµ‹è¯• LLM è°ƒç”¨...")
        test_prompt = "ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±"
        response = llm_client.invoke(test_prompt)
        print(f"âœ… LLM è°ƒç”¨æˆåŠŸ")
        print(f"   å›ç­”: {response[:50]}...")
        
    except Exception as e:
        # ç®€åŒ–é”™è¯¯ä¿¡æ¯
        error_msg = str(e)
        if "API key" in error_msg or "authentication" in error_msg.lower():
            print(f"âš ï¸  LLM è°ƒç”¨æµ‹è¯•å¤±è´¥ï¼ˆAPI å¯†é’¥é—®é¢˜ï¼‰")
        elif "network" in error_msg.lower() or "connection" in error_msg.lower():
            print(f"âš ï¸  LLM è°ƒç”¨æµ‹è¯•å¤±è´¥ï¼ˆç½‘ç»œè¿æ¥é—®é¢˜ï¼‰")
        else:
            print(f"âš ï¸  LLM è°ƒç”¨æµ‹è¯•å¤±è´¥: {error_msg[:50]}...")
    
    print("\n=== æµ‹è¯•å®Œæˆ ===")
```

2ï¼‰è¿è¡Œæµ‹è¯•

```bash
uv run python services/llm_client.py
```

3ï¼‰é¢„æœŸæ•ˆæœ

```bash
=== å¼€å§‹æµ‹è¯• LLM å®¢æˆ·ç«¯ (Online æ¨¡å¼) ===
âœ… é…ç½®åŠ è½½æˆåŠŸ - æä¾›å•†: online
   æ¨¡å‹: qwen-plus
   åµŒå…¥æ¨¡å‹: text-embedding-v1 
âœ… LLM å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ - æä¾›å•†: online
   æä¾›å•†ä¿¡æ¯: {'provider': 'online', 'model': 'qwen-plus', 'temperature': '0.1', 'max_tokens': '100', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1'}
âœ… åµŒå…¥å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ - æä¾›å•†: online
   åµŒå…¥æ¨¡å‹: text-embedding-v1
æ­£åœ¨æµ‹è¯•åµŒå…¥æ¨¡å‹...
âœ… åµŒå…¥æ¨¡å‹æµ‹è¯•æˆåŠŸ
   åµŒå…¥ç»´åº¦: 1536   
æ­£åœ¨æµ‹è¯• LLM è°ƒç”¨...
âœ… LLM è°ƒç”¨æˆåŠŸ
   å›ç­”: æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œç”±é˜¿é‡Œäº‘ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œè¿˜èƒ½è¡¨è¾¾è§‚ç‚¹ã€ç©æ¸¸æˆç­‰ã€‚...
=== æµ‹è¯•å®Œæˆ ===
```



## å››ã€å®Œæ•´ä»£ç æ€»ç»“

ä¸Šé¢çš„5ä¸ªéƒ¨åˆ†ç»„æˆäº†å®Œæ•´çš„`llm_client.py`ï¼ˆ216è¡Œï¼‰ï¼š

1. **å®¢æˆ·ç«¯åˆå§‹åŒ–**ï¼ˆ52è¡Œï¼‰ï¼šæ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©æä¾›å•†
2. **Ollamaæ¨¡å¼**ï¼ˆ14è¡Œï¼‰ï¼šåˆå§‹åŒ–æœ¬åœ°æ¨¡å‹ï¼Œæ˜ å°„å‚æ•°å
3. **åœ¨çº¿APIæ¨¡å¼**ï¼ˆ15è¡Œï¼‰ï¼šåˆå§‹åŒ–äº‘ç«¯æ¨¡å‹ï¼Œå…¼å®¹OpenAIæ¥å£
4. **ç»Ÿä¸€è°ƒç”¨æ¥å£**ï¼ˆ44è¡Œï¼‰ï¼šå±è”½å·®å¼‚ï¼Œæä¾›ä¸€è‡´çš„API
5. **åµŒå…¥æ¨¡å‹å®¢æˆ·ç«¯**ï¼ˆ86è¡Œï¼‰ï¼šåŒæ¨¡å¼æ”¯æŒï¼ŒåŒºåˆ†æŸ¥è¯¢å’Œæ–‡æ¡£

**æ ¸å¿ƒè®¾è®¡æ¨¡å¼**ï¼š

| æ¨¡å¼ | åº”ç”¨åœºæ™¯ | ä»£ç ä½ç½® |
|------|---------|---------|
| **å·¥å‚æ¨¡å¼** | æ ¹æ®é…ç½®åˆ›å»ºä¸åŒçš„LLM | `_initialize_llm()` |
| **é€‚é…å™¨æ¨¡å¼** | ç»Ÿä¸€ä¸åŒLLMçš„æ¥å£ | `invoke()` |
| **ç­–ç•¥æ¨¡å¼** | åˆ‡æ¢ä¸åŒçš„æä¾›å•† | `provider` åˆ¤æ–­ |
| **å•ä¸€èŒè´£** | åˆ†ç¦»LLMå’ŒEmbedding | ä¸¤ä¸ªç‹¬ç«‹ç±» |

**åŒæ¨¡å¼åˆ‡æ¢ç¤ºæ„å›¾**ï¼š

```python
# åªéœ€ä¿®æ”¹ç¯å¢ƒå˜é‡
LLM_PROVIDER=ollama    # å¼€å‘ç¯å¢ƒ
LLM_PROVIDER=online    # ç”Ÿäº§ç¯å¢ƒ

# ä»£ç å®Œå…¨ä¸å˜ï¼
client = UnifiedLLMClient(model_name="qwen2.5:7b")
response = client.invoke("ä½ å¥½")
```

## äº”ã€å®é™…ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šåœ¨Agentä¸­ä½¿ç”¨

```python
from services.llm_client import UnifiedLLMClient
from langchain.agents import initialize_agent, Tool

# åˆ›å»ºç»Ÿä¸€å®¢æˆ·ç«¯ï¼ˆè‡ªåŠ¨æ ¹æ®ç¯å¢ƒå˜é‡é€‰æ‹©æä¾›å•†ï¼‰
client = UnifiedLLMClient(
    model_name="qwen2.5:7b",
    temperature=0.7,
    max_tokens=2048
)

# å®šä¹‰å·¥å…·
tools = [
    Tool(name="Calculator", func=calculate, description="è®¡ç®—æ•°å­¦é—®é¢˜"),
]

# åˆå§‹åŒ–Agentï¼ˆä½¿ç”¨get_llm()è·å–åº•å±‚LLMï¼‰
agent = initialize_agent(
    tools,
    client.get_llm(),
    agent="zero-shot-react-description",
    verbose=True
)

# è¿è¡Œ
result = agent.run("125 * 37 ç­‰äºå¤šå°‘ï¼Ÿ")
```

### ç¤ºä¾‹2ï¼šåœ¨å‘é‡å­˜å‚¨ä¸­ä½¿ç”¨

```python
from services.llm_client import UnifiedEmbeddingClient
from langchain.vectorstores import FAISS

# åˆ›å»ºåµŒå…¥å®¢æˆ·ç«¯
embedding_client = UnifiedEmbeddingClient()

# åˆ›å»ºå‘é‡å­˜å‚¨
documents = ["è¿™æ˜¯ç¬¬ä¸€ä¸ªæ–‡æ¡£", "è¿™æ˜¯ç¬¬äºŒä¸ªæ–‡æ¡£"]
vector_store = FAISS.from_texts(
    documents,
    embedding_client.get_embeddings()  # ç»Ÿä¸€æ¥å£
)

# æŸ¥è¯¢
results = vector_store.similarity_search("æ–‡æ¡£")
```

### ç¤ºä¾‹3ï¼šè·å–æä¾›å•†ä¿¡æ¯

```python
client = UnifiedLLMClient(model_name="qwen2.5:7b")
info = client.get_provider_info()

print(info)
# è¾“å‡ºï¼š
# {
#     "provider": "ollama",
#     "model": "qwen2.5:7b",
#     "temperature": "0.7",
#     "max_tokens": "2048",
#     "base_url": "http://localhost:11434"
# }
```

## å…­ã€é…ç½®ç®¡ç†ï¼ˆå›é¡¾settings.pyï¼‰

åŒæ¨¡å¼çš„é…ç½®åœ¨`config/settings.py`ä¸­ç®¡ç†ï¼š

```python
# ä»ç¯å¢ƒå˜é‡è¯»å–
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "online")  # é»˜è®¤online

# Ollamaé…ç½®
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
OLLAMA_LLM_MODEL = "qwen2.5:7b"
OLLAMA_EMBEDDING_MODEL = "nomic-embed-text:latest"

# åœ¨çº¿APIé…ç½®
ONLINE_BASE_URL = os.getenv("ONLINE_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")
ONLINE_API_KEY = os.getenv("ONLINE_API_KEY", "")
ONLINE_LLM_MODEL = "qwen-plus"
ONLINE_EMBEDDING_MODEL = "text-embedding-v1"
```

**ç¯å¢ƒå˜é‡ä¼˜å…ˆçº§**ï¼š
1. è¿è¡Œæ—¶ç¯å¢ƒå˜é‡ï¼ˆæœ€é«˜ï¼‰
2. `.env`æ–‡ä»¶
3. ä»£ç é»˜è®¤å€¼ï¼ˆæœ€ä½ï¼‰

## ä¸ƒã€æœ¬è®²æ€»ç»“

æˆ‘ä»¬å®Œæˆäº†åŒæ¨¡å¼LLMé›†æˆï¼š

1. **ç»Ÿä¸€æ¥å£è®¾è®¡**ï¼š`UnifiedLLMClient`å’Œ`UnifiedEmbeddingClient`
2. **å·¥å‚æ¨¡å¼**ï¼šæ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©æä¾›å•†
3. **å‚æ•°æ˜ å°„**ï¼š`max_tokens` â†” `num_predict`
4. **è°ƒç”¨å·®å¼‚**ï¼šå­—ç¬¦ä¸² vs æ¶ˆæ¯åˆ—è¡¨
5. **LangChainé›†æˆ**ï¼š`get_llm()`å’Œ`get_embeddings()`æ–¹æ³•

**å…³é”®æŠ€æœ¯ç‚¹**ï¼š
- é…ç½®é©±åŠ¨çš„æ¶æ„è®¾è®¡
- é€‚é…å™¨æ¨¡å¼ç»Ÿä¸€æ¥å£
- æ—¥å¿—è®°å½•ä¾¿äºè°ƒè¯•
- é˜²å¾¡æ€§ç¼–ç¨‹ï¼ˆ`hasattr`ï¼‰
- å»¶è¿Ÿå¯¼å…¥ï¼ˆ`from langchain_community...`ï¼‰

**åˆ‡æ¢æ¨¡å¼åªéœ€**ï¼š
```bash
# å¼€å‘ç¯å¢ƒ - ä½¿ç”¨æœ¬åœ°Ollama
export LLM_PROVIDER=ollama

# ç”Ÿäº§ç¯å¢ƒ - ä½¿ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼
export LLM_PROVIDER=online
export ONLINE_API_KEY=sk-xxx
```

