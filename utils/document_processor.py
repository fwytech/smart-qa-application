import os
import hashlib
import logging
from typing import List, Dict, Optional, Any
from pathlib import Path
import streamlit as st
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredWordDocumentLoader
from config.settings import Settings

logger = logging.getLogger(__name__)

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ç±»"""

    def __init__(self):
        self.settings = Settings()
        self.cache_dir = self.settings.DATA_DIR / "document_cache"
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def _get_file_hash(self, file_content: bytes) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        return hashlib.md5(file_content).hexdigest()

    def process_uploaded_file(self, uploaded_file) -> List[Document]:
        """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆæ”¯æŒ UploadedFile å¯¹è±¡å’Œæ–‡ä»¶è·¯å¾„ï¼‰"""
        try:
            # å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥
            if hasattr(uploaded_file, 'size') and hasattr(uploaded_file, 'read'):
                # Streamlit UploadedFile å¯¹è±¡
                if uploaded_file.size > self.settings.MAX_FILE_SIZE:
                    raise ValueError(f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶: {uploaded_file.size} > {self.settings.MAX_FILE_SIZE}")
                file_content = uploaded_file.read()
                file_name = uploaded_file.name
            else:
                # æ–‡ä»¶è·¯å¾„
                file_path = Path(uploaded_file)
                if not file_path.exists():
                    raise ValueError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                file_size = file_path.stat().st_size
                if file_size > self.settings.MAX_FILE_SIZE:
                    raise ValueError(f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶: {file_size} > {self.settings.MAX_FILE_SIZE}")
                with open(file_path, 'rb') as f:
                    file_content = f.read()
                file_name = file_path.name
            
            file_type = Path(file_name).suffix.lower()

            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            if file_type not in self.settings.SUPPORTED_FILE_TYPES:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")

            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
            file_hash = self._get_file_hash(file_content)
            cache_path = self._get_cache_path(file_hash, file_name)

            # å°è¯•ä»ç¼“å­˜åŠ è½½
            cached_documents = self._load_from_cache(cache_path)
            if cached_documents is not None:
                return cached_documents

            # å¤„ç†æ–‡ä»¶
            documents = self._process_file_content(file_content, file_name, file_type)

            # ä¿å­˜åˆ°ç¼“å­˜
            self._save_to_cache(cache_path, documents)

            logger.info(f"å¤„ç†æ–‡ä»¶æˆåŠŸ: {file_name}, æ–‡æ¡£æ•°é‡: {len(documents)}")
            return documents

        except Exception as e:
            logger.error(f"å¤„ç†ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {str(e)}")
            raise

    def _process_file_content(self, file_content: bytes, file_name: str, file_type: str) -> List[Document]:
        """å¤„ç†æ–‡ä»¶å†…å®¹"""
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            temp_dir = self.settings.DATA_DIR / "temp"
            temp_dir.mkdir(parents=True, exist_ok=True)
            temp_path = temp_dir / file_name

            # å†™å…¥ä¸´æ—¶æ–‡ä»¶
            with open(temp_path, 'wb') as f:
                f.write(file_content)

            try:
                # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
                if file_type == '.pdf':
                    documents = self._load_pdf(temp_path)
                elif file_type == '.txt':
                    documents = self._load_text(temp_path)
                elif file_type == '.md':
                    documents = self._load_markdown(temp_path)
                elif file_type == '.docx':
                    documents = self._load_word(temp_path)
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")

                # æ·»åŠ å…ƒæ•°æ®
                for i, doc in enumerate(documents):
                    doc.metadata.update({
                        'source': file_name,
                        'file_type': file_type,
                        'chunk_index': i,
                        'total_chunks': len(documents),
                        'processing_timestamp': str(Path(temp_path).stat().st_mtime)
                    })

                return documents

            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_path.exists():
                    temp_path.unlink()

        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å†…å®¹å¤±è´¥: {str(e)}")
            raise

    def _load_pdf(self, file_path: Path) -> List[Document]:
        """åŠ è½½PDFæ–‡ä»¶"""
        try:
            loader = PyPDFLoader(str(file_path))
            documents = loader.load()

            # æ·»åŠ é¡µç ä¿¡æ¯
            for i, doc in enumerate(documents):
                if 'page' not in doc.metadata:
                    doc.metadata['page'] = i + 1

            logger.info(f"åŠ è½½PDFæˆåŠŸ: {file_path.name}, é¡µæ•°: {len(documents)}")
            return documents

        except Exception as e:
            logger.error(f"åŠ è½½PDFå¤±è´¥: {str(e)}")
            raise
            
    def _load_text(self, file_path: Path) -> List[Document]:
        """åŠ è½½æ–‡æœ¬æ–‡ä»¶"""
        try:
            loader = TextLoader(str(file_path), encoding='utf-8')
            documents = loader.load()
            
            logger.info(f"åŠ è½½æ–‡æœ¬æ–‡ä»¶æˆåŠŸ: {file_path.name}")
            return documents
            
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {str(e)}")
            raise
    
    def _load_markdown(self, file_path: Path) -> List[Document]:
        """åŠ è½½Markdownæ–‡ä»¶"""
        try:
            # Markdownæ–‡ä»¶ä¹Ÿä½¿ç”¨æ–‡æœ¬åŠ è½½å™¨
            loader = TextLoader(str(file_path), encoding='utf-8')
            documents = loader.load()
            
            # æ·»åŠ æ–‡ä»¶ç±»å‹æ ‡è¯†
            for doc in documents:
                doc.metadata['file_type'] = '.md'
            
            logger.info(f"åŠ è½½Markdownæ–‡ä»¶æˆåŠŸ: {file_path.name}")
            return documents
            
        except Exception as e:
            logger.error(f"åŠ è½½Markdownæ–‡ä»¶å¤±è´¥: {str(e)}")
            raise
    
    def _load_word(self, file_path: Path) -> List[Document]:
        """åŠ è½½Wordæ–‡æ¡£"""
        try:
            loader = UnstructuredWordDocumentLoader(str(file_path))
            documents = loader.load()
            
            logger.info(f"åŠ è½½Wordæ–‡æ¡£æˆåŠŸ: {file_path.name}")
            return documents
            
        except Exception as e:
            logger.error(f"åŠ è½½Wordæ–‡æ¡£å¤±è´¥: {str(e)}")
            raise           


    def split_documents(self, documents: List[Document], chunk_size: int = None, chunk_overlap: int = None) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£"""
        try:
            chunk_size = chunk_size or self.settings.CHUNK_SIZE
            chunk_overlap = chunk_overlap or self.settings.CHUNK_OVERLAP
            
            logger.info(f"åˆ†å‰²æ–‡æ¡£ï¼Œchunk_size: {chunk_size}, chunk_overlap: {chunk_overlap}")
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                length_function=len,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
            )
            
            split_docs = text_splitter.split_documents(documents)
            
            # æ›´æ–°å…ƒæ•°æ®
            for i, doc in enumerate(split_docs):
                doc.metadata['chunk_index'] = i
                doc.metadata['chunk_size'] = len(doc.page_content)
                doc.metadata['total_chunks'] = len(split_docs)
            
            logger.info(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œç‰‡æ®µæ•°é‡: {len(split_docs)}")
            return split_docs
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ†å‰²å¤±è´¥: {str(e)}")
            return documents    

    def _get_cache_path(self, file_hash: str, file_name: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{file_hash}_{file_name}.json"
    
    def _load_from_cache(self, cache_path: Path) -> Optional[List[Document]]:
        """ä»ç¼“å­˜åŠ è½½æ–‡æ¡£"""
        try:
            if cache_path.exists() and self.settings.CACHE_ENABLED:
                import json
                with open(cache_path, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                import time
                current_time = time.time()
                cache_time = cache_data.get('timestamp', 0)
                
                if current_time - cache_time < self.settings.CACHE_EXPIRE_TIME:
                    # é‡å»ºDocumentå¯¹è±¡
                    documents = []
                    for doc_data in cache_data.get('documents', []):
                        doc = Document(
                            page_content=doc_data['page_content'],
                            metadata=doc_data['metadata']
                        )
                        documents.append(doc)
                    
                    logger.info(f"ä»ç¼“å­˜åŠ è½½æ–‡æ¡£æˆåŠŸ: {len(documents)} ä¸ªæ–‡æ¡£")
                    return documents
                else:
                    logger.info("ç¼“å­˜å·²è¿‡æœŸ")
                    
        except Exception as e:
            logger.error(f"ä»ç¼“å­˜åŠ è½½å¤±è´¥: {str(e)}")
        
        return None
    
    def _save_to_cache(self, cache_path: Path, documents: List[Document]):
        """ä¿å­˜æ–‡æ¡£åˆ°ç¼“å­˜"""
        try:
            if not self.settings.CACHE_ENABLED:
                return
                
            import json
            import time
            
            cache_data = {
                'timestamp': time.time(),
                'documents': [
                    {
                        'page_content': doc.page_content,
                        'metadata': doc.metadata
                    }
                    for doc in documents
                ]
            }
            
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ä¿å­˜åˆ°ç¼“å­˜æˆåŠŸ: {cache_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ°ç¼“å­˜å¤±è´¥: {str(e)}")
            
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        try:
            cache_files = list(self.cache_dir.glob("*.json"))
            total_size = sum(f.stat().st_size for f in cache_files)
            
            stats = {
                'cache_enabled': self.settings.CACHE_ENABLED,
                'cache_expire_time': self.settings.CACHE_EXPIRE_TIME,
                'cache_files_count': len(cache_files),
                'cache_total_size_bytes': total_size,
                'cache_total_size_mb': round(total_size / (1024 * 1024), 2)
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {'error': str(e)}
    
    def clear_cache(self) -> bool:
        """æ¸…ç©ºç¼“å­˜"""
        try:
            cache_files = list(self.cache_dir.glob("*.json"))
            deleted_count = 0
            
            for cache_file in cache_files:
                try:
                    cache_file.unlink()
                    deleted_count += 1
                except Exception as e:
                    logger.error(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {cache_file}, é”™è¯¯: {str(e)}")
            
            logger.info(f"æ¸…ç©ºç¼“å­˜æˆåŠŸï¼Œåˆ é™¤æ–‡ä»¶æ•°: {deleted_count}")
            return True
            
        except Exception as e:
            logger.error(f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {str(e)}")
            return False                

    def process_documents_batch(self, uploaded_files: List) -> Dict[str, Any]:
        """æ‰¹é‡å¤„ç†æ–‡æ¡£"""
        try:
            results = {
                'total_files': len(uploaded_files),
                'processed_files': 0,
                'failed_files': 0,
                'total_documents': 0,
                'errors': []
            }
            
            all_documents = []
            
            for file in uploaded_files:
                try:
                    documents = self.process_uploaded_file(file)
                    all_documents.extend(documents)
                    results['processed_files'] += 1
                    results['total_documents'] += len(documents)
                    
                    logger.info(f"å¤„ç†æ–‡ä»¶æˆåŠŸ: {file.name}")
                    
                except Exception as e:
                    results['failed_files'] += 1
                    error_info = {
                        'file_name': file.name,
                        'error': str(e)
                    }
                    results['errors'].append(error_info)
                    logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {file.name}, é”™è¯¯: {str(e)}")
            
            results['all_documents'] = all_documents
            return results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†æ–‡æ¡£å¤±è´¥: {str(e)}")
            raise


def test_document_processor():
    """æµ‹è¯•æ–‡æ¡£å¤„ç†å™¨æ ¸å¿ƒåŠŸèƒ½"""
    import logging
    import time
    from pathlib import Path
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    
    print("=" * 60)
    print("ğŸ“š æ–‡æ¡£å¤„ç†å™¨æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–è®¾ç½®å’Œå¤„ç†å™¨
        from config.settings import Settings
        settings = Settings()
        processor = DocumentProcessor()
        
        # æµ‹è¯•æ–‡ä»¶è·¯å¾„
        test_files_dir = Path("files")
        if not test_files_dir.exists():
            print(f"âŒ æµ‹è¯•ç›®å½•ä¸å­˜åœ¨: {test_files_dir}")
            return
        
        # è·å–æµ‹è¯•æ–‡ä»¶
        test_files = list(test_files_dir.glob("*"))
        if not test_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•æ–‡ä»¶")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶:")
        for i, file in enumerate(test_files, 1):
            file_size = file.stat().st_size / 1024
            print(f"   {i}. {file.name} ({file_size:.1f} KB)")
        
        print("\n" + "=" * 40)
        print("ğŸ”§ å¼€å§‹åŠŸèƒ½æµ‹è¯•...")
        print("=" * 40)
        
        # 1. æµ‹è¯•å•ä¸ªæ–‡ä»¶å¤„ç†
        print("\nğŸ“„ 1. æµ‹è¯•å•ä¸ªæ–‡ä»¶å¤„ç†")
        print("-" * 30)
        
        test_file = test_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶
        print(f"å¤„ç†æ–‡ä»¶: {test_file.name}")
        
        try:
            documents = processor.process_uploaded_file(test_file)
            print(f"âœ… æˆåŠŸå¤„ç†ï¼æå–äº† {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            
            if documents:
                # æ˜¾ç¤ºç¬¬ä¸€ä¸ªç‰‡æ®µçš„ä¿¡æ¯
                first_doc = documents[0]
                content_preview = first_doc.page_content[:100] + "..." if len(first_doc.page_content) > 100 else first_doc.page_content
                print(f"   ç¬¬ä¸€ä¸ªç‰‡æ®µé¢„è§ˆ: {content_preview}")
                print(f"   ç‰‡æ®µé•¿åº¦: {len(first_doc.page_content)} å­—ç¬¦")
                print(f"   å…ƒæ•°æ®: {first_doc.metadata}")
                
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
        
        # 2. æµ‹è¯•æ–‡æ¡£åˆ†å‰²
        print("\nâœ‚ï¸ 2. æµ‹è¯•æ–‡æ¡£åˆ†å‰²åŠŸèƒ½")
        print("-" * 30)
        
        if 'documents' in locals() and documents:
            try:
                # æµ‹è¯•ä¸åŒçš„åˆ†å‰²å‚æ•°
                test_params = [
                    (500, 50),
                    (1000, 100)
                ]
                
                for chunk_size, overlap in test_params:
                    split_docs = processor.split_documents(documents, chunk_size, overlap)
                    print(f"   chunk_size={chunk_size}, overlap={overlap}: {len(split_docs)} ä¸ªç‰‡æ®µ")
                    
                    if split_docs:
                        avg_size = sum(len(doc.page_content) for doc in split_docs) / len(split_docs)
                        print(f"   å¹³å‡ç‰‡æ®µå¤§å°: {avg_size:.0f} å­—ç¬¦")
                        
            except Exception as e:
                print(f"âŒ åˆ†å‰²å¤±è´¥: {str(e)}")
                import traceback
                traceback.print_exc()
        
        # 3. æµ‹è¯•æ‰¹é‡å¤„ç†
        print("\nğŸ“¦ 3. æµ‹è¯•æ‰¹é‡å¤„ç†")
        print("-" * 30)
        
        try:
            # æ¨¡æ‹ŸStreamlitçš„UploadedFileå¯¹è±¡
            class MockUploadedFile:
                def __init__(self, file_path):
                    self.name = file_path.name
                    self.type = self._get_file_type(file_path)
                    self.size = file_path.stat().st_size
                    self._file_path = file_path
                    
                def _get_file_type(self, file_path):
                    suffix = file_path.suffix.lower()
                    type_map = {
                        '.pdf': 'application/pdf',
                        '.txt': 'text/plain',
                        '.md': 'text/markdown',
                        '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
                    }
                    return type_map.get(suffix, 'application/octet-stream')
                
                def read(self):
                    return self._file_path.read_bytes()
            
            # åˆ›å»ºæ¨¡æ‹Ÿä¸Šä¼ æ–‡ä»¶åˆ—è¡¨
            mock_files = [MockUploadedFile(f) for f in test_files]
            
            print(f"æ‰¹é‡å¤„ç† {len(mock_files)} ä¸ªæ–‡ä»¶...")
            batch_results = processor.process_documents_batch(mock_files)
            
            print(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆï¼")
            print(f"   æ€»æ–‡ä»¶æ•°: {batch_results['total_files']}")
            print(f"   æˆåŠŸå¤„ç†: {batch_results['processed_files']}")
            print(f"   å¤„ç†å¤±è´¥: {batch_results['failed_files']}")
            print(f"   æ€»æ–‡æ¡£ç‰‡æ®µ: {batch_results['total_documents']}")
            
            # æ˜¾ç¤ºå¤±è´¥æ–‡ä»¶ä¿¡æ¯
            if batch_results['errors']:
                print("\n   å¤±è´¥çš„æ–‡ä»¶:")
                for error in batch_results['errors']:
                    print(f"   - {error['file_name']}: {error['error']}")
                    
        except Exception as e:
            print(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
        
        # 4. æµ‹è¯•ç¼“å­˜åŠŸèƒ½
        print("\nğŸ’¾ 4. æµ‹è¯•ç¼“å­˜åŠŸèƒ½")
        print("-" * 30)
        
        try:
            # è·å–ç¼“å­˜ç»Ÿè®¡
            cache_stats = processor.get_cache_stats()
            print(f"âœ… ç¼“å­˜ç»Ÿè®¡:")
            print(f"   ç¼“å­˜å¯ç”¨: {cache_stats['cache_enabled']}")
            print(f"   ç¼“å­˜æ–‡ä»¶æ•°: {cache_stats['cache_files_count']}")
            print(f"   ç¼“å­˜å¤§å°: {cache_stats['cache_total_size_mb']} MB")
            print(f"   è¿‡æœŸæ—¶é—´: {cache_stats['cache_expire_time']} ç§’")
            
            # æµ‹è¯•å†æ¬¡å¤„ç†ç›¸åŒæ–‡ä»¶ï¼ˆåº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰
            print(f"\n   å†æ¬¡å¤„ç†ç›¸åŒæ–‡ä»¶æµ‹è¯•ç¼“å­˜...")
            start_time = time.time()
            cached_docs = processor.process_uploaded_file(test_files[0])
            cache_time = time.time() - start_time
            
            print(f"   ç¼“å­˜å¤„ç†æ—¶é—´: {cache_time:.3f} ç§’")
            print(f"   ç¼“å­˜æ–‡æ¡£æ•°: {len(cached_docs)}")
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜æµ‹è¯•å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
        
        # 5. æµ‹è¯•æ–‡ä»¶ç±»å‹è¯†åˆ«
        print("\nğŸ” 5. æµ‹è¯•æ–‡ä»¶ç±»å‹å¤„ç†")
        print("-" * 30)
        
        supported_types = ['.pdf', '.txt', '.md', '.docx']
        success_count = 0
        
        for file_path in test_files:
            file_ext = file_path.suffix.lower()
            if file_ext in supported_types:
                print(f"   {file_path.name} ({file_ext}): ", end="")
                try:
                    docs = processor.process_uploaded_file(file_path)
                    print(f"âœ… æˆåŠŸ ({len(docs)} ç‰‡æ®µ)")
                    success_count += 1
                except Exception as e:
                    print(f"âŒ å¤±è´¥ ({str(e)})")
        
        print(f"\n   æˆåŠŸå¤„ç†: {success_count}/{len(test_files)} ä¸ªæ–‡ä»¶")
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
        print("=" * 60)
        
        # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
        if 'batch_results' in locals():
            print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
            print(f"   å¤„ç†æ–‡ä»¶ç±»å‹: {len(set(f.suffix for f in test_files))}")
            print(f"   æ€»æ–‡æ¡£ç‰‡æ®µ: {batch_results['total_documents']}")
            if batch_results['total_files'] > 0:
                success_rate = (batch_results['processed_files']/batch_results['total_files']*100)
                print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_document_processor()