# ç¬¬04ç« ï¼šFAISSå‘é‡å­˜å‚¨ - æ„å»ºé«˜æ€§èƒ½è¯­ä¹‰æ£€ç´¢å¼•æ“

æœ¬ç« å®ç°å‘é‡å­˜å‚¨æœåŠ¡ï¼Œè¿™æ˜¯RAGç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ã€‚

## ä¸€ã€FAISSç®€ä»‹

FAISSï¼ˆFacebook AI Similarity Searchï¼‰æ˜¯Facebookå¼€æºçš„å‘é‡æ£€ç´¢åº“ï¼Œç‰¹ç‚¹ï¼š
- âš¡ é«˜æ€§èƒ½ï¼šç™¾ä¸‡çº§å‘é‡æ¯«ç§’çº§æ£€ç´¢
- ğŸ’¾ æœ¬åœ°éƒ¨ç½²ï¼šæ— éœ€å¤–éƒ¨æœåŠ¡
- ğŸ”§ åŠŸèƒ½ä¸°å¯Œï¼šæ”¯æŒå¤šç§ç´¢å¼•ç±»å‹

## äºŒã€VectorStoreServiceæ¶æ„

```
VectorStoreService (476è¡Œ)
â”œâ”€â”€ åˆå§‹åŒ–ä¸åŠ è½½ (__init__, load_index)
â”œâ”€â”€ å‘é‡å­˜å‚¨åˆ›å»º (create_vector_store)
â”œâ”€â”€ æ–‡æ¡£æœç´¢ (search, similarity_search_with_threshold)
â”œâ”€â”€ ç´¢å¼•ç®¡ç† (save_index, clear)
â”œâ”€â”€ æ–‡æ¡£ç®¡ç† (add_documents, delete_document)
â”œâ”€â”€ æ–‡æ¡£åˆ†å‰² (split_documents)
â””â”€â”€ ç»Ÿè®¡ä¿¡æ¯ (get_stats, get_document_list)
```

## ä¸‰ã€æ ¸å¿ƒåŠŸèƒ½å®ç°

### å…³é”®æ–¹æ³•è¯´æ˜

1. **create_vector_store()**: åˆ›å»ºFAISSç´¢å¼•
2. **search()**: æ”¯æŒsimilarityå’Œmmrä¸¤ç§æ£€ç´¢ç­–ç•¥  
3. **save_index/load_index()**: ç´¢å¼•æŒä¹…åŒ–
4. **_sanitize_documents()**: æ–‡æ¡£æ¸…æ´—ï¼Œé™åˆ¶é•¿åº¦é¿å…è¶…è¿‡åµŒå…¥æ¨¡å‹é™åˆ¶

### å®Œæ•´ä»£ç 

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ services/vector_store.pyï¼ˆ476è¡Œï¼‰</summary>

```python
import os
import json
import pickle
import logging
from typing import List, Dict, Optional, Any, Tuple
from datetime import datetime
import numpy as np
import faiss
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain.embeddings.base import Embeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from config.settings import Settings
from services.llm_client import UnifiedEmbeddingClient

logger = logging.getLogger(__name__)

class VectorStoreService:
    """å‘é‡å­˜å‚¨æœåŠ¡ç±» - æ”¯æŒ Ollama å’Œåœ¨çº¿ API embedding"""

    def __init__(self):
        self.settings = Settings()
        # ä½¿ç”¨ç»Ÿä¸€çš„åµŒå…¥å®¢æˆ·ç«¯
        self.embedding_client = UnifiedEmbeddingClient()
        self.embeddings = self.embedding_client.get_embeddings()
        self.vector_store = None
        self.documents = []
        self.index_path = None

        logger.info(f"å‘é‡å­˜å‚¨æœåŠ¡åˆå§‹åŒ–æˆåŠŸ - æä¾›å•†: {self.settings.LLM_PROVIDER}, åµŒå…¥æ¨¡å‹: {self.settings.get_embedding_model()}")

        # å°è¯•è‡ªåŠ¨åŠ è½½å·²å­˜åœ¨çš„ç´¢å¼•ï¼Œé¿å…è¿è¡ŒæœŸé—´çŠ¶æ€ä¸¢å¤±
        try:
            idx_path = str(self.settings.VECTOR_STORE_PATH)
            if os.path.exists(idx_path):
                loaded = self.load_index(idx_path)
                if loaded:
                    logger.info("æ£€æµ‹åˆ°å·²æœ‰å‘é‡ç´¢å¼•ï¼Œå·²è‡ªåŠ¨åŠ è½½")
        except Exception as e:
            logger.warning(f"è‡ªåŠ¨åŠ è½½å‘é‡ç´¢å¼•å¤±è´¥: {str(e)}")

    def create_vector_store(self, documents: List[Document]) -> FAISS:
        """åˆ›å»ºå‘é‡å­˜å‚¨"""
        try:
            logger.info(f"åˆ›å»ºå‘é‡å­˜å‚¨ï¼Œæ–‡æ¡£æ•°é‡: {len(documents)}")

            # æ¸…æ´—æ–‡æ¡£ï¼Œé¿å…ç©ºç‰‡æ®µå’Œè¶…é•¿å†…å®¹
            documents = self._sanitize_documents(documents)

            # åˆ›å»ºå‘é‡å­˜å‚¨
            vector_store = FAISS.from_documents(
                documents=documents,
                embedding=self.embeddings
            )

            self.vector_store = vector_store
            self.documents = documents

            logger.info("å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸ")
            return vector_store

        except Exception as e:
            logger.error(f"åˆ›å»ºå‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")
            raise

    def load_index(self, index_path: str) -> bool:
        """åŠ è½½å‘é‡å­˜å‚¨ç´¢å¼•"""
        try:
            logger.info(f"åŠ è½½å‘é‡å­˜å‚¨ç´¢å¼•: {index_path}")

            if not os.path.exists(index_path):
                logger.warning(f"ç´¢å¼•è·¯å¾„ä¸å­˜åœ¨: {index_path}")
                return False

            # åŠ è½½FAISSç´¢å¼•
            self.vector_store = FAISS.load_local(
                index_path,
                self.embeddings,
                allow_dangerous_deserialization=True
            )

            # åŠ è½½æ–‡æ¡£ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            docs_path = f"{index_path}_docs.pkl"
            if os.path.exists(docs_path):
                with open(docs_path, 'rb') as f:
                    self.documents = pickle.load(f)

            self.index_path = index_path
            logger.info("å‘é‡å­˜å‚¨ç´¢å¼•åŠ è½½æˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"åŠ è½½å‘é‡å­˜å‚¨ç´¢å¼•å¤±è´¥: {str(e)}")
            return False

    def save_index(self, index_path: str) -> bool:
        """ä¿å­˜å‘é‡å­˜å‚¨ç´¢å¼•"""
        try:
            if not self.vector_store:
                logger.warning("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜")
                return False

            logger.info(f"ä¿å­˜å‘é‡å­˜å‚¨ç´¢å¼•: {index_path}")

            # ä¿å­˜FAISSç´¢å¼•
            self.vector_store.save_local(index_path)

            # ä¿å­˜æ–‡æ¡£
            docs_path = f"{index_path}_docs.pkl"
            with open(docs_path, 'wb') as f:
                pickle.dump(self.documents, f)

            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                "created_at": datetime.now().isoformat(),
                "documents_count": len(self.documents),
                "embedding_model": self.embedding_model_name,
                "vector_dimension": self.settings.VECTOR_DIMENSION
            }

            metadata_path = f"{index_path}_metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            self.index_path = index_path
            logger.info("å‘é‡å­˜å‚¨ç´¢å¼•ä¿å­˜æˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"ä¿å­˜å‘é‡å­˜å‚¨ç´¢å¼•å¤±è´¥: {str(e)}")
            return False

    def search(
        self,
        query: str,
        top_k: int = 5,
        search_type: str = "similarity",
        score_threshold: float = 0.2
    ) -> List[Dict[str, Any]]:
        """æœç´¢å‘é‡å­˜å‚¨"""
        try:
            if not self.vector_store:
                logger.warning("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
                return []

            logger.info(f"æœç´¢æŸ¥è¯¢: {query}, top_k: {top_k}, search_type: {search_type}")

            if search_type == "similarity":
                # ç›¸ä¼¼åº¦æœç´¢
                results = self.vector_store.similarity_search_with_score(
                    query=query,
                    k=top_k
                )
            elif search_type == "mmr":
                # MMRæœç´¢ï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼‰- ç¤¾åŒºç‰ˆä¸è¿”å›åˆ†æ•°
                mmr_docs = self.vector_store.max_marginal_relevance_search(
                    query=query,
                    k=top_k,
                    fetch_k=top_k * 2
                )
                results = [(doc, 1.0) for doc in mmr_docs]
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æœç´¢ç±»å‹: {search_type}")

            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for doc, score in results:
                if score >= score_threshold:  # è¿‡æ»¤ä½åˆ†ç»“æœ
                    formatted_results.append({
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "score": float(score)
                    })

            # å¦‚æœç›¸ä¼¼åº¦æœç´¢æ²¡æœ‰å‘½ä¸­ï¼Œè‡ªåŠ¨å›é€€åˆ°MMRæé«˜å¬å›
            if not formatted_results and search_type == "similarity":
                mmr_docs = self.vector_store.max_marginal_relevance_search(
                    query=query,
                    k=top_k,
                    fetch_k=top_k * 2
                )
                formatted_results = [{
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "score": 1.0
                } for doc in mmr_docs]

            logger.info(f"æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(formatted_results)} ä¸ªç»“æœ")
            return formatted_results

        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {str(e)}")
            return []

    def add_documents(self, documents: List[Document]) -> bool:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨"""
        try:
            logger.info(f"æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨ï¼Œæ•°é‡: {len(documents)}")

            documents = self._sanitize_documents(documents)

            if not self.vector_store:
                # å¦‚æœå‘é‡å­˜å‚¨ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„
                self.create_vector_store(documents)
            else:
                # æ·»åŠ åˆ°ç°æœ‰å‘é‡å­˜å‚¨
                self.vector_store.add_documents(documents)
                self.documents.extend(documents)

            logger.info("æ–‡æ¡£æ·»åŠ æˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {str(e)}")
            return False

    def _sanitize_documents(self, documents: List[Document]) -> List[Document]:
        """æ¸…æ´—æ–‡æ¡£å†…å®¹ï¼Œè¿‡æ»¤ç©ºå†…å®¹å¹¶è£å‰ªè¿‡é•¿æ–‡æœ¬ä»¥æ»¡è¶³åµŒå…¥é•¿åº¦é™åˆ¶"""
        cleaned: List[Document] = []
        # è¿‘ä¼¼å­—ç¬¦ä¸Šé™ï¼Œé¿å…è¶…è¿‡ 2048 tokenï¼ˆä¸­æ–‡åœºæ™¯ä¸‹å­—ç¬¦ä¸tokenè¿‘ä¼¼ï¼‰
        max_chars = 2000
        for doc in documents:
            content = (doc.page_content or "").strip()
            if not content:
                continue
            if len(content) > max_chars:
                content = content[:max_chars]
            cleaned.append(Document(page_content=content, metadata=doc.metadata))
        logger.info(f"æ¸…æ´—åæ–‡æ¡£æ•°é‡: {len(cleaned)}")
        return cleaned

    def delete_document(self, doc_id: str) -> bool:
        """ä»å‘é‡å­˜å‚¨ä¸­åˆ é™¤æ–‡æ¡£"""
        try:
            logger.info(f"åˆ é™¤æ–‡æ¡£: {doc_id}")

            if not self.vector_store:
                logger.warning("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
                return False

            # FAISSä¸ç›´æ¥æ”¯æŒåˆ é™¤æ“ä½œï¼Œéœ€è¦é‡æ–°åˆ›å»ºç´¢å¼•
            remaining_docs = [
                doc for doc in self.documents
                if doc.metadata.get("id") != doc_id
            ]

            if len(remaining_docs) < len(self.documents):
                self.create_vector_store(remaining_docs)
                logger.info(f"æ–‡æ¡£åˆ é™¤æˆåŠŸ: {doc_id}")
                return True
            else:
                logger.warning(f"æœªæ‰¾åˆ°æ–‡æ¡£: {doc_id}")
                return False

        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")
            return False

    def clear(self):
        """æ¸…ç©ºå‘é‡å­˜å‚¨"""
        try:
            logger.info("æ¸…ç©ºå‘é‡å­˜å‚¨")

            # åŒæ—¶æ¸…ç†ç£ç›˜ç´¢å¼•æ–‡ä»¶ï¼Œé¿å…è‡ªåŠ¨åŠ è½½æ—§ç´¢å¼•
            idx_path = self.index_path or str(self.settings.VECTOR_STORE_PATH)
            try:
                if idx_path and os.path.exists(idx_path):
                    import shutil
                    shutil.rmtree(idx_path, ignore_errors=True)
                # åˆ é™¤é™„åŠ çš„docsä¸metadataæ–‡ä»¶
                docs_path = f"{idx_path}_docs.pkl"
                meta_path = f"{idx_path}_metadata.json"
                if os.path.exists(docs_path):
                    os.remove(docs_path)
                if os.path.exists(meta_path):
                    os.remove(meta_path)
            except Exception as e:
                logger.warning(f"åˆ é™¤ç´¢å¼•æ–‡ä»¶å¤±è´¥: {str(e)}")

            # æ¸…ç†å†…å­˜çŠ¶æ€
            self.vector_store = None
            self.documents = []
            self.index_path = None

            logger.info("å‘é‡å­˜å‚¨å·²æ¸…ç©ºï¼ˆå†…å­˜ä¸ç£ç›˜ï¼‰")

        except Exception as e:
            logger.error(f"æ¸…ç©ºå‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")

    def get_stats(self) -> Dict[str, Any]:
        """è·å–å‘é‡å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {
                "documents_count": len(self.documents),
                "vector_store_initialized": self.vector_store is not None,
                "embedding_model": self.embedding_model_name,
                "index_path": self.index_path
            }

            if self.vector_store:
                # è·å–ç´¢å¼•ä¿¡æ¯
                index = self.vector_store.index
                stats.update({
                    "total_vectors": index.ntotal if hasattr(index, 'ntotal') else 0,
                    "dimension": index.d if hasattr(index, 'd') else 0
                })

            return stats

        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {"error": str(e)}

    def get_document_list(self) -> List[Dict[str, Any]]:
        """è·å–çŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£åˆ—è¡¨åŠç»Ÿè®¡ä¿¡æ¯"""
        try:
            docs = self.documents or []
            by_source: Dict[str, Dict[str, Any]] = {}
            for d in docs:
                meta = d.metadata or {}
                src = meta.get("source", "æœªçŸ¥æ¥æº")
                ftype = meta.get("file_type", "")
                by_source.setdefault(src, {"æ–‡ä»¶å": src, "æ–‡ä»¶ç±»å‹": ftype, "ç‰‡æ®µæ•°": 0})
                by_source[src]["ç‰‡æ®µæ•°"] += 1
            # è½¬ä¸ºåˆ—è¡¨å¹¶æŒ‰æ–‡ä»¶åæ’åº
            result = list(by_source.values())
            result.sort(key=lambda x: x["æ–‡ä»¶å"].lower())
            return result
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []

    def split_documents(
        self,
        documents: List[Document],
        chunk_size: int = None,
        chunk_overlap: int = None
    ) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£"""
        try:
            chunk_size = chunk_size or self.settings.CHUNK_SIZE
            chunk_overlap = chunk_overlap or self.settings.CHUNK_OVERLAP

            logger.info(f"åˆ†å‰²æ–‡æ¡£ï¼Œchunk_size: {chunk_size}, chunk_overlap: {chunk_overlap}")

            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                length_function=len,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
            )

            split_docs = text_splitter.split_documents(documents)

            # æ·»åŠ å…ƒæ•°æ®
            for i, doc in enumerate(split_docs):
                if "chunk_id" not in doc.metadata:
                    doc.metadata["chunk_id"] = i
                if "chunk_size" not in doc.metadata:
                    doc.metadata["chunk_size"] = len(doc.page_content)

            logger.info(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œç‰‡æ®µæ•°é‡: {len(split_docs)}")
            return split_docs

        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ†å‰²å¤±è´¥: {str(e)}")
            return documents

    def similarity_search_with_threshold(
        self,
        query: str,
        threshold: float = 0.7,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """å¸¦é˜ˆå€¼çš„ç›¸ä¼¼åº¦æœç´¢"""
        try:
            results = self.search(query, top_k * 2, "similarity")  # è·å–æ›´å¤šç»“æœç”¨äºè¿‡æ»¤

            # æŒ‰é˜ˆå€¼è¿‡æ»¤
            filtered_results = [
                result for result in results
                if result["score"] >= threshold
            ]

            # è¿”å›å‰top_kä¸ªç»“æœ
            return filtered_results[:top_k]

        except Exception as e:
            logger.error(f"é˜ˆå€¼æœç´¢å¤±è´¥: {str(e)}")
            return []

    @property
    def embedding_model_name(self) -> str:
        """è·å–åµŒå…¥æ¨¡å‹åç§°"""
        if hasattr(self.embeddings, 'embedding_model_name'):
            return self.embeddings.embedding_model_name
        return self.settings.get_embedding_model()


# ==============================================================================
#  ONLINE-ONLY å¿«é€Ÿæµ‹è¯•
# ==============================================================================
def test_online_only():
    """
    çº¯åœ¨çº¿æ¨¡å¼åŠŸèƒ½æµ‹è¯•
    å¼ºåˆ¶ä½¿ç”¨ online / text-embedding-v1ï¼Œæ— éœ€æœ¬åœ° Ollama
    """
    import os, tempfile, json
    os.environ["LLM_PROVIDER"] = "online"          # é”æ­» online
    os.environ["ONLINE_API_KEY"] = os.getenv(
        "ONLINE_API_KEY",
        "sk-abe3417c96f6441b83efed38708bcfb6"      # é»˜è®¤ Demo key
    )

    print("===  VectorStoreService åœ¨çº¿æ¨¡å‹åŠŸèƒ½æµ‹è¯•  ===")
    print("LLM_PROVIDER =", os.environ["LLM_PROVIDER"])
    print("ONLINE_API_KEY =", os.environ["ONLINE_API_KEY"][:10] + "***")

    try:
        # 1. åˆå§‹åŒ–
        print("\n1. åˆå§‹åŒ–æœåŠ¡ï¼ˆonlineï¼‰...")
        vs = VectorStoreService()
        print("   âœ“ åˆå§‹åŒ–å®Œæˆï¼ŒåµŒå…¥æ¨¡å‹ï¼š", vs.settings.get_embedding_model())

        # 2. æ„é€ æµ‹è¯•æ–‡æ¡£
        docs = [
            Document(page_content="Python æ˜¯ç®€æ´å¼ºå¤§çš„ç¼–ç¨‹è¯­è¨€ã€‚", metadata={"id": "1"}),
            Document(page_content="æœºå™¨å­¦ä¹ æ— éœ€æ˜¾å¼ç¼–ç¨‹å³å¯å­¦ä¹ ã€‚", metadata={"id": "2"}),
            Document(page_content="å‘é‡æ•°æ®åº“æ”¯æ’‘è¯­ä¹‰æœç´¢ã€‚", metadata={"id": "3"}),
        ]

        # 3. åˆ›å»ºç´¢å¼•
        print("\n2. åˆ›å»ºå‘é‡ç´¢å¼•...")
        vs.create_vector_store(docs)
        print("   âœ“ ç´¢å¼•åˆ›å»ºæˆåŠŸï¼Œæ–‡æ¡£æ•°ï¼š", len(vs.documents))

        # 4. æœç´¢æµ‹è¯•
        print("\n3. ç›¸ä¼¼åº¦æœç´¢...")
        res = vs.search("Python è¯­è¨€ç‰¹ç‚¹", top_k=2, score_threshold=0.3)
        print("   è¿”å›ç»“æœæ•°ï¼š", len(res))
        for r in res:
            print("   - score={:.3f}, content={}".format(r["score"], r["content"][:60]))

        print("\n4. MMR æœç´¢...")
        mmr = vs.search("æœºå™¨å­¦ä¹ ", top_k=2, search_type="mmr", score_threshold=0.2)
        print("   MMR ç»“æœæ•°ï¼š", len(mmr))

        # 5. ä¿å­˜ / åŠ è½½
        print("\n5. ä¿å­˜ & åŠ è½½ç´¢å¼•...")
        with tempfile.TemporaryDirectory() as tmp:
            idx_path = os.path.join(tmp, "online_index")
            assert vs.save_index(idx_path), "ä¿å­˜å¤±è´¥"
            print("   âœ“ ä¿å­˜æˆåŠŸ")

            vs2 = VectorStoreService()              # æ–°å®ä¾‹
            assert vs2.load_index(idx_path), "åŠ è½½å¤±è´¥"
            print("   âœ“ åŠ è½½æˆåŠŸï¼Œæ–‡æ¡£æ•°ï¼š", len(vs2.documents))

        # 6. ç»Ÿè®¡ & æ¸…ç©º
        print("\n6. ç»Ÿè®¡ä¿¡æ¯ï¼š", vs.get_stats())
        vs.clear()
        print("   âœ“ å·²æ¸…ç©ºï¼Œæ–‡æ¡£æ•°ï¼š", len(vs.documents))

        print("\n===  åœ¨çº¿æ¨¡å‹æµ‹è¯•å…¨éƒ¨é€šè¿‡  ===")
        return True

    except Exception as e:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼š", e)
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œå³èµ° online æµ‹è¯•
    test_online_only()
```

</details>

## å››ã€æœ¬ç« æ€»ç»“

âœ… å®ç°äº†å®Œæ•´çš„FAISSå‘é‡å­˜å‚¨æœåŠ¡
âœ… æ”¯æŒsimilarityå’Œmmrä¸¤ç§æ£€ç´¢ç­–ç•¥
âœ… å®ç°äº†ç´¢å¼•æŒä¹…åŒ–å’Œè‡ªåŠ¨åŠ è½½
âœ… æ–‡æ¡£æ¸…æ´—é¿å…è¶…é•¿å†…å®¹

**ä¸‹èŠ‚é¢„å‘Š**ï¼šç¬¬05ç« å°†å®ç°è£…é¥°å™¨å’Œæ–‡æ¡£å¤„ç†å™¨ç­‰è¾…åŠ©å·¥å…·ç±»ã€‚
