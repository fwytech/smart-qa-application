# ç¬¬02ç« ï¼šé…ç½®ä¸­å¿ƒ - åŒæ¨¡å¼LLMçš„ç»Ÿä¸€ç®¡ç†ä¸çµæ´»åˆ‡æ¢

åœ¨ä¸Šä¸€ç« ï¼Œæˆ‘ä»¬å®Œæˆäº†é¡¹ç›®ç¯å¢ƒçš„æ­å»ºã€‚ç°åœ¨æˆ‘ä»¬è¦ç¼–å†™ç¬¬ä¸€ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š**é…ç½®ä¸­å¿ƒï¼ˆSettingsï¼‰**ã€‚

é…ç½®ä¸­å¿ƒå°±åƒæ˜¯æ•´ä¸ªç³»ç»Ÿçš„"å¤§è„‘"ï¼Œæ‰€æœ‰æ¨¡å—éƒ½éœ€è¦ä»è¿™é‡Œè·å–é…ç½®ä¿¡æ¯ã€‚ä¸€ä¸ªè®¾è®¡è‰¯å¥½çš„é…ç½®ä¸­å¿ƒï¼Œèƒ½è®©ç³»ç»Ÿåœ¨æœ¬åœ°å¼€å‘å’Œç”Ÿäº§ç¯å¢ƒä¹‹é—´æ— ç¼åˆ‡æ¢ã€‚

## ä¸€ã€ä¸ºä»€ä¹ˆéœ€è¦é…ç½®ä¸­å¿ƒï¼Ÿ

### 1.1 æ²¡æœ‰é…ç½®ä¸­å¿ƒçš„ç—›è‹¦

æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœæ¯ä¸ªæ¨¡å—éƒ½è‡ªå·±ç®¡ç†é…ç½®ï¼š

```python
# vector_store.py ä¸­ç¡¬ç¼–ç 
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200

# llm_client.py ä¸­ç¡¬ç¼–ç 
DEFAULT_TEMPERATURE = 0.7
DEFAULT_MAX_TOKENS = 2048

# app.py ä¸­ç¡¬ç¼–ç 
API_KEY = "sk-xxx"  # ğŸ˜± APIå¯†é’¥æ³„éœ²ï¼
```

**å­˜åœ¨çš„é—®é¢˜**ï¼š
- âŒ é…ç½®åˆ†æ•£åœ¨å„å¤„ï¼Œéš¾ä»¥ç»´æŠ¤
- âŒ ä¿®æ”¹å‚æ•°éœ€è¦æ”¹å¤šä¸ªæ–‡ä»¶
- âŒ APIå¯†é’¥ç¡¬ç¼–ç åœ¨ä»£ç ä¸­ï¼Œä¸å®‰å…¨
- âŒ æœ¬åœ°å¼€å‘å’Œç”Ÿäº§ç¯å¢ƒæ— æ³•çµæ´»åˆ‡æ¢

### 1.2 é…ç½®ä¸­å¿ƒçš„ä»·å€¼

ä½¿ç”¨ç»Ÿä¸€çš„é…ç½®ä¸­å¿ƒåï¼š

```python
# æ‰€æœ‰æ¨¡å—éƒ½ä»é…ç½®ä¸­å¿ƒè·å–
from config.settings import Settings

settings = Settings()
chunk_size = settings.CHUNK_SIZE  # ç»Ÿä¸€ç®¡ç†
api_key = settings.ONLINE_API_KEY  # ä»ç¯å¢ƒå˜é‡è¯»å–
```

**å¸¦æ¥çš„å¥½å¤„**ï¼š
- âœ… **é›†ä¸­ç®¡ç†**ï¼šæ‰€æœ‰é…ç½®åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­
- âœ… **ç¯å¢ƒéš”ç¦»**ï¼šå¼€å‘/æµ‹è¯•/ç”Ÿäº§é…ç½®åˆ†ç¦»
- âœ… **å®‰å…¨æ€§**ï¼šæ•æ„Ÿä¿¡æ¯ä»ç¯å¢ƒå˜é‡è¯»å–
- âœ… **çµæ´»åˆ‡æ¢**ï¼šä¸€è¡Œä»£ç åˆ‡æ¢Ollamaå’Œåœ¨çº¿API
- âœ… **å¯è¿½æº¯**ï¼šé…ç½®å˜æ›´æœ‰è®°å½•

## äºŒã€åŒæ¨¡å¼LLMåœºæ™¯å¯¹æ¯”

æˆ‘ä»¬çš„ç³»ç»Ÿæ”¯æŒä¸¤ç§LLMæ¨¡å¼ï¼Œç†è§£å®ƒä»¬çš„å·®å¼‚å¾ˆé‡è¦ï¼š

### 2.1 åœºæ™¯å¯¹æ¯”è¡¨

| ä½¿ç”¨åœºæ™¯ | æ¨èæ¨¡å¼ | åŸå›  |
|---------|----------|------|
| **å¼€å‘è°ƒè¯•** | Ollamaæœ¬åœ° | å…è´¹ã€å“åº”å¿«ã€å¯ç¦»çº¿ |
| **ç”Ÿäº§ç¯å¢ƒ** | åœ¨çº¿API | ç¨³å®šã€é«˜å¹¶å‘ã€æ— éœ€ç»´æŠ¤ |
| **æ•æ„Ÿæ•°æ®** | Ollamaæœ¬åœ° | æ•°æ®ä¸å‡ºæœ¬åœ°ï¼Œç¬¦åˆåˆè§„ |
| **é«˜å¹¶å‘æœåŠ¡** | åœ¨çº¿API | äº‘ç«¯å¼¹æ€§æ‰©å®¹ï¼Œæ— ç¡¬ä»¶é™åˆ¶ |
| **æ¼”ç¤ºDemo** | Ollamaæœ¬åœ° | ä¸ä¾èµ–ç½‘ç»œï¼Œæˆæœ¬å¯æ§ |
| **ä¼ä¸šå†…ç½‘** | Ollamaæœ¬åœ° | å†…ç½‘éƒ¨ç½²ï¼Œæ— å¤–ç½‘ä¾èµ– |

### 2.2 åŒæ¨¡å¼å·¥ä½œæµç¨‹

```mermaid
graph LR
    A[ç³»ç»Ÿå¯åŠ¨] --> B{è¯»å–LLM_PROVIDER}
    B -->|ollama| C[Ollamaæœ¬åœ°æ¨¡å¼]
    B -->|online| D[åœ¨çº¿APIæ¨¡å¼]
    C --> E[åŠ è½½Ollamaé…ç½®]
    D --> F[åŠ è½½åœ¨çº¿APIé…ç½®]
    E --> G[åˆå§‹åŒ–LLMå®¢æˆ·ç«¯]
    F --> G
    G --> H[ç³»ç»Ÿå°±ç»ª]
```

**åˆ‡æ¢åªéœ€ä¸€è¡Œ**ï¼š

```bash
# åˆ‡æ¢åˆ°æœ¬åœ°æ¨¡å¼
LLM_PROVIDER=ollama

# åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼
LLM_PROVIDER=online
```

## ä¸‰ã€Settingsé…ç½®ç±»è®¾è®¡æ€æƒ³

### 3.1 è®¾è®¡åŸåˆ™

æˆ‘ä»¬çš„Settingsç±»éµå¾ªä»¥ä¸‹è®¾è®¡åŸåˆ™ï¼š

1. **å•ä¸€èŒè´£**ï¼šåªè´Ÿè´£é…ç½®ç®¡ç†ï¼Œä¸åšä¸šåŠ¡é€»è¾‘
2. **é…ç½®åˆ†å±‚**ï¼šåŸºç¡€é…ç½® + Ollamaé…ç½® + åœ¨çº¿APIé…ç½®
3. **ç¯å¢ƒé©±åŠ¨**ï¼šä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå…¶æ¬¡ä½¿ç”¨é»˜è®¤å€¼
4. **ç±»æ–¹æ³•**ï¼šé…ç½®è·å–ä½¿ç”¨@classmethodï¼Œæ— éœ€å®ä¾‹åŒ–
5. **ç±»å‹å®‰å…¨**ï¼šæ˜ç¡®çš„ç±»å‹æ³¨è§£å’Œè¿”å›å€¼

### 3.2 é…ç½®åˆ†ç±»

```
Settingsé…ç½®ç±»
â”œâ”€â”€ åŸºç¡€è·¯å¾„é…ç½®
â”‚   â”œâ”€â”€ DATA_DIRï¼ˆæ•°æ®ç›®å½•ï¼‰
â”‚   â”œâ”€â”€ VECTOR_STORE_DIRï¼ˆå‘é‡åº“ç›®å½•ï¼‰
â”‚   â”œâ”€â”€ CHAT_HISTORY_DIRï¼ˆèŠå¤©è®°å½•ç›®å½•ï¼‰
â”‚   â””â”€â”€ LOG_DIRï¼ˆæ—¥å¿—ç›®å½•ï¼‰
â”‚
â”œâ”€â”€ LLMæä¾›å•†é…ç½®
â”‚   â”œâ”€â”€ LLM_PROVIDERï¼ˆæä¾›å•†é€‰æ‹©ï¼‰
â”‚   â”œâ”€â”€ Ollamaé…ç½®
â”‚   â”‚   â”œâ”€â”€ OLLAMA_BASE_URL
â”‚   â”‚   â”œâ”€â”€ OLLAMA_MODELS
â”‚   â”‚   â””â”€â”€ OLLAMA_EMBEDDING_MODEL
â”‚   â””â”€â”€ åœ¨çº¿APIé…ç½®
â”‚       â”œâ”€â”€ ONLINE_API_KEY
â”‚       â”œâ”€â”€ ONLINE_BASE_URL
â”‚       â”œâ”€â”€ ONLINE_MODELS
â”‚       â””â”€â”€ ONLINE_EMBEDDING_MODEL
â”‚
â”œâ”€â”€ æ¨¡å‹å‚æ•°é…ç½®
â”‚   â”œâ”€â”€ DEFAULT_TEMPERATUREï¼ˆæ¸©åº¦ç³»æ•°ï¼‰
â”‚   â”œâ”€â”€ DEFAULT_MAX_TOKENSï¼ˆæœ€å¤§Tokenæ•°ï¼‰
â”‚   â”œâ”€â”€ DEFAULT_TOP_Kï¼ˆæ£€ç´¢æ•°é‡ï¼‰
â”‚   â””â”€â”€ DEFAULT_SEARCH_TYPEï¼ˆæœç´¢ç±»å‹ï¼‰
â”‚
â”œâ”€â”€ å‘é‡å­˜å‚¨é…ç½®
â”‚   â”œâ”€â”€ VECTOR_DIMENSIONï¼ˆå‘é‡ç»´åº¦ï¼‰
â”‚   â”œâ”€â”€ CHUNK_SIZEï¼ˆæ–‡æ¡£åˆ†å—å¤§å°ï¼‰
â”‚   â””â”€â”€ CHUNK_OVERLAPï¼ˆåˆ†å—é‡å ï¼‰
â”‚
â””â”€â”€ å…¶ä»–é…ç½®
    â”œâ”€â”€ å¤©æ°”APIé…ç½®
    â”œâ”€â”€ æ–‡æ¡£å¤„ç†é…ç½®
    â”œâ”€â”€ æ—¥å¿—é…ç½®
    â””â”€â”€ ç³»ç»ŸåŠŸèƒ½å¼€å…³
```

## å››ã€ä»£ç å®ç°è¯¦è§£

ç°åœ¨å¼€å§‹ç¼–å†™ `config/settings.py`ã€‚ç”±äºæ–‡ä»¶æœ‰216è¡Œï¼Œæˆ‘ä»¬åˆ†4ä¸ªéƒ¨åˆ†è¯¦è§£ã€‚

### ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€é…ç½®ä¸è·¯å¾„ç®¡ç†

è¿™éƒ¨åˆ†å®šä¹‰äº†é¡¹ç›®çš„åŸºç¡€è·¯å¾„å’Œæ–‡ä»¶å­˜å‚¨ä½ç½®ã€‚

**æŠ€æœ¯é“ºå«**ï¼š
- ä½¿ç”¨ `pathlib.Path` è€Œä¸æ˜¯å­—ç¬¦ä¸²æ‹¼æ¥è·¯å¾„ï¼Œæ›´å®‰å…¨è·¨å¹³å°
- æ‰€æœ‰è·¯å¾„éƒ½åŸºäº `BASE_DIR`ï¼Œæ–¹ä¾¿é¡¹ç›®è¿ç§»
- ä½¿ç”¨ç±»å˜é‡ï¼Œæ‰€æœ‰å®ä¾‹å…±äº«é…ç½®

**æ–‡ä»¶è·¯å¾„**ï¼š`config/settings.py`ï¼ˆç¬¬1-23è¡Œï¼‰

```python
import os
from pathlib import Path
from typing import Dict, List, Any

class Settings:
    """ç³»ç»Ÿé…ç½®ç±»"""

    # åŸºç¡€è·¯å¾„
    BASE_DIR = Path(__file__).parent.parent
    DATA_DIR = BASE_DIR / "data"
    VECTOR_STORE_DIR = BASE_DIR / "vector_store"
    CHAT_HISTORY_DIR = BASE_DIR / "chat_history"
    LOG_DIR = BASE_DIR / "logs"

    # æ–‡ä»¶è·¯å¾„
    VECTOR_STORE_PATH = VECTOR_STORE_DIR / "faiss_index"
    CHAT_HISTORY_PATH = CHAT_HISTORY_DIR / "chat_history.json"
    LOG_FILE = LOG_DIR / "app.log"
```

**ä»£ç è§£æ**ï¼š

1. **BASE_DIR = Path(__file__).parent.parent**
   - `__file__`ï¼šå½“å‰æ–‡ä»¶çš„è·¯å¾„ï¼ˆconfig/settings.pyï¼‰
   - `.parent`ï¼šä¸Šä¸€çº§ç›®å½•ï¼ˆconfig/ï¼‰
   - `.parent.parent`ï¼šå†ä¸Šä¸€çº§ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰

2. **ä½¿ç”¨ / è¿ç®—ç¬¦æ‹¼æ¥è·¯å¾„**
   - `BASE_DIR / "data"` ç­‰ä»·äº `os.path.join(BASE_DIR, "data")`
   - ä½†æ›´ç®€æ´ï¼Œä¸”è·¨å¹³å°å…¼å®¹

3. **ä¸ºä»€ä¹ˆåˆ†ç›®å½•å­˜å‚¨ï¼Ÿ**
   - `data/`ï¼šä¸´æ—¶æ–‡ä»¶ã€ç¼“å­˜
   - `vector_store/`ï¼šFAISSç´¢å¼•
   - `chat_history/`ï¼šèŠå¤©è®°å½•
   - `logs/`ï¼šæ—¥å¿—æ–‡ä»¶
   - æ¸…æ™°çš„ç›®å½•ç»“æ„ä¾¿äºæ•°æ®ç®¡ç†å’Œå¤‡ä»½

### ç¬¬äºŒéƒ¨åˆ†ï¼šåŒæ¨¡å¼LLMé…ç½®

è¿™æ˜¯é…ç½®ä¸­å¿ƒçš„æ ¸å¿ƒï¼Œå®ç°äº†åŒæ¨¡å¼LLMçš„çµæ´»åˆ‡æ¢ã€‚

**æŠ€æœ¯é“ºå«**ï¼š
- ä½¿ç”¨ `os.getenv()` ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
- æä¾›é»˜è®¤å€¼ä½œä¸ºåå¤‡ï¼ˆç¬¬äºŒä¸ªå‚æ•°ï¼‰
- æ¨¡å‹åˆ—è¡¨ç”¨åˆ—è¡¨å­˜å‚¨ï¼Œæ–¹ä¾¿UIå±•ç¤º

**æ–‡ä»¶è·¯å¾„**ï¼š`config/settings.py`ï¼ˆç¬¬20-78è¡Œï¼‰

```python
    # ==================== LLM æä¾›å•†é…ç½® ====================
    # å¯é€‰: "ollama" æˆ– "online"
    LLM_PROVIDER = os.getenv("LLM_PROVIDER", "online")  # é»˜è®¤ä½¿ç”¨ API æ–¹å¼

    # ==================== Ollama é…ç½® ====================
    OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    OLLAMA_MODELS = [
        "qwen:7b",
        "qwen:14b",
        "qwen:32b",
        "llama2:7b",
        "llama2:13b",
        "llama2:70b",
        "mistral:7b",
        "codellama:7b",
        "vicuna:7b",
        "baichuan:7b"
    ]
    OLLAMA_EMBEDDING_MODEL = "nomic-embed-text"

    # ==================== åœ¨çº¿ API é…ç½®ï¼ˆé˜¿é‡Œäº‘ç™¾ç‚¼ï¼‰ ====================
    ONLINE_API_KEY = os.getenv("ONLINE_API_KEY", "sk-abe3417c96f6441b83efed38708bcfb6")
    ONLINE_BASE_URL = os.getenv("ONLINE_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")
    ONLINE_MODELS = [
        "qwen-plus",
        "qwen-turbo",
        "qwen-max",
        "qwen-max-longcontext"
    ]
    ONLINE_EMBEDDING_MODEL = "text-embedding-v1"  # é˜¿é‡Œäº‘ç™¾ç‚¼çš„åµŒå…¥æ¨¡å‹

    # ==================== é€šç”¨æ¨¡å‹é…ç½® ====================
    @classmethod
    def get_available_models(cls) -> List[str]:
        """æ ¹æ® LLM æä¾›å•†è¿”å›å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        provider = os.getenv("LLM_PROVIDER", cls.LLM_PROVIDER)
        if provider == "ollama":
            return cls.OLLAMA_MODELS
        else:  # online
            return cls.ONLINE_MODELS

    @classmethod
    def get_default_model(cls) -> str:
        """è·å–é»˜è®¤æ¨¡å‹"""
        provider = os.getenv("LLM_PROVIDER", cls.LLM_PROVIDER)
        if provider == "ollama":
            return "qwen:7b"
        else:  # online
            return "qwen-plus"

    @classmethod
    def get_embedding_model(cls) -> str:
        """è·å–åµŒå…¥æ¨¡å‹"""
        provider = os.getenv("LLM_PROVIDER", cls.LLM_PROVIDER)
        if provider == "ollama":
            return cls.OLLAMA_EMBEDDING_MODEL
        else:  # online
            return cls.ONLINE_EMBEDDING_MODEL
```

**ä»£ç è§£æ**ï¼š

1. **LLM_PROVIDER ç¯å¢ƒå˜é‡**
   ```python
   LLM_PROVIDER = os.getenv("LLM_PROVIDER", "online")
   ```
   - ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œå¦‚æœæœªè®¾ç½®åˆ™é»˜è®¤ä¸º "online"
   - è¿™ä¸ªå€¼å†³å®šäº†æ•´ä¸ªç³»ç»Ÿä½¿ç”¨å“ªç§æ¨¡å¼

2. **Ollamaæ¨¡å‹åˆ—è¡¨**
   ```python
   OLLAMA_MODELS = ["qwen:7b", "qwen:14b", ...]
   ```
   - æ”¯æŒå¤šç§å¼€æºæ¨¡å‹
   - qwenç³»åˆ—ï¼šä¸­æ–‡æ•ˆæœå¥½
   - llama2ï¼šè‹±æ–‡æ•ˆæœå¥½
   - ç”¨æˆ·å¯ä»¥åœ¨UIä¸­åˆ‡æ¢

3. **ä¸ºä»€ä¹ˆä½¿ç”¨ @classmethodï¼Ÿ**
   ```python
   @classmethod
   def get_available_models(cls) -> List[str]:
   ```
   - ä¸éœ€è¦åˆ›å»ºå®ä¾‹å°±èƒ½è°ƒç”¨ï¼š`Settings.get_available_models()`
   - é…ç½®è·å–æ˜¯é™æ€çš„ï¼Œä¸éœ€è¦å®ä¾‹çŠ¶æ€
   - æ›´ç¬¦åˆé…ç½®ç±»çš„è¯­ä¹‰

4. **è¿è¡Œæ—¶è¯»å–ç¯å¢ƒå˜é‡**
   ```python
   provider = os.getenv("LLM_PROVIDER", cls.LLM_PROVIDER)
   ```
   - æ¯æ¬¡è°ƒç”¨éƒ½é‡æ–°è¯»å–ï¼Œæ”¯æŒè¿è¡Œæ—¶åˆ‡æ¢
   - å¦‚æœç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œä½¿ç”¨ç±»å±æ€§çš„é»˜è®¤å€¼

### ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¨¡å‹å‚æ•°ä¸å‘é‡é…ç½®

è¿™éƒ¨åˆ†å®šä¹‰äº†æ¨¡å‹çš„è¿è¡Œå‚æ•°å’Œå‘é‡å­˜å‚¨çš„é…ç½®ã€‚

**æŠ€æœ¯é“ºå«**ï¼š
- Temperatureï¼ˆæ¸©åº¦ç³»æ•°ï¼‰ï¼šæ§åˆ¶å›ç­”çš„éšæœºæ€§ï¼Œ0=ç¡®å®šï¼Œ1=éšæœº
- Max Tokensï¼šå•æ¬¡å›ç­”çš„æœ€å¤§é•¿åº¦
- Top-Kï¼šæ£€ç´¢æ—¶è¿”å›çš„æ–‡æ¡£æ•°é‡
- FAISSå‘é‡ç»´åº¦å¿…é¡»åŒ¹é…åµŒå…¥æ¨¡å‹

**æ–‡ä»¶è·¯å¾„**ï¼š`config/settings.py`ï¼ˆç¬¬79-115è¡Œï¼‰

```python
    DEFAULT_TEMPERATURE = 0.7
    DEFAULT_MAX_TOKENS = 2048
    DEFAULT_TOP_K = 3
    DEFAULT_SEARCH_TYPE = "similarity"  # similarity æˆ– mmr

    # å‘é‡å­˜å‚¨é…ç½®
    VECTOR_DIMENSION = 768  # Ollama nomic-embed-text å’Œé˜¿é‡Œäº‘ text-embedding-v1 éƒ½æ˜¯ 768 ç»´
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 200

    # å¤©æ°”APIé…ç½®
    WEATHER_API_KEY = os.getenv("WEATHER_API_KEY", "73053d990f2e27ad6e600344eee77866")  # è¯·æ›¿æ¢ä¸ºå®é™…çš„APIå¯†é’¥
    WEATHER_API_URL = "https://restapi.amap.com/v3/weather/weatherInfo"
    WEATHER_CITY_URL = "https://restapi.amap.com/v3/config/district"

    # æ–‡æ¡£å¤„ç†é…ç½®
    SUPPORTED_FILE_TYPES = [
        ".pdf",
        ".txt",
        ".md",
        ".docx"
    ]

    MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB
    CACHE_ENABLED = True
    CACHE_EXPIRE_TIME = 3600  # 1å°æ—¶

    # æ˜¾ç¤ºé…ç½®
    MAX_CHAT_HISTORY_DISPLAY = 100
    MESSAGE_TRUNCATE_LENGTH = 500

    # æ—¥å¿—é…ç½®
    LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
    LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    LOG_MAX_BYTES = 10 * 1024 * 1024  # 10MB
    LOG_BACKUP_COUNT = 5

    # ç³»ç»Ÿé…ç½®
    ENABLE_FILE_CACHE = True
    ENABLE_CHAT_HISTORY = True
    ENABLE_VECTOR_STORE = True
    ENABLE_WEATHER_TOOL = True
    ENABLE_DOCUMENT_TOOL = True
```

**ä»£ç è§£æ**ï¼š

1. **DEFAULT_TEMPERATURE = 0.7**
   - æ¸©åº¦ç³»æ•°æ§åˆ¶LLMçš„éšæœºæ€§
   - 0.0ï¼šå®Œå…¨ç¡®å®šæ€§ï¼Œå›ç­”ä¸€è‡´
   - 0.7ï¼šå¹³è¡¡éšæœºæ€§å’Œä¸€è‡´æ€§ï¼ˆæ¨èï¼‰
   - 1.0ï¼šæœ€å¤§éšæœºæ€§ï¼Œå›ç­”å¤šæ ·

2. **VECTOR_DIMENSION = 768**
   - åµŒå…¥å‘é‡çš„ç»´åº¦
   - nomic-embed-textï¼ˆOllamaï¼‰ï¼š768ç»´
   - text-embedding-v1ï¼ˆé˜¿é‡Œäº‘ï¼‰ï¼š768ç»´
   - ç»´åº¦å¿…é¡»åŒ¹é…ï¼Œå¦åˆ™æ— æ³•æ£€ç´¢

3. **CHUNK_SIZE = 1000, CHUNK_OVERLAP = 200**
   - æ–‡æ¡£åˆ†å—ç­–ç•¥
   - CHUNK_SIZEï¼šæ¯ä¸ªå—1000å­—ç¬¦
   - CHUNK_OVERLAPï¼šå—ä¹‹é—´é‡å 200å­—ç¬¦
   - é‡å ä¿è¯è¯­ä¹‰è¿ç»­æ€§

4. **MAX_FILE_SIZE = 50 * 1024 * 1024**
   - é™åˆ¶ä¸Šä¼ æ–‡ä»¶å¤§å°ä¸º50MB
   - é˜²æ­¢å†…å­˜æº¢å‡º
   - å®é™…ç”Ÿäº§ç¯å¢ƒå¯èƒ½éœ€è¦æ›´å¤§

5. **ç³»ç»ŸåŠŸèƒ½å¼€å…³**
   ```python
   ENABLE_FILE_CACHE = True
   ENABLE_WEATHER_TOOL = True
   ```
   - é€šè¿‡é…ç½®å¼€å…³åŠŸèƒ½
   - ä¾¿äºè°ƒè¯•å’ŒåŠŸèƒ½è£å‰ª

### ç¬¬å››éƒ¨åˆ†ï¼šé«˜çº§é…ç½®æ–¹æ³•

è¿™éƒ¨åˆ†å®ç°äº†ä¸€äº›é«˜çº§çš„é…ç½®è·å–æ–¹æ³•ï¼Œæä¾›æ›´çµæ´»çš„é…ç½®ç®¡ç†ã€‚

**æŠ€æœ¯é“ºå«**ï¼š
- `get_model_config()`ï¼šæ ¹æ®æ¨¡å‹åç§°è¿”å›å¯¹åº”çš„é…ç½®å­—å…¸
- `get_provider_info()`ï¼šè¿”å›å½“å‰æä¾›å•†çš„è¯¦ç»†ä¿¡æ¯
- `initialize_directories()`ï¼šåˆå§‹åŒ–å¿…è¦çš„ç›®å½•

**æ–‡ä»¶è·¯å¾„**ï¼š`config/settings.py`ï¼ˆç¬¬116-216è¡Œï¼‰

```python
    @classmethod
    def initialize_directories(cls):
        """åˆå§‹åŒ–å¿…è¦çš„ç›®å½•"""
        directories = [
            cls.DATA_DIR,
            cls.VECTOR_STORE_DIR,
            cls.CHAT_HISTORY_DIR,
            cls.LOG_DIR
        ]

        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)

    @classmethod
    def get_model_config(cls, model_name: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹é…ç½®"""
        # Ollama æ¨¡å‹é…ç½®
        ollama_configs = {
            "qwen:7b": {
                "temperature": cls.DEFAULT_TEMPERATURE,
                "max_tokens": cls.DEFAULT_MAX_TOKENS,
                "top_p": 0.9,
                "frequency_penalty": 0.0,
                "presence_penalty": 0.0
            },
            "qwen:14b": {
                "temperature": cls.DEFAULT_TEMPERATURE,
                "max_tokens": cls.DEFAULT_MAX_TOKENS * 2,
                "top_p": 0.9,
                "frequency_penalty": 0.0,
                "presence_penalty": 0.0
            },
            "llama2:7b": {
                "temperature": 0.8,
                "max_tokens": cls.DEFAULT_MAX_TOKENS,
                "top_p": 0.95,
                "frequency_penalty": 0.1,
                "presence_penalty": 0.1
            }
        }

        # åœ¨çº¿ API æ¨¡å‹é…ç½®
        online_configs = {
            "qwen-plus": {
                "temperature": cls.DEFAULT_TEMPERATURE,
                "max_tokens": cls.DEFAULT_MAX_TOKENS,
                "top_p": 0.9,
            },
            "qwen-turbo": {
                "temperature": cls.DEFAULT_TEMPERATURE,
                "max_tokens": cls.DEFAULT_MAX_TOKENS,
                "top_p": 0.9,
            },
            "qwen-max": {
                "temperature": cls.DEFAULT_TEMPERATURE,
                "max_tokens": 4096,
                "top_p": 0.9,
            }
        }

        # æ ¹æ®æä¾›å•†é€‰æ‹©é…ç½®
        provider = os.getenv("LLM_PROVIDER", cls.LLM_PROVIDER)
        if provider == "ollama":
            return ollama_configs.get(model_name, {
                "temperature": cls.DEFAULT_TEMPERATURE,
                "max_tokens": cls.DEFAULT_MAX_TOKENS,
                "top_p": 0.9,
                "frequency_penalty": 0.0,
                "presence_penalty": 0.0
            })
        else:  # online
            return online_configs.get(model_name, {
                "temperature": cls.DEFAULT_TEMPERATURE,
                "max_tokens": cls.DEFAULT_MAX_TOKENS,
                "top_p": 0.9,
            })

    @classmethod
    def get_provider_info(cls) -> Dict[str, str]:
        """è·å–å½“å‰æä¾›å•†ä¿¡æ¯"""
        provider = os.getenv("LLM_PROVIDER", cls.LLM_PROVIDER)
        if provider == "ollama":
            return {
                "provider": "Ollama (æœ¬åœ°)",
                "base_url": cls.OLLAMA_BASE_URL,
                "embedding": cls.OLLAMA_EMBEDDING_MODEL
            }
        else:  # online
            return {
                "provider": "é˜¿é‡Œäº‘ç™¾ç‚¼ (åœ¨çº¿)",
                "base_url": cls.ONLINE_BASE_URL,
                "embedding": cls.ONLINE_EMBEDDING_MODEL
            }
```

**ä»£ç è§£æ**ï¼š

1. **initialize_directories()**
   ```python
   directory.mkdir(parents=True, exist_ok=True)
   ```
   - `parents=True`ï¼šå¦‚æœçˆ¶ç›®å½•ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨åˆ›å»º
   - `exist_ok=True`ï¼šå¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œä¸æŠ¥é”™
   - ç¡®ä¿ç³»ç»Ÿè¿è¡Œå‰æ‰€æœ‰å¿…è¦ç›®å½•å­˜åœ¨

2. **get_model_config()**
   - ä¸ºä¸åŒæ¨¡å‹æä¾›ä¸ªæ€§åŒ–é…ç½®
   - qwen:14b çš„ max_tokens æ˜¯é»˜è®¤çš„2å€ï¼ˆå¤„ç†é•¿æ–‡æœ¬ï¼‰
   - llama2:7b çš„ temperature æ˜¯ 0.8ï¼ˆæ›´éšæœºï¼‰
   - å¦‚æœæ¨¡å‹ä¸åœ¨é…ç½®ä¸­ï¼Œè¿”å›é»˜è®¤é…ç½®

3. **get_provider_info()**
   - è¿”å›å½“å‰æä¾›å•†çš„å¯è¯»ä¿¡æ¯
   - ç”¨äºUIæ˜¾ç¤ºï¼Œè®©ç”¨æˆ·çŸ¥é“å½“å‰æ¨¡å¼
   - åŒ…å«æä¾›å•†åç§°ã€æœåŠ¡åœ°å€ã€åµŒå…¥æ¨¡å‹

## äº”ã€å®Œæ•´ä»£ç æ±‡æ€»

ä»¥ä¸Šæ‰€æœ‰ä»£ç ç‰‡æ®µç»„åˆåœ¨ä¸€èµ·ï¼Œå°±æ˜¯å®Œæ•´çš„ `config/settings.py`ï¼š

<details>
<summary>ç‚¹å‡»å±•å¼€å®Œæ•´ä»£ç ï¼ˆ216è¡Œï¼‰</summary>

**æ–‡ä»¶è·¯å¾„**ï¼š`config/settings.py`

```python
import os
from pathlib import Path
from typing import Dict, List, Any

class Settings:
    """ç³»ç»Ÿé…ç½®ç±»"""

    # åŸºç¡€è·¯å¾„
    BASE_DIR = Path(__file__).parent.parent
    DATA_DIR = BASE_DIR / "data"
    VECTOR_STORE_DIR = BASE_DIR / "vector_store"
    CHAT_HISTORY_DIR = BASE_DIR / "chat_history"
    LOG_DIR = BASE_DIR / "logs"

    # æ–‡ä»¶è·¯å¾„
    VECTOR_STORE_PATH = VECTOR_STORE_DIR / "faiss_index"
    CHAT_HISTORY_PATH = CHAT_HISTORY_DIR / "chat_history.json"
    LOG_FILE = LOG_DIR / "app.log"

    # ==================== LLM æä¾›å•†é…ç½® ====================
    # å¯é€‰: "ollama" æˆ– "online"
    LLM_PROVIDER = os.getenv("LLM_PROVIDER", "online")  # é»˜è®¤ä½¿ç”¨ API æ–¹å¼

    # ==================== Ollama é…ç½® ====================
    OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    OLLAMA_MODELS = [
        "qwen:7b",
        "qwen:14b",
        "qwen:32b",
        "llama2:7b",
        "llama2:13b",
        "llama2:70b",
        "mistral:7b",
        "codellama:7b",
        "vicuna:7b",
        "baichuan:7b"
    ]
    OLLAMA_EMBEDDING_MODEL = "nomic-embed-text"

    # ==================== åœ¨çº¿ API é…ç½®ï¼ˆé˜¿é‡Œäº‘ç™¾ç‚¼ï¼‰ ====================
    ONLINE_API_KEY = os.getenv("ONLINE_API_KEY", "sk-abe3417c96f6441b83efed38708bcfb6")
    ONLINE_BASE_URL = os.getenv("ONLINE_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")
    ONLINE_MODELS = [
        "qwen-plus",
        "qwen-turbo",
        "qwen-max",
        "qwen-max-longcontext"
    ]
    ONLINE_EMBEDDING_MODEL = "text-embedding-v1"  # é˜¿é‡Œäº‘ç™¾ç‚¼çš„åµŒå…¥æ¨¡å‹

    # ==================== é€šç”¨æ¨¡å‹é…ç½® ====================
    @classmethod
    def get_available_models(cls) -> List[str]:
        """æ ¹æ® LLM æä¾›å•†è¿”å›å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        provider = os.getenv("LLM_PROVIDER", cls.LLM_PROVIDER)
        if provider == "ollama":
            return cls.OLLAMA_MODELS
        else:  # online
            return cls.ONLINE_MODELS

    @classmethod
    def get_default_model(cls) -> str:
        """è·å–é»˜è®¤æ¨¡å‹"""
        provider = os.getenv("LLM_PROVIDER", cls.LLM_PROVIDER)
        if provider == "ollama":
            return "qwen:7b"
        else:  # online
            return "qwen-plus"

    @classmethod
    def get_embedding_model(cls) -> str:
        """è·å–åµŒå…¥æ¨¡å‹"""
        provider = os.getenv("LLM_PROVIDER", cls.LLM_PROVIDER)
        if provider == "ollama":
            return cls.OLLAMA_EMBEDDING_MODEL
        else:  # online
            return cls.ONLINE_EMBEDDING_MODEL

    DEFAULT_TEMPERATURE = 0.7
    DEFAULT_MAX_TOKENS = 2048
    DEFAULT_TOP_K = 3
    DEFAULT_SEARCH_TYPE = "similarity"  # similarity æˆ– mmr

    # å‘é‡å­˜å‚¨é…ç½®
    VECTOR_DIMENSION = 768  # Ollama nomic-embed-text å’Œé˜¿é‡Œäº‘ text-embedding-v1 éƒ½æ˜¯ 768 ç»´
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 200

    # å¤©æ°”APIé…ç½®
    WEATHER_API_KEY = os.getenv("WEATHER_API_KEY", "73053d990f2e27ad6e600344eee77866")  # è¯·æ›¿æ¢ä¸ºå®é™…çš„APIå¯†é’¥
    WEATHER_API_URL = "https://restapi.amap.com/v3/weather/weatherInfo"
    WEATHER_CITY_URL = "https://restapi.amap.com/v3/config/district"

    # æ–‡æ¡£å¤„ç†é…ç½®
    SUPPORTED_FILE_TYPES = [
        ".pdf",
        ".txt",
        ".md",
        ".docx"
    ]

    MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB
    CACHE_ENABLED = True
    CACHE_EXPIRE_TIME = 3600  # 1å°æ—¶

    # æ˜¾ç¤ºé…ç½®
    MAX_CHAT_HISTORY_DISPLAY = 100
    MESSAGE_TRUNCATE_LENGTH = 500

    # æ—¥å¿—é…ç½®
    LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
    LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    LOG_MAX_BYTES = 10 * 1024 * 1024  # 10MB
    LOG_BACKUP_COUNT = 5

    # ç³»ç»Ÿé…ç½®
    ENABLE_FILE_CACHE = True
    ENABLE_CHAT_HISTORY = True
    ENABLE_VECTOR_STORE = True
    ENABLE_WEATHER_TOOL = True
    ENABLE_DOCUMENT_TOOL = True

    @classmethod
    def initialize_directories(cls):
        """åˆå§‹åŒ–å¿…è¦çš„ç›®å½•"""
        directories = [
            cls.DATA_DIR,
            cls.VECTOR_STORE_DIR,
            cls.CHAT_HISTORY_DIR,
            cls.LOG_DIR
        ]

        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)

    @classmethod
    def get_model_config(cls, model_name: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹é…ç½®"""
        # Ollama æ¨¡å‹é…ç½®
        ollama_configs = {
            "qwen:7b": {
                "temperature": cls.DEFAULT_TEMPERATURE,
                "max_tokens": cls.DEFAULT_MAX_TOKENS,
                "top_p": 0.9,
                "frequency_penalty": 0.0,
                "presence_penalty": 0.0
            },
            "qwen:14b": {
                "temperature": cls.DEFAULT_TEMPERATURE,
                "max_tokens": cls.DEFAULT_MAX_TOKENS * 2,
                "top_p": 0.9,
                "frequency_penalty": 0.0,
                "presence_penalty": 0.0
            },
            "llama2:7b": {
                "temperature": 0.8,
                "max_tokens": cls.DEFAULT_MAX_TOKENS,
                "top_p": 0.95,
                "frequency_penalty": 0.1,
                "presence_penalty": 0.1
            }
        }

        # åœ¨çº¿ API æ¨¡å‹é…ç½®
        online_configs = {
            "qwen-plus": {
                "temperature": cls.DEFAULT_TEMPERATURE,
                "max_tokens": cls.DEFAULT_MAX_TOKENS,
                "top_p": 0.9,
            },
            "qwen-turbo": {
                "temperature": cls.DEFAULT_TEMPERATURE,
                "max_tokens": cls.DEFAULT_MAX_TOKENS,
                "top_p": 0.9,
            },
            "qwen-max": {
                "temperature": cls.DEFAULT_TEMPERATURE,
                "max_tokens": 4096,
                "top_p": 0.9,
            }
        }

        # æ ¹æ®æä¾›å•†é€‰æ‹©é…ç½®
        provider = os.getenv("LLM_PROVIDER", cls.LLM_PROVIDER)
        if provider == "ollama":
            return ollama_configs.get(model_name, {
                "temperature": cls.DEFAULT_TEMPERATURE,
                "max_tokens": cls.DEFAULT_MAX_TOKENS,
                "top_p": 0.9,
                "frequency_penalty": 0.0,
                "presence_penalty": 0.0
            })
        else:  # online
            return online_configs.get(model_name, {
                "temperature": cls.DEFAULT_TEMPERATURE,
                "max_tokens": cls.DEFAULT_MAX_TOKENS,
                "top_p": 0.9,
            })

    @classmethod
    def get_provider_info(cls) -> Dict[str, str]:
        """è·å–å½“å‰æä¾›å•†ä¿¡æ¯"""
        provider = os.getenv("LLM_PROVIDER", cls.LLM_PROVIDER)
        if provider == "ollama":
            return {
                "provider": "Ollama (æœ¬åœ°)",
                "base_url": cls.OLLAMA_BASE_URL,
                "embedding": cls.OLLAMA_EMBEDDING_MODEL
            }
        else:  # online
            return {
                "provider": "é˜¿é‡Œäº‘ç™¾ç‚¼ (åœ¨çº¿)",
                "base_url": cls.ONLINE_BASE_URL,
                "embedding": cls.ONLINE_EMBEDDING_MODEL
            }
```

</details>

## å…­ã€ä»£ç éªŒè¯

è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæµ‹è¯•è„šæœ¬ï¼ŒéªŒè¯é…ç½®ä¸­å¿ƒæ˜¯å¦æ­£å¸¸å·¥ä½œï¼š

**æ–‡ä»¶è·¯å¾„**ï¼š`test_settings.py`

```python
"""é…ç½®ä¸­å¿ƒæµ‹è¯•è„šæœ¬"""
from config.settings import Settings
import os

print("=" * 60)
print("é…ç½®ä¸­å¿ƒæµ‹è¯•")
print("=" * 60)

# 1. æµ‹è¯•åŸºç¡€è·¯å¾„
print("\n1. åŸºç¡€è·¯å¾„é…ç½®:")
print(f"   BASE_DIR: {Settings.BASE_DIR}")
print(f"   DATA_DIR: {Settings.DATA_DIR}")
print(f"   VECTOR_STORE_DIR: {Settings.VECTOR_STORE_DIR}")

# 2. æµ‹è¯•LLMæä¾›å•†é…ç½®
print("\n2. LLMæä¾›å•†é…ç½®:")
provider_info = Settings.get_provider_info()
print(f"   å½“å‰æä¾›å•†: {provider_info['provider']}")
print(f"   æœåŠ¡åœ°å€: {provider_info['base_url']}")
print(f"   åµŒå…¥æ¨¡å‹: {provider_info['embedding']}")

# 3. æµ‹è¯•æ¨¡å‹åˆ—è¡¨
print("\n3. å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
available_models = Settings.get_available_models()
for i, model in enumerate(available_models, 1):
    print(f"   {i}. {model}")

# 4. æµ‹è¯•é»˜è®¤æ¨¡å‹
print("\n4. é»˜è®¤æ¨¡å‹:")
default_model = Settings.get_default_model()
print(f"   é»˜è®¤æ¨¡å‹: {default_model}")

# 5. æµ‹è¯•æ¨¡å‹é…ç½®
print("\n5. æ¨¡å‹é…ç½®:")
model_config = Settings.get_model_config(default_model)
print(f"   Temperature: {model_config.get('temperature')}")
print(f"   Max Tokens: {model_config.get('max_tokens')}")
print(f"   Top P: {model_config.get('top_p')}")

# 6. æµ‹è¯•å‘é‡å­˜å‚¨é…ç½®
print("\n6. å‘é‡å­˜å‚¨é…ç½®:")
print(f"   å‘é‡ç»´åº¦: {Settings.VECTOR_DIMENSION}")
print(f"   åˆ†å—å¤§å°: {Settings.CHUNK_SIZE}")
print(f"   åˆ†å—é‡å : {Settings.CHUNK_OVERLAP}")

# 7. æµ‹è¯•ç›®å½•åˆå§‹åŒ–
print("\n7. åˆå§‹åŒ–ç›®å½•:")
Settings.initialize_directories()
print("   âœ… ç›®å½•åˆå§‹åŒ–æˆåŠŸ")

# 8. æµ‹è¯•ç¯å¢ƒå˜é‡åˆ‡æ¢ï¼ˆæ¨¡æ‹Ÿï¼‰
print("\n8. æµ‹è¯•æ¨¡å¼åˆ‡æ¢:")
original_provider = os.getenv("LLM_PROVIDER")

# åˆ‡æ¢åˆ° Ollama
os.environ["LLM_PROVIDER"] = "ollama"
print(f"   åˆ‡æ¢åˆ° Ollama:")
print(f"   - é»˜è®¤æ¨¡å‹: {Settings.get_default_model()}")
print(f"   - å¯ç”¨æ¨¡å‹æ•°: {len(Settings.get_available_models())}")

# åˆ‡æ¢åˆ° Online
os.environ["LLM_PROVIDER"] = "online"
print(f"   åˆ‡æ¢åˆ° Online:")
print(f"   - é»˜è®¤æ¨¡å‹: {Settings.get_default_model()}")
print(f"   - å¯ç”¨æ¨¡å‹æ•°: {len(Settings.get_available_models())}")

# æ¢å¤åŸå§‹é…ç½®
if original_provider:
    os.environ["LLM_PROVIDER"] = original_provider

print("\n" + "=" * 60)
print("âœ… é…ç½®ä¸­å¿ƒæµ‹è¯•å®Œæˆï¼")
print("=" * 60)
```

è¿è¡Œæµ‹è¯•ï¼š

```bash
python test_settings.py
```

é¢„æœŸè¾“å‡ºï¼š

```
============================================================
é…ç½®ä¸­å¿ƒæµ‹è¯•
============================================================

1. åŸºç¡€è·¯å¾„é…ç½®:
   BASE_DIR: /path/to/smart-qa-application
   DATA_DIR: /path/to/smart-qa-application/data
   VECTOR_STORE_DIR: /path/to/smart-qa-application/vector_store

2. LLMæä¾›å•†é…ç½®:
   å½“å‰æä¾›å•†: é˜¿é‡Œäº‘ç™¾ç‚¼ (åœ¨çº¿)
   æœåŠ¡åœ°å€: https://dashscope.aliyuncs.com/compatible-mode/v1
   åµŒå…¥æ¨¡å‹: text-embedding-v1

3. å¯ç”¨æ¨¡å‹åˆ—è¡¨:
   1. qwen-plus
   2. qwen-turbo
   3. qwen-max
   4. qwen-max-longcontext

4. é»˜è®¤æ¨¡å‹:
   é»˜è®¤æ¨¡å‹: qwen-plus

5. æ¨¡å‹é…ç½®:
   Temperature: 0.7
   Max Tokens: 2048
   Top P: 0.9

6. å‘é‡å­˜å‚¨é…ç½®:
   å‘é‡ç»´åº¦: 768
   åˆ†å—å¤§å°: 1000
   åˆ†å—é‡å : 200

7. åˆå§‹åŒ–ç›®å½•:
   âœ… ç›®å½•åˆå§‹åŒ–æˆåŠŸ

8. æµ‹è¯•æ¨¡å¼åˆ‡æ¢:
   åˆ‡æ¢åˆ° Ollama:
   - é»˜è®¤æ¨¡å‹: qwen:7b
   - å¯ç”¨æ¨¡å‹æ•°: 10
   åˆ‡æ¢åˆ° Online:
   - é»˜è®¤æ¨¡å‹: qwen-plus
   - å¯ç”¨æ¨¡å‹æ•°: 4

============================================================
âœ… é…ç½®ä¸­å¿ƒæµ‹è¯•å®Œæˆï¼
============================================================
```

## ä¸ƒã€ä¸é¡¹ç›®æºç å¯¹æ¯”éªŒè¯

è®©æˆ‘ä»¬éªŒè¯ç¼–å†™çš„ä»£ç ä¸é¡¹ç›®æºç æ˜¯å¦ä¸€è‡´ï¼š

```bash
# ç»Ÿè®¡æºç è¡Œæ•°
wc -l config/settings.py
```

è¾“å‡ºåº”è¯¥æ˜¯ï¼š**216 è¡Œ**

```bash
# æ£€æŸ¥æ ¸å¿ƒé…ç½®é¡¹
grep "LLM_PROVIDER" config/settings.py
grep "OLLAMA_BASE_URL" config/settings.py
grep "VECTOR_DIMENSION" config/settings.py
```

ç¡®è®¤æ‰€æœ‰é…ç½®é¡¹éƒ½å­˜åœ¨ä¸”ä¸€è‡´ã€‚

## å…«ã€æœ¬ç« æ€»ç»“

åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å®Œæˆäº†é…ç½®ä¸­å¿ƒçš„å®Œæ•´å®ç°ï¼š

1. âœ… ç†è§£äº†é…ç½®ä¸­å¿ƒçš„ä»·å€¼å’Œè®¾è®¡åŸåˆ™
2. âœ… å®ç°äº†åŒæ¨¡å¼LLMé…ç½®ç®¡ç†ï¼ˆOllama + åœ¨çº¿APIï¼‰
3. âœ… ç¼–å†™äº†216è¡Œå®Œæ•´çš„Settingsç±»
4. âœ… å®ç°äº†ç¯å¢ƒé©±åŠ¨çš„é…ç½®åˆ‡æ¢
5. âœ… æä¾›äº†æ¨¡å‹é…ç½®ã€å‘é‡é…ç½®ã€ç³»ç»Ÿé…ç½®ç­‰
6. âœ… é€šè¿‡æµ‹è¯•éªŒè¯äº†é…ç½®ä¸­å¿ƒçš„åŠŸèƒ½

**å…³é”®è¦ç‚¹**ï¼š
- **é›†ä¸­ç®¡ç†**ï¼šæ‰€æœ‰é…ç½®åœ¨ä¸€ä¸ªç±»ä¸­ï¼Œä¾¿äºç»´æŠ¤
- **ç¯å¢ƒé©±åŠ¨**ï¼šé€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶è¡Œä¸ºï¼Œæ— éœ€ä¿®æ”¹ä»£ç 
- **åŒæ¨¡å¼è®¾è®¡**ï¼šOllamaå’Œåœ¨çº¿APIå¯ä¸€é”®åˆ‡æ¢
- **ç±»æ–¹æ³•è®¾è®¡**ï¼šæ— éœ€å®ä¾‹åŒ–å³å¯ä½¿ç”¨
- **å®‰å…¨æ€§**ï¼šæ•æ„Ÿä¿¡æ¯ä»ç¯å¢ƒå˜é‡è¯»å–

## ä¹ã€ä¸‹èŠ‚é¢„å‘Š

**ç¬¬03ç« ï¼šç»Ÿä¸€LLMå®¢æˆ·ç«¯ - Ollamaä¸åœ¨çº¿APIçš„æ— ç¼é›†æˆ**

æˆ‘ä»¬å°†ç¼–å†™ `services/llm_client.py`ï¼ˆ374è¡Œï¼‰ï¼Œå®ç°ï¼š
- UnifiedLLMClientç»Ÿä¸€LLMå®¢æˆ·ç«¯
- UnifiedEmbeddingClientç»Ÿä¸€åµŒå…¥å®¢æˆ·ç«¯
- Ollamaå’ŒOpenAIå…¼å®¹æ¥å£çš„é€‚é…
- è‡ªå®šä¹‰AliyunEmbeddingWrapper
- åŒæ¨¡å¼æ— ç¼åˆ‡æ¢çš„æ ¸å¿ƒé€»è¾‘

é…ç½®ä¸­å¿ƒæ­å»ºå®Œæˆï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬è¦è®©å®ƒçœŸæ­£"åŠ¨èµ·æ¥"ï¼ğŸš€
