# ç¬¬04ç« ï¼šFAISSå‘é‡å­˜å‚¨ - æ„å»ºé«˜æ€§èƒ½è¯­ä¹‰æ£€ç´¢å¼•æ“

> **æœ¬ç« ç›®æ ‡**ï¼šå®ç°åŸºäºFAISSçš„å‘é‡å­˜å‚¨æœåŠ¡ï¼Œè¿™æ˜¯RAGç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£å°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡å¹¶æ”¯æŒé«˜æ•ˆçš„è¯­ä¹‰æ£€ç´¢ã€‚

---

## ä¸€ã€ä¸ºä»€ä¹ˆéœ€è¦å‘é‡å­˜å‚¨ï¼Ÿä¼ ç»Ÿæ£€ç´¢ vs è¯­ä¹‰æ£€ç´¢

åœ¨å®ç°RAGç³»ç»Ÿä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆç†è§£**ä¸ºä»€ä¹ˆéœ€è¦å‘é‡å­˜å‚¨**ã€‚

### 1.1 ä¼ ç»Ÿå…³é”®è¯æ£€ç´¢çš„å±€é™

å‡è®¾çŸ¥è¯†åº“ä¸­æœ‰è¿™æ ·ä¸€æ®µæ–‡æœ¬ï¼š
```
"Python æ˜¯ä¸€ç§è§£é‡Šå‹ã€é¢å‘å¯¹è±¡ã€åŠ¨æ€æ•°æ®ç±»å‹çš„é«˜çº§ç¨‹åºè®¾è®¡è¯­è¨€"
```

**ä¼ ç»Ÿå…³é”®è¯æœç´¢**ï¼ˆå¦‚Elasticsearchï¼‰ï¼š
- ç”¨æˆ·æŸ¥è¯¢ï¼š"Python ç‰¹ç‚¹" â†’ âŒ åŒ¹é…å¤±è´¥ï¼ˆæ²¡æœ‰"ç‰¹ç‚¹"å…³é”®è¯ï¼‰
- ç”¨æˆ·æŸ¥è¯¢ï¼š"Python æ˜¯ä»€ä¹ˆ" â†’ âŒ åŒ¹é…å¤±è´¥ï¼ˆæ²¡æœ‰"æ˜¯ä»€ä¹ˆ"å…³é”®è¯ï¼‰
- ç”¨æˆ·æŸ¥è¯¢ï¼š"Python è¯­è¨€" â†’ âœ… éƒ¨åˆ†åŒ¹é…ï¼ˆæœ‰"Python"å…³é”®è¯ï¼‰

**é—®é¢˜æœ¬è´¨**ï¼šä¼ ç»Ÿæœç´¢åŸºäº**å­—é¢åŒ¹é…**ï¼Œæ— æ³•ç†è§£è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚

### 1.2 å‘é‡æ£€ç´¢å¦‚ä½•è§£å†³é—®é¢˜

**å‘é‡æ£€ç´¢**å°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡ï¼ˆå¦‚768ç»´ï¼‰ï¼Œé€šè¿‡å‘é‡ç›¸ä¼¼åº¦è®¡ç®—æ‰¾åˆ°è¯­ä¹‰ç›¸å…³çš„å†…å®¹ï¼š

```python
# ä¼ªä»£ç ç¤ºæ„
embedding("Python æ˜¯ä¸€ç§è§£é‡Šå‹è¯­è¨€")    â†’ [0.23, -0.45, 0.78, ...] (768ç»´å‘é‡)
embedding("Python ç‰¹ç‚¹")               â†’ [0.21, -0.43, 0.76, ...] (è¯­ä¹‰ç›¸ä¼¼ï¼Œå‘é‡æ¥è¿‘)
embedding("ä»Šå¤©å¤©æ°”ä¸é”™")              â†’ [-0.88, 0.12, -0.33, ...] (è¯­ä¹‰ä¸åŒï¼Œå‘é‡è¿œç¦»)

# é€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
cosine_similarity(vec1, vec2) = 0.95  â† é«˜ç›¸ä¼¼åº¦ï¼
cosine_similarity(vec1, vec3) = 0.12  â† ä½ç›¸ä¼¼åº¦
```

**ä¼˜åŠ¿**ï¼š
- âœ… è¯­ä¹‰ç†è§£ï¼šæŸ¥è¯¢"Python ç‰¹ç‚¹"èƒ½æ‰¾åˆ°"Python æ˜¯ä¸€ç§è§£é‡Šå‹è¯­è¨€"
- âœ… è·¨è¯­è¨€ï¼šè‹±æ–‡æŸ¥è¯¢å¯ä»¥åŒ¹é…ä¸­æ–‡ç­”æ¡ˆï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒå¤šè¯­è¨€ï¼‰
- âœ… åŒä¹‰è¯ï¼šæŸ¥è¯¢"æœºå™¨å­¦ä¹ "èƒ½æ‰¾åˆ°"AI"ã€"äººå·¥æ™ºèƒ½"

---

## äºŒã€FAISSï¼šFacebookå¼€æºçš„å‘é‡æ£€ç´¢åˆ©å™¨

### 2.1 ä»€ä¹ˆæ˜¯FAISSï¼Ÿ

**FAISS** (Facebook AI Similarity Search) æ˜¯Metaå¼€æºçš„å‘é‡ç›¸ä¼¼åº¦æœç´¢åº“ï¼Œä¸“ä¸º**é«˜æ•ˆçš„å‘é‡æ£€ç´¢**è®¾è®¡ã€‚

**æ ¸å¿ƒç‰¹ç‚¹**ï¼š
| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| âš¡ é«˜æ€§èƒ½ | ç™¾ä¸‡çº§å‘é‡æ¯«ç§’çº§æ£€ç´¢ï¼ˆå•æœºCPUå¯å¤„ç†ï¼‰ |
| ğŸ’¾ æœ¬åœ°éƒ¨ç½² | æ— éœ€å¤–éƒ¨æœåŠ¡ï¼ˆå¦‚Pineconeã€Weaviateï¼‰ï¼Œé™ä½æˆæœ¬ |
| ğŸ”§ çµæ´»ç´¢å¼• | æ”¯æŒIndexFlatL2ã€IndexIVFFlatã€HNSWç­‰å¤šç§ç´¢å¼•ç±»å‹ |
| ğŸ“¦ æ˜“äºé›†æˆ | Pythonæ¥å£ç®€å•ï¼ŒLangChainåŸç”Ÿæ”¯æŒ |

### 2.2 FAISS vs å…¶ä»–å‘é‡æ•°æ®åº“

| å¯¹æ¯”é¡¹ | FAISS | Pinecone | Milvus | Weaviate |
|--------|-------|----------|--------|----------|
| **éƒ¨ç½²æ–¹å¼** | æœ¬åœ°æ–‡ä»¶ | äº‘æœåŠ¡ | è‡ªæ‰˜ç®¡/äº‘ | è‡ªæ‰˜ç®¡/äº‘ |
| **æˆæœ¬** | å…è´¹ | æŒ‰é‡ä»˜è´¹ | å…è´¹ï¼ˆéœ€è‡ªå·±è¿ç»´ï¼‰ | å…è´¹ï¼ˆéœ€è‡ªå·±è¿ç»´ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | ä¸­å°è§„æ¨¡ | å¤§è§„æ¨¡åˆ†å¸ƒå¼ | å¤§è§„æ¨¡åˆ†å¸ƒå¼ | å¤§è§„æ¨¡åˆ†å¸ƒå¼ |
| **å­¦ä¹ æˆæœ¬** | ä½ | ä½ | ä¸­ | ä¸­ |

**æˆ‘ä»¬ä¸ºä»€ä¹ˆé€‰æ‹©FAISSï¼Ÿ**
- é¡¹ç›®åˆæœŸæ— éœ€å¤§è§„æ¨¡åˆ†å¸ƒå¼èƒ½åŠ›
- æœ¬åœ°éƒ¨ç½²é™ä½æˆæœ¬å’Œå¤æ‚åº¦
- LangChainç”Ÿæ€å®Œç¾é›†æˆ

### 2.3 FAISSå·¥ä½œåŸç†ï¼ˆç®€åŒ–ç‰ˆï¼‰

```mermaid
graph LR
    A[åŸå§‹æ–‡æ¡£] -->|1. æ–‡æ¡£åˆ†å‰²| B[æ–‡æœ¬ç‰‡æ®µ]
    B -->|2. Embeddingæ¨¡å‹| C[768ç»´å‘é‡]
    C -->|3. FAISSç´¢å¼•| D[å‘é‡ç´¢å¼•æ–‡ä»¶]

    E[ç”¨æˆ·æŸ¥è¯¢] -->|4. Embedding| F[æŸ¥è¯¢å‘é‡]
    F -->|5. ç›¸ä¼¼åº¦æœç´¢| D
    D -->|6. è¿”å›Top-K| G[ç›¸å…³æ–‡æ¡£ç‰‡æ®µ]

    style D fill:#e1f5ff
    style C fill:#fff4e1
    style G fill:#e7f9e7
```

**å…³é”®æ­¥éª¤**ï¼š
1. **æ–‡æ¡£å‘é‡åŒ–**ï¼šå°†çŸ¥è¯†åº“æ–‡æ¡£é€šè¿‡Embeddingæ¨¡å‹è½¬æ¢ä¸ºå‘é‡
2. **æ„å»ºç´¢å¼•**ï¼šFAISSå°†å‘é‡ç»„ç»‡æˆé«˜æ•ˆçš„ç´¢å¼•ç»“æ„ï¼ˆå¦‚èšç±»æ ‘ï¼‰
3. **æŸ¥è¯¢åŒ¹é…**ï¼šç”¨æˆ·æŸ¥è¯¢ä¹Ÿè½¬æ¢ä¸ºå‘é‡ï¼ŒFAISSå¿«é€Ÿæ‰¾åˆ°æœ€ç›¸ä¼¼çš„Kä¸ªå‘é‡
4. **è¿”å›ç»“æœ**ï¼šæ ¹æ®ç›¸ä¼¼åº¦åˆ†æ•°æ’åºï¼Œè¿”å›å¯¹åº”çš„æ–‡æ¡£ç‰‡æ®µ

---

## ä¸‰ã€VectorStoreServiceæ¶æ„è®¾è®¡

æˆ‘ä»¬çš„ `VectorStoreService` ç±»å°è£…äº†å®Œæ•´çš„å‘é‡å­˜å‚¨åŠŸèƒ½ï¼Œæ€»å…±**476è¡Œä»£ç **ï¼Œåˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š

```
VectorStoreService åŠŸèƒ½æ¨¡å—
â”œâ”€â”€ ğŸ—ï¸ åˆå§‹åŒ–ä¸åŠ è½½
â”‚   â”œâ”€â”€ __init__()            - åˆå§‹åŒ–embeddingå®¢æˆ·ç«¯ï¼Œè‡ªåŠ¨åŠ è½½å·²æœ‰ç´¢å¼•
â”‚   â””â”€â”€ load_index()          - ä»ç£ç›˜åŠ è½½FAISSç´¢å¼•
â”‚
â”œâ”€â”€ ğŸ”¨ å‘é‡å­˜å‚¨åˆ›å»º
â”‚   â””â”€â”€ create_vector_store() - ä»æ–‡æ¡£åˆ—è¡¨åˆ›å»ºæ–°çš„FAISSç´¢å¼•
â”‚
â”œâ”€â”€ ğŸ” æ–‡æ¡£æ£€ç´¢
â”‚   â”œâ”€â”€ search()              - æ ¸å¿ƒæœç´¢æ–¹æ³•ï¼ˆæ”¯æŒsimilarityå’Œmmrï¼‰
â”‚   â””â”€â”€ similarity_search_with_threshold() - å¸¦é˜ˆå€¼çš„ç›¸ä¼¼åº¦æœç´¢
â”‚
â”œâ”€â”€ ğŸ’¾ ç´¢å¼•ç®¡ç†
â”‚   â”œâ”€â”€ save_index()          - ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜ï¼ˆå«å…ƒæ•°æ®ï¼‰
â”‚   â””â”€â”€ clear()               - æ¸…ç©ºå†…å­˜å’Œç£ç›˜ç´¢å¼•
â”‚
â”œâ”€â”€ ğŸ“„ æ–‡æ¡£ç®¡ç†
â”‚   â”œâ”€â”€ add_documents()       - å‘ç°æœ‰ç´¢å¼•æ·»åŠ æ–°æ–‡æ¡£
â”‚   â”œâ”€â”€ delete_document()     - åˆ é™¤æŒ‡å®šæ–‡æ¡£ï¼ˆé‡å»ºç´¢å¼•å®ç°ï¼‰
â”‚   â””â”€â”€ split_documents()     - æ–‡æ¡£æ™ºèƒ½åˆ†å‰²
â”‚
â”œâ”€â”€ ğŸ§¹ æ•°æ®æ¸…æ´—
â”‚   â””â”€â”€ _sanitize_documents() - è¿‡æ»¤ç©ºå†…å®¹ã€è£å‰ªè¶…é•¿æ–‡æœ¬
â”‚
â””â”€â”€ ğŸ“Š ç»Ÿè®¡ä¿¡æ¯
    â”œâ”€â”€ get_stats()           - è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
    â””â”€â”€ get_document_list()   - è·å–çŸ¥è¯†åº“æ–‡æ¡£åˆ—è¡¨
```

---

## å››ã€ä»£ç å®ç°è¯¦è§£

> **è¯´æ˜**ï¼šæºä»£ç å…±476è¡Œï¼Œæˆ‘ä»¬å°†å…¶æ‹†åˆ†ä¸º7ä¸ªéƒ¨åˆ†é€ä¸€è®²è§£ï¼Œæ¯éƒ¨åˆ†ä»£ç å‡æ¥è‡ª `services/vector_store.py`ã€‚

### 4.1 ç¬¬ä¸€éƒ¨åˆ†ï¼šå¯¼å…¥æ¨¡å—ä¸åˆå§‹åŒ–ï¼ˆ1-40è¡Œï¼‰

```python
import os
import json
import pickle
import logging
from typing import List, Dict, Optional, Any, Tuple
from datetime import datetime
import numpy as np
import faiss
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain.embeddings.base import Embeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from config.settings import Settings
from services.llm_client import UnifiedEmbeddingClient

logger = logging.getLogger(__name__)

class VectorStoreService:
    """å‘é‡å­˜å‚¨æœåŠ¡ç±» - æ”¯æŒ Ollama å’Œåœ¨çº¿ API embedding"""

    def __init__(self):
        self.settings = Settings()
        # ä½¿ç”¨ç»Ÿä¸€çš„åµŒå…¥å®¢æˆ·ç«¯
        self.embedding_client = UnifiedEmbeddingClient()
        self.embeddings = self.embedding_client.get_embeddings()
        self.vector_store = None
        self.documents = []
        self.index_path = None

        logger.info(f"å‘é‡å­˜å‚¨æœåŠ¡åˆå§‹åŒ–æˆåŠŸ - æä¾›å•†: {self.settings.LLM_PROVIDER}, åµŒå…¥æ¨¡å‹: {self.settings.get_embedding_model()}")

        # å°è¯•è‡ªåŠ¨åŠ è½½å·²å­˜åœ¨çš„ç´¢å¼•ï¼Œé¿å…è¿è¡ŒæœŸé—´çŠ¶æ€ä¸¢å¤±
        try:
            idx_path = str(self.settings.VECTOR_STORE_PATH)
            if os.path.exists(idx_path):
                loaded = self.load_index(idx_path)
                if loaded:
                    logger.info("æ£€æµ‹åˆ°å·²æœ‰å‘é‡ç´¢å¼•ï¼Œå·²è‡ªåŠ¨åŠ è½½")
        except Exception as e:
            logger.warning(f"è‡ªåŠ¨åŠ è½½å‘é‡ç´¢å¼•å¤±è´¥: {str(e)}")
```

**æŠ€æœ¯è¦ç‚¹**ï¼š

1. **ä¾èµ–å¯¼å…¥**ï¼š
   - `faiss`ï¼šåŸç”ŸFAISSåº“ï¼ˆç”¨äºåº•å±‚ç´¢å¼•æ“ä½œï¼‰
   - `langchain_community.vectorstores.FAISS`ï¼šLangChainå°è£…çš„FAISSï¼ˆç®€åŒ–æ¥å£ï¼‰
   - `UnifiedEmbeddingClient`ï¼šç¬¬03ç« å®ç°çš„ç»Ÿä¸€Embeddingå®¢æˆ·ç«¯

2. **åˆå§‹åŒ–é€»è¾‘**ï¼š
   ```python
   self.embeddings = self.embedding_client.get_embeddings()
   ```
   - æ ¹æ® `LLM_PROVIDER` è‡ªåŠ¨é€‰æ‹©Ollamaæˆ–åœ¨çº¿APIçš„Embeddingæ¨¡å‹
   - æ— éœ€æ‰‹åŠ¨åˆ‡æ¢ä»£ç 

3. **è‡ªåŠ¨åŠ è½½æœºåˆ¶**ï¼š
   ```python
   if os.path.exists(idx_path):
       loaded = self.load_index(idx_path)
   ```
   - **ä¸ºä»€ä¹ˆéœ€è¦**ï¼ŸStreamlitåº”ç”¨åœ¨ç”¨æˆ·äº¤äº’æ—¶å¯èƒ½é‡æ–°åˆå§‹åŒ–å¯¹è±¡ï¼Œå¦‚æœä¸è‡ªåŠ¨åŠ è½½å·²æœ‰ç´¢å¼•ï¼Œä¼šå¯¼è‡´"çŸ¥è¯†åº“ä¸¢å¤±"
   - **å®ç°æ–¹å¼**ï¼šæ£€æŸ¥ `VECTOR_STORE_PATH` è·¯å¾„æ˜¯å¦å­˜åœ¨ç´¢å¼•æ–‡ä»¶ï¼Œè‹¥å­˜åœ¨åˆ™è‡ªåŠ¨åŠ è½½

---

### 4.2 ç¬¬äºŒéƒ¨åˆ†ï¼šåˆ›å»ºå‘é‡å­˜å‚¨ï¼ˆ42-103è¡Œï¼‰

```python
    def create_vector_store(self, documents: List[Document]) -> FAISS:
        """åˆ›å»ºå‘é‡å­˜å‚¨"""
        try:
            logger.info(f"åˆ›å»ºå‘é‡å­˜å‚¨ï¼Œæ–‡æ¡£æ•°é‡: {len(documents)}")

            # æ¸…æ´—æ–‡æ¡£ï¼Œé¿å…ç©ºç‰‡æ®µå’Œè¶…é•¿å†…å®¹
            documents = self._sanitize_documents(documents)

            # åˆ›å»ºå‘é‡å­˜å‚¨
            vector_store = FAISS.from_documents(
                documents=documents,
                embedding=self.embeddings
            )

            self.vector_store = vector_store
            self.documents = documents

            logger.info("å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸ")
            return vector_store

        except Exception as e:
            logger.error(f"åˆ›å»ºå‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")
            raise
```

**æ ¸å¿ƒæ–¹æ³•è§£æ**ï¼š

1. **FAISS.from_documents()**ï¼š
   ```python
   vector_store = FAISS.from_documents(
       documents=documents,      # LangChain Documentå¯¹è±¡åˆ—è¡¨
       embedding=self.embeddings # Embeddingæ¨¡å‹å®ä¾‹
   )
   ```
   - LangChainæä¾›çš„ä¾¿æ·æ–¹æ³•ï¼Œå†…éƒ¨è‡ªåŠ¨æ‰§è¡Œï¼š
     1. éå†æ‰€æœ‰æ–‡æ¡£çš„ `page_content`
     2. è°ƒç”¨ `embedding.embed_documents()` æ‰¹é‡è½¬æ¢ä¸ºå‘é‡
     3. åˆ›å»ºFAISSç´¢å¼•ï¼ˆé»˜è®¤ä½¿ç”¨ `IndexFlatL2`ï¼Œå³æš´åŠ›æœç´¢ï¼‰
     4. å°†å‘é‡æ’å…¥ç´¢å¼•

2. **æ–‡æ¡£æ¸…æ´—**ï¼š
   ```python
   documents = self._sanitize_documents(documents)
   ```
   - **å¿…è¦æ€§**ï¼šEmbeddingæ¨¡å‹é€šå¸¸æœ‰é•¿åº¦é™åˆ¶ï¼ˆå¦‚text-embedding-v1é™åˆ¶2048 tokensï¼‰
   - **å®ç°**ï¼šè§ç¬¬6éƒ¨åˆ†è¯¦è§£

3. **çŠ¶æ€ç®¡ç†**ï¼š
   ```python
   self.vector_store = vector_store
   self.documents = documents
   ```
   - ä¿å­˜åˆ°å®ä¾‹å˜é‡ä¾›åç»­æœç´¢/ä¿å­˜ä½¿ç”¨

---

### 4.3 ç¬¬ä¸‰éƒ¨åˆ†ï¼šç´¢å¼•æŒä¹…åŒ–ï¼ˆ105-170è¡Œï¼‰

```python
    def load_index(self, index_path: str) -> bool:
        """åŠ è½½å‘é‡å­˜å‚¨ç´¢å¼•"""
        try:
            logger.info(f"åŠ è½½å‘é‡å­˜å‚¨ç´¢å¼•: {index_path}")

            if not os.path.exists(index_path):
                logger.warning(f"ç´¢å¼•è·¯å¾„ä¸å­˜åœ¨: {index_path}")
                return False

            # åŠ è½½FAISSç´¢å¼•
            self.vector_store = FAISS.load_local(
                index_path,
                self.embeddings,
                allow_dangerous_deserialization=True
            )

            # åŠ è½½æ–‡æ¡£ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            docs_path = f"{index_path}_docs.pkl"
            if os.path.exists(docs_path):
                with open(docs_path, 'rb') as f:
                    self.documents = pickle.load(f)

            self.index_path = index_path
            logger.info("å‘é‡å­˜å‚¨ç´¢å¼•åŠ è½½æˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"åŠ è½½å‘é‡å­˜å‚¨ç´¢å¼•å¤±è´¥: {str(e)}")
            return False

    def save_index(self, index_path: str) -> bool:
        """ä¿å­˜å‘é‡å­˜å‚¨ç´¢å¼•"""
        try:
            if not self.vector_store:
                logger.warning("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜")
                return False

            logger.info(f"ä¿å­˜å‘é‡å­˜å‚¨ç´¢å¼•: {index_path}")

            # ä¿å­˜FAISSç´¢å¼•
            self.vector_store.save_local(index_path)

            # ä¿å­˜æ–‡æ¡£
            docs_path = f"{index_path}_docs.pkl"
            with open(docs_path, 'wb') as f:
                pickle.dump(self.documents, f)

            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                "created_at": datetime.now().isoformat(),
                "documents_count": len(self.documents),
                "embedding_model": self.embedding_model_name,
                "vector_dimension": self.settings.VECTOR_DIMENSION
            }

            metadata_path = f"{index_path}_metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            self.index_path = index_path
            logger.info("å‘é‡å­˜å‚¨ç´¢å¼•ä¿å­˜æˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"ä¿å­˜å‘é‡å­˜å‚¨ç´¢å¼•å¤±è´¥: {str(e)}")
            return False
```

**æŒä¹…åŒ–ç­–ç•¥**ï¼š

1. **ä¸‰æ–‡ä»¶å­˜å‚¨ç»“æ„**ï¼š
   ```
   data/vector_store/              â† index_path
   â”œâ”€â”€ index.faiss                 â† FAISSç´¢å¼•ï¼ˆå‘é‡å’Œç´¢å¼•ç»“æ„ï¼‰
   â”œâ”€â”€ index.pkl                   â† FAISSå…ƒæ•°æ®ï¼ˆdocstoreã€index_to_docstore_idï¼‰
   â”œâ”€â”€ data/vector_store_docs.pkl  â† åŸå§‹æ–‡æ¡£å¯¹è±¡ï¼ˆDocumentåˆ—è¡¨ï¼‰
   â””â”€â”€ data/vector_store_metadata.json â† ç´¢å¼•å…ƒä¿¡æ¯ï¼ˆåˆ›å»ºæ—¶é—´ã€æ–‡æ¡£æ•°ç­‰ï¼‰
   ```

2. **ä¸ºä»€ä¹ˆéœ€è¦å•ç‹¬ä¿å­˜documentsï¼Ÿ**
   - FAISSç´¢å¼•åªå­˜å‚¨å‘é‡å’ŒIDæ˜ å°„ï¼Œä½†LangChainçš„ `docstore` å¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹ä¸å®Œæ•´
   - å•ç‹¬ç”¨pickleä¿å­˜å®Œæ•´çš„ `Document` å¯¹è±¡ï¼Œç¡®ä¿å…ƒæ•°æ®ä¸ä¸¢å¤±

3. **allow_dangerous_deserialization=True**ï¼š
   - FAISSä½¿ç”¨pickleååºåˆ—åŒ–ï¼Œå­˜åœ¨å®‰å…¨é£é™©ï¼ˆæ¶æ„æ–‡ä»¶å¯èƒ½æ‰§è¡Œä»»æ„ä»£ç ï¼‰
   - é¡¹ç›®ç¯å¢ƒå¯æ§æ—¶è®¾ç½®ä¸ºTrueï¼Œç”Ÿäº§ç¯å¢ƒéœ€è¯„ä¼°é£é™©

4. **å…ƒæ•°æ®ä½œç”¨**ï¼š
   ```json
   {
     "created_at": "2024-01-15T10:30:00",
     "documents_count": 150,
     "embedding_model": "text-embedding-v1",
     "vector_dimension": 1536
   }
   ```
   - ç”¨äºè°ƒè¯•å’Œç‰ˆæœ¬ç®¡ç†ï¼ˆä¸åŒembeddingæ¨¡å‹çš„å‘é‡ä¸å…¼å®¹ï¼‰

---

### 4.4 ç¬¬å››éƒ¨åˆ†ï¼šæ ¸å¿ƒæœç´¢æ–¹æ³•ï¼ˆ172-232è¡Œï¼‰

```python
    def search(
        self,
        query: str,
        top_k: int = 5,
        search_type: str = "similarity",
        score_threshold: float = 0.2
    ) -> List[Dict[str, Any]]:
        """æœç´¢å‘é‡å­˜å‚¨"""
        try:
            if not self.vector_store:
                logger.warning("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
                return []

            logger.info(f"æœç´¢æŸ¥è¯¢: {query}, top_k: {top_k}, search_type: {search_type}")

            if search_type == "similarity":
                # ç›¸ä¼¼åº¦æœç´¢
                results = self.vector_store.similarity_search_with_score(
                    query=query,
                    k=top_k
                )
            elif search_type == "mmr":
                # MMRæœç´¢ï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼‰- ç¤¾åŒºç‰ˆä¸è¿”å›åˆ†æ•°
                mmr_docs = self.vector_store.max_marginal_relevance_search(
                    query=query,
                    k=top_k,
                    fetch_k=top_k * 2
                )
                results = [(doc, 1.0) for doc in mmr_docs]
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æœç´¢ç±»å‹: {search_type}")

            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for doc, score in results:
                if score >= score_threshold:  # è¿‡æ»¤ä½åˆ†ç»“æœ
                    formatted_results.append({
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "score": float(score)
                    })

            # å¦‚æœç›¸ä¼¼åº¦æœç´¢æ²¡æœ‰å‘½ä¸­ï¼Œè‡ªåŠ¨å›é€€åˆ°MMRæé«˜å¬å›
            if not formatted_results and search_type == "similarity":
                mmr_docs = self.vector_store.max_marginal_relevance_search(
                    query=query,
                    k=top_k,
                    fetch_k=top_k * 2
                )
                formatted_results = [{
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "score": 1.0
                } for doc in mmr_docs]

            logger.info(f"æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(formatted_results)} ä¸ªç»“æœ")
            return formatted_results

        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {str(e)}")
            return []
```

**ä¸¤ç§æœç´¢ç­–ç•¥å¯¹æ¯”**ï¼š

| ç­–ç•¥ | åŸç† | ä¼˜åŠ¿ | åŠ£åŠ¿ | é€‚ç”¨åœºæ™¯ |
|------|------|------|------|----------|
| **similarity** | çº¯ä½™å¼¦ç›¸ä¼¼åº¦æ’åº | ç²¾å‡†åŒ¹é…ç›¸å…³å†…å®¹ | å¯èƒ½è¿”å›é‡å¤/å†—ä½™ç»“æœ | é—®ç­”ã€ç²¾ç¡®æ£€ç´¢ |
| **mmr** | åœ¨ç›¸å…³æ€§å’Œå¤šæ ·æ€§ä¹‹é—´å¹³è¡¡ | ç»“æœè¦†ç›–é¢æ›´å¹¿ | å¯èƒ½ç‰ºç‰²éƒ¨åˆ†ç²¾åº¦ | éœ€è¦å¤šè§’åº¦ä¿¡æ¯çš„åœºæ™¯ |

**MMRï¼ˆMaximum Marginal Relevanceï¼‰è¯¦è§£**ï¼š

```python
mmr_docs = self.vector_store.max_marginal_relevance_search(
    query=query,
    k=top_k,          # æœ€ç»ˆè¿”å›Kä¸ªç»“æœ
    fetch_k=top_k * 2 # å…ˆå–2Kä¸ªå€™é€‰ï¼ˆåœ¨è¿™äº›å€™é€‰ä¸­åšå¤šæ ·æ€§ç­›é€‰ï¼‰
)
```

**MMRç®—æ³•æµç¨‹**ï¼š
1. å…ˆç”¨ç›¸ä¼¼åº¦æœç´¢æ‰¾åˆ° `fetch_k` ä¸ªç›¸å…³æ–‡æ¡£ï¼ˆå¦‚10ä¸ªï¼‰
2. é€‰æ‹©ç›¸ä¼¼åº¦æœ€é«˜çš„ç¬¬1ä¸ªæ–‡æ¡£
3. åç»­æ¯æ¬¡é€‰æ‹©æ—¶ï¼Œè®¡ç®—ï¼š
   ```
   MMRåˆ†æ•° = Î» * ä¸æŸ¥è¯¢çš„ç›¸ä¼¼åº¦ - (1-Î») * ä¸å·²é€‰æ–‡æ¡£çš„æœ€å¤§ç›¸ä¼¼åº¦
   ```
   - `Î»=1`ï¼šå®Œå…¨çœ‹ç›¸ä¼¼åº¦ï¼ˆé€€åŒ–ä¸ºsimilarityæœç´¢ï¼‰
   - `Î»=0.5`ï¼šå¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§ï¼ˆLangChainé»˜è®¤å€¼ï¼‰
4. é‡å¤æ­¥éª¤3ï¼Œç›´åˆ°é€‰å‡º `k` ä¸ªæ–‡æ¡£

**è‡ªåŠ¨å›é€€æœºåˆ¶**ï¼š
```python
if not formatted_results and search_type == "similarity":
    # è‡ªåŠ¨åˆ‡æ¢åˆ°MMRæé«˜å¬å›
```
- **ä¸ºä»€ä¹ˆéœ€è¦**ï¼Ÿå½“ç›¸ä¼¼åº¦æœç´¢å› é˜ˆå€¼è¿‡é«˜å¯¼è‡´æ— ç»“æœæ—¶ï¼Œè‡ªåŠ¨å°è¯•MMRå¢åŠ å¬å›ç‡
- **ç”Ÿäº§ç»éªŒ**ï¼šé¿å…"ä¸€é—®ä¸‰ä¸çŸ¥"çš„å°´å°¬

---

### 4.5 ç¬¬äº”éƒ¨åˆ†ï¼šæ–‡æ¡£ç®¡ç†ï¼ˆ234-296è¡Œï¼‰

```python
    def add_documents(self, documents: List[Document]) -> bool:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨"""
        try:
            logger.info(f"æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨ï¼Œæ•°é‡: {len(documents)}")

            documents = self._sanitize_documents(documents)

            if not self.vector_store:
                # å¦‚æœå‘é‡å­˜å‚¨ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„
                self.create_vector_store(documents)
            else:
                # æ·»åŠ åˆ°ç°æœ‰å‘é‡å­˜å‚¨
                self.vector_store.add_documents(documents)
                self.documents.extend(documents)

            logger.info("æ–‡æ¡£æ·»åŠ æˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {str(e)}")
            return False

    def delete_document(self, doc_id: str) -> bool:
        """ä»å‘é‡å­˜å‚¨ä¸­åˆ é™¤æ–‡æ¡£"""
        try:
            logger.info(f"åˆ é™¤æ–‡æ¡£: {doc_id}")

            if not self.vector_store:
                logger.warning("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
                return False

            # FAISSä¸ç›´æ¥æ”¯æŒåˆ é™¤æ“ä½œï¼Œéœ€è¦é‡æ–°åˆ›å»ºç´¢å¼•
            remaining_docs = [
                doc for doc in self.documents
                if doc.metadata.get("id") != doc_id
            ]

            if len(remaining_docs) < len(self.documents):
                self.create_vector_store(remaining_docs)
                logger.info(f"æ–‡æ¡£åˆ é™¤æˆåŠŸ: {doc_id}")
                return True
            else:
                logger.warning(f"æœªæ‰¾åˆ°æ–‡æ¡£: {doc_id}")
                return False

        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")
            return False

    def clear(self):
        """æ¸…ç©ºå‘é‡å­˜å‚¨"""
        try:
            logger.info("æ¸…ç©ºå‘é‡å­˜å‚¨")

            # åŒæ—¶æ¸…ç†ç£ç›˜ç´¢å¼•æ–‡ä»¶ï¼Œé¿å…è‡ªåŠ¨åŠ è½½æ—§ç´¢å¼•
            idx_path = self.index_path or str(self.settings.VECTOR_STORE_PATH)
            try:
                if idx_path and os.path.exists(idx_path):
                    import shutil
                    shutil.rmtree(idx_path, ignore_errors=True)
                # åˆ é™¤é™„åŠ çš„docsä¸metadataæ–‡ä»¶
                docs_path = f"{idx_path}_docs.pkl"
                meta_path = f"{idx_path}_metadata.json"
                if os.path.exists(docs_path):
                    os.remove(docs_path)
                if os.path.exists(meta_path):
                    os.remove(meta_path)
            except Exception as e:
                logger.warning(f"åˆ é™¤ç´¢å¼•æ–‡ä»¶å¤±è´¥: {str(e)}")

            # æ¸…ç†å†…å­˜çŠ¶æ€
            self.vector_store = None
            self.documents = []
            self.index_path = None

            logger.info("å‘é‡å­˜å‚¨å·²æ¸…ç©ºï¼ˆå†…å­˜ä¸ç£ç›˜ï¼‰")

        except Exception as e:
            logger.error(f"æ¸…ç©ºå‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")
```

**å…³é”®è®¾è®¡å†³ç­–**ï¼š

1. **FAISSä¸æ”¯æŒç›´æ¥åˆ é™¤å‘é‡**ï¼š
   ```python
   remaining_docs = [doc for doc in self.documents if doc.metadata.get("id") != doc_id]
   self.create_vector_store(remaining_docs)  # é‡æ–°æ„å»ºç´¢å¼•
   ```
   - **åŸå› **ï¼šFAISSç´¢å¼•æ˜¯ç´§å‡‘çš„å‘é‡æ•°ç»„ï¼Œåˆ é™¤éœ€è¦é‡å»º
   - **æ›¿ä»£æ–¹æ¡ˆ**ï¼šå¦‚æœéœ€è¦é¢‘ç¹åˆ é™¤ï¼Œè€ƒè™‘ä½¿ç”¨Milvusç­‰æ”¯æŒåˆ é™¤çš„å‘é‡åº“

2. **clear()åŒæ—¶æ¸…ç†å†…å­˜å’Œç£ç›˜**ï¼š
   ```python
   shutil.rmtree(idx_path, ignore_errors=True)  # åˆ é™¤ç´¢å¼•ç›®å½•
   self.vector_store = None                      # æ¸…ç©ºå†…å­˜
   ```
   - **ä¸ºä»€ä¹ˆ**ï¼Ÿé¿å…è‡ªåŠ¨åŠ è½½æœºåˆ¶åŠ è½½å·²åˆ é™¤çš„ç´¢å¼•

---

### 4.6 ç¬¬å…­éƒ¨åˆ†ï¼šæ–‡æ¡£åˆ†å‰²ä¸æ¸…æ´—ï¼ˆ256-406è¡Œï¼‰

```python
    def _sanitize_documents(self, documents: List[Document]) -> List[Document]:
        """æ¸…æ´—æ–‡æ¡£å†…å®¹ï¼Œè¿‡æ»¤ç©ºå†…å®¹å¹¶è£å‰ªè¿‡é•¿æ–‡æœ¬ä»¥æ»¡è¶³åµŒå…¥é•¿åº¦é™åˆ¶"""
        cleaned: List[Document] = []
        # è¿‘ä¼¼å­—ç¬¦ä¸Šé™ï¼Œé¿å…è¶…è¿‡ 2048 tokenï¼ˆä¸­æ–‡åœºæ™¯ä¸‹å­—ç¬¦ä¸tokenè¿‘ä¼¼ï¼‰
        max_chars = 2000
        for doc in documents:
            content = (doc.page_content or "").strip()
            if not content:
                continue
            if len(content) > max_chars:
                content = content[:max_chars]
            cleaned.append(Document(page_content=content, metadata=doc.metadata))
        logger.info(f"æ¸…æ´—åæ–‡æ¡£æ•°é‡: {len(cleaned)}")
        return cleaned

    def split_documents(
        self,
        documents: List[Document],
        chunk_size: int = None,
        chunk_overlap: int = None
    ) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£"""
        try:
            chunk_size = chunk_size or self.settings.CHUNK_SIZE
            chunk_overlap = chunk_overlap or self.settings.CHUNK_OVERLAP

            logger.info(f"åˆ†å‰²æ–‡æ¡£ï¼Œchunk_size: {chunk_size}, chunk_overlap: {chunk_overlap}")

            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                length_function=len,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
            )

            split_docs = text_splitter.split_documents(documents)

            # æ·»åŠ å…ƒæ•°æ®
            for i, doc in enumerate(split_docs):
                if "chunk_id" not in doc.metadata:
                    doc.metadata["chunk_id"] = i
                if "chunk_size" not in doc.metadata:
                    doc.metadata["chunk_size"] = len(doc.page_content)

            logger.info(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œç‰‡æ®µæ•°é‡: {len(split_docs)}")
            return split_docs

        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ†å‰²å¤±è´¥: {str(e)}")
            return documents
```

**æ–‡æ¡£åˆ†å‰²ç­–ç•¥è¯¦è§£**ï¼š

1. **ä¸ºä»€ä¹ˆéœ€è¦åˆ†å‰²ï¼Ÿ**
   - Embeddingæ¨¡å‹æœ‰é•¿åº¦é™åˆ¶ï¼ˆå¦‚2048 tokensï¼‰
   - æ£€ç´¢ç²’åº¦è¿‡ç²—ä¼šé™ä½ç²¾åº¦ï¼ˆå¦‚æ•´ç¯‡æ–‡æ¡£vsæ®µè½ï¼‰

2. **RecursiveCharacterTextSplitteråŸç†**ï¼š
   ```python
   separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
   ```
   - **é€’å½’ç­–ç•¥**ï¼šä¼˜å…ˆç”¨ `\n\n` åˆ†å‰²ï¼Œå¦‚æœchunkä»è¶…é•¿ï¼Œåˆ™ç”¨ `\n` åˆ†å‰²ï¼Œä¾æ­¤ç±»æ¨
   - **ä¸­æ–‡ä¼˜åŒ–**ï¼šæ·»åŠ  `ã€‚ï¼ï¼Ÿï¼Œ` ç­‰ä¸­æ–‡æ ‡ç‚¹ï¼Œé¿å…åœ¨è¯è¯­ä¸­é—´æˆªæ–­

3. **chunk_overlapçš„ä½œç”¨**ï¼š
   ```python
   chunk_overlap=100  # ç›¸é‚»chunkä¹‹é—´é‡å 100ä¸ªå­—ç¬¦
   ```
   - **é—®é¢˜**ï¼šå¦‚æœå…³é”®ä¿¡æ¯æ°å¥½åœ¨chunkè¾¹ç•Œï¼Œä¼šè¢«æˆªæ–­
   - **è§£å†³**ï¼šç›¸é‚»chunkæœ‰é‡å ï¼Œç¡®ä¿ä¸Šä¸‹æ–‡è¿è´¯

   **ç¤ºä¾‹**ï¼š
   ```
   åŸæ–‡ï¼š"Pythonæ˜¯ç¼–ç¨‹è¯­è¨€ã€‚å®ƒæ”¯æŒé¢å‘å¯¹è±¡ã€‚"

   ä¸é‡å ï¼š
   Chunk1: "Pythonæ˜¯ç¼–ç¨‹è¯­è¨€ã€‚"
   Chunk2: "å®ƒæ”¯æŒé¢å‘å¯¹è±¡ã€‚"  â† "å®ƒ"æŒ‡ä»£ä¸æ˜

   é‡å 100å­—ç¬¦ï¼š
   Chunk1: "Pythonæ˜¯ç¼–ç¨‹è¯­è¨€ã€‚å®ƒæ”¯æŒé¢å‘å¯¹è±¡ã€‚"
   Chunk2: "Pythonæ˜¯ç¼–ç¨‹è¯­è¨€ã€‚å®ƒæ”¯æŒé¢å‘å¯¹è±¡ã€‚" â† ä¿ç•™ä¸Šä¸‹æ–‡
   ```

4. **å…ƒæ•°æ®å¢å¼º**ï¼š
   ```python
   doc.metadata["chunk_id"] = i
   doc.metadata["chunk_size"] = len(doc.page_content)
   ```
   - ç”¨äºè°ƒè¯•å’Œæº¯æºï¼ˆçŸ¥é“æ˜¯å“ªä¸ªchunkåŒ¹é…æˆåŠŸï¼‰

---

### 4.7 ç¬¬ä¸ƒéƒ¨åˆ†ï¼šç»Ÿè®¡ä¸è¾…åŠ©æ–¹æ³•ï¼ˆ298-437è¡Œï¼‰

```python
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å‘é‡å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {
                "documents_count": len(self.documents),
                "vector_store_initialized": self.vector_store is not None,
                "embedding_model": self.embedding_model_name,
                "index_path": self.index_path
            }

            if self.vector_store:
                # è·å–ç´¢å¼•ä¿¡æ¯
                index = self.vector_store.index
                stats.update({
                    "total_vectors": index.ntotal if hasattr(index, 'ntotal') else 0,
                    "dimension": index.d if hasattr(index, 'd') else 0
                })

            return stats

        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {"error": str(e)}

    def get_document_list(self) -> List[Dict[str, Any]]:
        """è·å–çŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£åˆ—è¡¨åŠç»Ÿè®¡ä¿¡æ¯"""
        try:
            docs = self.documents or []
            by_source: Dict[str, Dict[str, Any]] = {}
            for d in docs:
                meta = d.metadata or {}
                src = meta.get("source", "æœªçŸ¥æ¥æº")
                ftype = meta.get("file_type", "")
                by_source.setdefault(src, {"æ–‡ä»¶å": src, "æ–‡ä»¶ç±»å‹": ftype, "ç‰‡æ®µæ•°": 0})
                by_source[src]["ç‰‡æ®µæ•°"] += 1
            # è½¬ä¸ºåˆ—è¡¨å¹¶æŒ‰æ–‡ä»¶åæ’åº
            result = list(by_source.values())
            result.sort(key=lambda x: x["æ–‡ä»¶å"].lower())
            return result
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []

    def similarity_search_with_threshold(
        self,
        query: str,
        threshold: float = 0.7,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """å¸¦é˜ˆå€¼çš„ç›¸ä¼¼åº¦æœç´¢"""
        try:
            results = self.search(query, top_k * 2, "similarity")  # è·å–æ›´å¤šç»“æœç”¨äºè¿‡æ»¤

            # æŒ‰é˜ˆå€¼è¿‡æ»¤
            filtered_results = [
                result for result in results
                if result["score"] >= threshold
            ]

            # è¿”å›å‰top_kä¸ªç»“æœ
            return filtered_results[:top_k]

        except Exception as e:
            logger.error(f"é˜ˆå€¼æœç´¢å¤±è´¥: {str(e)}")
            return []

    @property
    def embedding_model_name(self) -> str:
        """è·å–åµŒå…¥æ¨¡å‹åç§°"""
        if hasattr(self.embeddings, 'embedding_model_name'):
            return self.embeddings.embedding_model_name
        return self.settings.get_embedding_model()
```

**å…³é”®åŠŸèƒ½è¯´æ˜**ï¼š

1. **get_stats()ç»Ÿè®¡ä¿¡æ¯**ï¼š
   ```python
   index.ntotal  # FAISSç´¢å¼•ä¸­å‘é‡æ€»æ•°
   index.d       # å‘é‡ç»´åº¦ï¼ˆå¦‚768æˆ–1536ï¼‰
   ```
   - ç”¨äºç›‘æ§å’Œè°ƒè¯•ï¼ˆå¦‚æ£€æŸ¥ç´¢å¼•æ˜¯å¦æ­£å¸¸åŠ è½½ï¼‰

2. **get_document_list()èšåˆæ˜¾ç¤º**ï¼š
   ```python
   by_source.setdefault(src, {"æ–‡ä»¶å": src, "æ–‡ä»¶ç±»å‹": ftype, "ç‰‡æ®µæ•°": 0})
   ```
   - **æ•ˆæœ**ï¼šå°†åŒä¸€æ–‡ä»¶çš„å¤šä¸ªchunkèšåˆæ˜¾ç¤º
   - **ç¤ºä¾‹è¾“å‡º**ï¼š
     ```python
     [
       {"æ–‡ä»¶å": "Pythonæ•™ç¨‹.pdf", "æ–‡ä»¶ç±»å‹": "pdf", "ç‰‡æ®µæ•°": 45},
       {"æ–‡ä»¶å": "æœºå™¨å­¦ä¹ .md", "æ–‡ä»¶ç±»å‹": "markdown", "ç‰‡æ®µæ•°": 23}
     ]
     ```

3. **similarity_search_with_threshold()é«˜é˜ˆå€¼æœç´¢**ï¼š
   ```python
   results = self.search(query, top_k * 2, "similarity")  # å…ˆå–2Kä¸ªå€™é€‰
   filtered_results = [r for r in results if r["score"] >= threshold]  # è¿‡æ»¤ä½åˆ†
   ```
   - **ç”¨é€”**ï¼šå¯¹ç²¾åº¦è¦æ±‚é«˜çš„åœºæ™¯ï¼ˆå¦‚æ³•å¾‹ã€åŒ»ç–—ï¼‰ï¼Œåªè¿”å›é«˜ç›¸ä¼¼åº¦ç»“æœ
   - **æŠ€å·§**ï¼šå…ˆå–æ›´å¤šå€™é€‰ï¼Œå†è¿‡æ»¤ï¼Œé¿å…å› è¿‡æ»¤å¯¼è‡´ç»“æœä¸è¶³

---

## äº”ã€å®Œæ•´ä»£ç 

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ services/vector_store.py å®Œæ•´ä»£ç ï¼ˆ476è¡Œï¼‰</summary>

```python
import os
import json
import pickle
import logging
from typing import List, Dict, Optional, Any, Tuple
from datetime import datetime
import numpy as np
import faiss
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain.embeddings.base import Embeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from config.settings import Settings
from services.llm_client import UnifiedEmbeddingClient

logger = logging.getLogger(__name__)

class VectorStoreService:
    """å‘é‡å­˜å‚¨æœåŠ¡ç±» - æ”¯æŒ Ollama å’Œåœ¨çº¿ API embedding"""

    def __init__(self):
        self.settings = Settings()
        # ä½¿ç”¨ç»Ÿä¸€çš„åµŒå…¥å®¢æˆ·ç«¯
        self.embedding_client = UnifiedEmbeddingClient()
        self.embeddings = self.embedding_client.get_embeddings()
        self.vector_store = None
        self.documents = []
        self.index_path = None

        logger.info(f"å‘é‡å­˜å‚¨æœåŠ¡åˆå§‹åŒ–æˆåŠŸ - æä¾›å•†: {self.settings.LLM_PROVIDER}, åµŒå…¥æ¨¡å‹: {self.settings.get_embedding_model()}")

        # å°è¯•è‡ªåŠ¨åŠ è½½å·²å­˜åœ¨çš„ç´¢å¼•ï¼Œé¿å…è¿è¡ŒæœŸé—´çŠ¶æ€ä¸¢å¤±
        try:
            idx_path = str(self.settings.VECTOR_STORE_PATH)
            if os.path.exists(idx_path):
                loaded = self.load_index(idx_path)
                if loaded:
                    logger.info("æ£€æµ‹åˆ°å·²æœ‰å‘é‡ç´¢å¼•ï¼Œå·²è‡ªåŠ¨åŠ è½½")
        except Exception as e:
            logger.warning(f"è‡ªåŠ¨åŠ è½½å‘é‡ç´¢å¼•å¤±è´¥: {str(e)}")

    def create_vector_store(self, documents: List[Document]) -> FAISS:
        """åˆ›å»ºå‘é‡å­˜å‚¨"""
        try:
            logger.info(f"åˆ›å»ºå‘é‡å­˜å‚¨ï¼Œæ–‡æ¡£æ•°é‡: {len(documents)}")

            # æ¸…æ´—æ–‡æ¡£ï¼Œé¿å…ç©ºç‰‡æ®µå’Œè¶…é•¿å†…å®¹
            documents = self._sanitize_documents(documents)

            # åˆ›å»ºå‘é‡å­˜å‚¨
            vector_store = FAISS.from_documents(
                documents=documents,
                embedding=self.embeddings
            )

            self.vector_store = vector_store
            self.documents = documents

            logger.info("å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸ")
            return vector_store

        except Exception as e:
            logger.error(f"åˆ›å»ºå‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")
            raise

    def load_index(self, index_path: str) -> bool:
        """åŠ è½½å‘é‡å­˜å‚¨ç´¢å¼•"""
        try:
            logger.info(f"åŠ è½½å‘é‡å­˜å‚¨ç´¢å¼•: {index_path}")

            if not os.path.exists(index_path):
                logger.warning(f"ç´¢å¼•è·¯å¾„ä¸å­˜åœ¨: {index_path}")
                return False

            # åŠ è½½FAISSç´¢å¼•
            self.vector_store = FAISS.load_local(
                index_path,
                self.embeddings,
                allow_dangerous_deserialization=True
            )

            # åŠ è½½æ–‡æ¡£ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            docs_path = f"{index_path}_docs.pkl"
            if os.path.exists(docs_path):
                with open(docs_path, 'rb') as f:
                    self.documents = pickle.load(f)

            self.index_path = index_path
            logger.info("å‘é‡å­˜å‚¨ç´¢å¼•åŠ è½½æˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"åŠ è½½å‘é‡å­˜å‚¨ç´¢å¼•å¤±è´¥: {str(e)}")
            return False

    def save_index(self, index_path: str) -> bool:
        """ä¿å­˜å‘é‡å­˜å‚¨ç´¢å¼•"""
        try:
            if not self.vector_store:
                logger.warning("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜")
                return False

            logger.info(f"ä¿å­˜å‘é‡å­˜å‚¨ç´¢å¼•: {index_path}")

            # ä¿å­˜FAISSç´¢å¼•
            self.vector_store.save_local(index_path)

            # ä¿å­˜æ–‡æ¡£
            docs_path = f"{index_path}_docs.pkl"
            with open(docs_path, 'wb') as f:
                pickle.dump(self.documents, f)

            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                "created_at": datetime.now().isoformat(),
                "documents_count": len(self.documents),
                "embedding_model": self.embedding_model_name,
                "vector_dimension": self.settings.VECTOR_DIMENSION
            }

            metadata_path = f"{index_path}_metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            self.index_path = index_path
            logger.info("å‘é‡å­˜å‚¨ç´¢å¼•ä¿å­˜æˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"ä¿å­˜å‘é‡å­˜å‚¨ç´¢å¼•å¤±è´¥: {str(e)}")
            return False

    def search(
        self,
        query: str,
        top_k: int = 5,
        search_type: str = "similarity",
        score_threshold: float = 0.2
    ) -> List[Dict[str, Any]]:
        """æœç´¢å‘é‡å­˜å‚¨"""
        try:
            if not self.vector_store:
                logger.warning("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
                return []

            logger.info(f"æœç´¢æŸ¥è¯¢: {query}, top_k: {top_k}, search_type: {search_type}")

            if search_type == "similarity":
                # ç›¸ä¼¼åº¦æœç´¢
                results = self.vector_store.similarity_search_with_score(
                    query=query,
                    k=top_k
                )
            elif search_type == "mmr":
                # MMRæœç´¢ï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼‰- ç¤¾åŒºç‰ˆä¸è¿”å›åˆ†æ•°
                mmr_docs = self.vector_store.max_marginal_relevance_search(
                    query=query,
                    k=top_k,
                    fetch_k=top_k * 2
                )
                results = [(doc, 1.0) for doc in mmr_docs]
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æœç´¢ç±»å‹: {search_type}")

            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for doc, score in results:
                if score >= score_threshold:  # è¿‡æ»¤ä½åˆ†ç»“æœ
                    formatted_results.append({
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "score": float(score)
                    })

            # å¦‚æœç›¸ä¼¼åº¦æœç´¢æ²¡æœ‰å‘½ä¸­ï¼Œè‡ªåŠ¨å›é€€åˆ°MMRæé«˜å¬å›
            if not formatted_results and search_type == "similarity":
                mmr_docs = self.vector_store.max_marginal_relevance_search(
                    query=query,
                    k=top_k,
                    fetch_k=top_k * 2
                )
                formatted_results = [{
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "score": 1.0
                } for doc in mmr_docs]

            logger.info(f"æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(formatted_results)} ä¸ªç»“æœ")
            return formatted_results

        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {str(e)}")
            return []

    def add_documents(self, documents: List[Document]) -> bool:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨"""
        try:
            logger.info(f"æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨ï¼Œæ•°é‡: {len(documents)}")

            documents = self._sanitize_documents(documents)

            if not self.vector_store:
                # å¦‚æœå‘é‡å­˜å‚¨ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„
                self.create_vector_store(documents)
            else:
                # æ·»åŠ åˆ°ç°æœ‰å‘é‡å­˜å‚¨
                self.vector_store.add_documents(documents)
                self.documents.extend(documents)

            logger.info("æ–‡æ¡£æ·»åŠ æˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {str(e)}")
            return False

    def _sanitize_documents(self, documents: List[Document]) -> List[Document]:
        """æ¸…æ´—æ–‡æ¡£å†…å®¹ï¼Œè¿‡æ»¤ç©ºå†…å®¹å¹¶è£å‰ªè¿‡é•¿æ–‡æœ¬ä»¥æ»¡è¶³åµŒå…¥é•¿åº¦é™åˆ¶"""
        cleaned: List[Document] = []
        # è¿‘ä¼¼å­—ç¬¦ä¸Šé™ï¼Œé¿å…è¶…è¿‡ 2048 tokenï¼ˆä¸­æ–‡åœºæ™¯ä¸‹å­—ç¬¦ä¸tokenè¿‘ä¼¼ï¼‰
        max_chars = 2000
        for doc in documents:
            content = (doc.page_content or "").strip()
            if not content:
                continue
            if len(content) > max_chars:
                content = content[:max_chars]
            cleaned.append(Document(page_content=content, metadata=doc.metadata))
        logger.info(f"æ¸…æ´—åæ–‡æ¡£æ•°é‡: {len(cleaned)}")
        return cleaned

    def delete_document(self, doc_id: str) -> bool:
        """ä»å‘é‡å­˜å‚¨ä¸­åˆ é™¤æ–‡æ¡£"""
        try:
            logger.info(f"åˆ é™¤æ–‡æ¡£: {doc_id}")

            if not self.vector_store:
                logger.warning("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
                return False

            # FAISSä¸ç›´æ¥æ”¯æŒåˆ é™¤æ“ä½œï¼Œéœ€è¦é‡æ–°åˆ›å»ºç´¢å¼•
            remaining_docs = [
                doc for doc in self.documents
                if doc.metadata.get("id") != doc_id
            ]

            if len(remaining_docs) < len(self.documents):
                self.create_vector_store(remaining_docs)
                logger.info(f"æ–‡æ¡£åˆ é™¤æˆåŠŸ: {doc_id}")
                return True
            else:
                logger.warning(f"æœªæ‰¾åˆ°æ–‡æ¡£: {doc_id}")
                return False

        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")
            return False

    def clear(self):
        """æ¸…ç©ºå‘é‡å­˜å‚¨"""
        try:
            logger.info("æ¸…ç©ºå‘é‡å­˜å‚¨")

            # åŒæ—¶æ¸…ç†ç£ç›˜ç´¢å¼•æ–‡ä»¶ï¼Œé¿å…è‡ªåŠ¨åŠ è½½æ—§ç´¢å¼•
            idx_path = self.index_path or str(self.settings.VECTOR_STORE_PATH)
            try:
                if idx_path and os.path.exists(idx_path):
                    import shutil
                    shutil.rmtree(idx_path, ignore_errors=True)
                # åˆ é™¤é™„åŠ çš„docsä¸metadataæ–‡ä»¶
                docs_path = f"{idx_path}_docs.pkl"
                meta_path = f"{idx_path}_metadata.json"
                if os.path.exists(docs_path):
                    os.remove(docs_path)
                if os.path.exists(meta_path):
                    os.remove(meta_path)
            except Exception as e:
                logger.warning(f"åˆ é™¤ç´¢å¼•æ–‡ä»¶å¤±è´¥: {str(e)}")

            # æ¸…ç†å†…å­˜çŠ¶æ€
            self.vector_store = None
            self.documents = []
            self.index_path = None

            logger.info("å‘é‡å­˜å‚¨å·²æ¸…ç©ºï¼ˆå†…å­˜ä¸ç£ç›˜ï¼‰")

        except Exception as e:
            logger.error(f"æ¸…ç©ºå‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")

    def get_stats(self) -> Dict[str, Any]:
        """è·å–å‘é‡å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {
                "documents_count": len(self.documents),
                "vector_store_initialized": self.vector_store is not None,
                "embedding_model": self.embedding_model_name,
                "index_path": self.index_path
            }

            if self.vector_store:
                # è·å–ç´¢å¼•ä¿¡æ¯
                index = self.vector_store.index
                stats.update({
                    "total_vectors": index.ntotal if hasattr(index, 'ntotal') else 0,
                    "dimension": index.d if hasattr(index, 'd') else 0
                })

            return stats

        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {"error": str(e)}

    def get_document_list(self) -> List[Dict[str, Any]]:
        """è·å–çŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£åˆ—è¡¨åŠç»Ÿè®¡ä¿¡æ¯"""
        try:
            docs = self.documents or []
            by_source: Dict[str, Dict[str, Any]] = {}
            for d in docs:
                meta = d.metadata or {}
                src = meta.get("source", "æœªçŸ¥æ¥æº")
                ftype = meta.get("file_type", "")
                by_source.setdefault(src, {"æ–‡ä»¶å": src, "æ–‡ä»¶ç±»å‹": ftype, "ç‰‡æ®µæ•°": 0})
                by_source[src]["ç‰‡æ®µæ•°"] += 1
            # è½¬ä¸ºåˆ—è¡¨å¹¶æŒ‰æ–‡ä»¶åæ’åº
            result = list(by_source.values())
            result.sort(key=lambda x: x["æ–‡ä»¶å"].lower())
            return result
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []

    def split_documents(
        self,
        documents: List[Document],
        chunk_size: int = None,
        chunk_overlap: int = None
    ) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£"""
        try:
            chunk_size = chunk_size or self.settings.CHUNK_SIZE
            chunk_overlap = chunk_overlap or self.settings.CHUNK_OVERLAP

            logger.info(f"åˆ†å‰²æ–‡æ¡£ï¼Œchunk_size: {chunk_size}, chunk_overlap: {chunk_overlap}")

            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                length_function=len,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
            )

            split_docs = text_splitter.split_documents(documents)

            # æ·»åŠ å…ƒæ•°æ®
            for i, doc in enumerate(split_docs):
                if "chunk_id" not in doc.metadata:
                    doc.metadata["chunk_id"] = i
                if "chunk_size" not in doc.metadata:
                    doc.metadata["chunk_size"] = len(doc.page_content)

            logger.info(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œç‰‡æ®µæ•°é‡: {len(split_docs)}")
            return split_docs

        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ†å‰²å¤±è´¥: {str(e)}")
            return documents

    def similarity_search_with_threshold(
        self,
        query: str,
        threshold: float = 0.7,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """å¸¦é˜ˆå€¼çš„ç›¸ä¼¼åº¦æœç´¢"""
        try:
            results = self.search(query, top_k * 2, "similarity")  # è·å–æ›´å¤šç»“æœç”¨äºè¿‡æ»¤

            # æŒ‰é˜ˆå€¼è¿‡æ»¤
            filtered_results = [
                result for result in results
                if result["score"] >= threshold
            ]

            # è¿”å›å‰top_kä¸ªç»“æœ
            return filtered_results[:top_k]

        except Exception as e:
            logger.error(f"é˜ˆå€¼æœç´¢å¤±è´¥: {str(e)}")
            return []

    @property
    def embedding_model_name(self) -> str:
        """è·å–åµŒå…¥æ¨¡å‹åç§°"""
        if hasattr(self.embeddings, 'embedding_model_name'):
            return self.embeddings.embedding_model_name
        return self.settings.get_embedding_model()


# ==============================================================================
#  ONLINE-ONLY å¿«é€Ÿæµ‹è¯•
# ==============================================================================
def test_online_only():
    """
    çº¯åœ¨çº¿æ¨¡å¼åŠŸèƒ½æµ‹è¯•
    å¼ºåˆ¶ä½¿ç”¨ online / text-embedding-v1ï¼Œæ— éœ€æœ¬åœ° Ollama
    """
    import os, tempfile, json
    os.environ["LLM_PROVIDER"] = "online"          # é”æ­» online
    os.environ["ONLINE_API_KEY"] = os.getenv(
        "ONLINE_API_KEY",
        "sk-abe3417c96f6441b83efed38708bcfb6"      # é»˜è®¤ Demo key
    )

    print("===  VectorStoreService åœ¨çº¿æ¨¡å‹åŠŸèƒ½æµ‹è¯•  ===")
    print("LLM_PROVIDER =", os.environ["LLM_PROVIDER"])
    print("ONLINE_API_KEY =", os.environ["ONLINE_API_KEY"][:10] + "***")

    try:
        # 1. åˆå§‹åŒ–
        print("\n1. åˆå§‹åŒ–æœåŠ¡ï¼ˆonlineï¼‰...")
        vs = VectorStoreService()
        print("   âœ“ åˆå§‹åŒ–å®Œæˆï¼ŒåµŒå…¥æ¨¡å‹ï¼š", vs.settings.get_embedding_model())

        # 2. æ„é€ æµ‹è¯•æ–‡æ¡£
        docs = [
            Document(page_content="Python æ˜¯ç®€æ´å¼ºå¤§çš„ç¼–ç¨‹è¯­è¨€ã€‚", metadata={"id": "1"}),
            Document(page_content="æœºå™¨å­¦ä¹ æ— éœ€æ˜¾å¼ç¼–ç¨‹å³å¯å­¦ä¹ ã€‚", metadata={"id": "2"}),
            Document(page_content="å‘é‡æ•°æ®åº“æ”¯æ’‘è¯­ä¹‰æœç´¢ã€‚", metadata={"id": "3"}),
        ]

        # 3. åˆ›å»ºç´¢å¼•
        print("\n2. åˆ›å»ºå‘é‡ç´¢å¼•...")
        vs.create_vector_store(docs)
        print("   âœ“ ç´¢å¼•åˆ›å»ºæˆåŠŸï¼Œæ–‡æ¡£æ•°ï¼š", len(vs.documents))

        # 4. æœç´¢æµ‹è¯•
        print("\n3. ç›¸ä¼¼åº¦æœç´¢...")
        res = vs.search("Python è¯­è¨€ç‰¹ç‚¹", top_k=2, score_threshold=0.3)
        print("   è¿”å›ç»“æœæ•°ï¼š", len(res))
        for r in res:
            print("   - score={:.3f}, content={}".format(r["score"], r["content"][:60]))

        print("\n4. MMR æœç´¢...")
        mmr = vs.search("æœºå™¨å­¦ä¹ ", top_k=2, search_type="mmr", score_threshold=0.2)
        print("   MMR ç»“æœæ•°ï¼š", len(mmr))

        # 5. ä¿å­˜ / åŠ è½½
        print("\n5. ä¿å­˜ & åŠ è½½ç´¢å¼•...")
        with tempfile.TemporaryDirectory() as tmp:
            idx_path = os.path.join(tmp, "online_index")
            assert vs.save_index(idx_path), "ä¿å­˜å¤±è´¥"
            print("   âœ“ ä¿å­˜æˆåŠŸ")

            vs2 = VectorStoreService()              # æ–°å®ä¾‹
            assert vs2.load_index(idx_path), "åŠ è½½å¤±è´¥"
            print("   âœ“ åŠ è½½æˆåŠŸï¼Œæ–‡æ¡£æ•°ï¼š", len(vs2.documents))

        # 6. ç»Ÿè®¡ & æ¸…ç©º
        print("\n6. ç»Ÿè®¡ä¿¡æ¯ï¼š", vs.get_stats())
        vs.clear()
        print("   âœ“ å·²æ¸…ç©ºï¼Œæ–‡æ¡£æ•°ï¼š", len(vs.documents))

        print("\n===  åœ¨çº¿æ¨¡å‹æµ‹è¯•å…¨éƒ¨é€šè¿‡  ===")
        return True

    except Exception as e:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼š", e)
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œå³èµ° online æµ‹è¯•
    test_online_only()
```

</details>

---

## å…­ã€åŠŸèƒ½æµ‹è¯•ä¸éªŒè¯

### 6.1 å¿«é€Ÿæµ‹è¯•è„šæœ¬

ä»£ç æœ«å°¾æä¾›äº† `test_online_only()` æµ‹è¯•å‡½æ•°ï¼Œç›´æ¥è¿è¡Œå³å¯éªŒè¯ï¼š

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆä½¿ç”¨åœ¨çº¿APIï¼‰
export LLM_PROVIDER=online
export ONLINE_API_KEY=your_key_here

# è¿è¡Œæµ‹è¯•
python services/vector_store.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```
===  VectorStoreService åœ¨çº¿æ¨¡å‹åŠŸèƒ½æµ‹è¯•  ===
LLM_PROVIDER = online
ONLINE_API_KEY = sk-abe3417***

1. åˆå§‹åŒ–æœåŠ¡ï¼ˆonlineï¼‰...
   âœ“ åˆå§‹åŒ–å®Œæˆï¼ŒåµŒå…¥æ¨¡å‹ï¼š text-embedding-v1

2. åˆ›å»ºå‘é‡ç´¢å¼•...
   âœ“ ç´¢å¼•åˆ›å»ºæˆåŠŸï¼Œæ–‡æ¡£æ•°ï¼š 3

3. ç›¸ä¼¼åº¦æœç´¢...
   è¿”å›ç»“æœæ•°ï¼š 2
   - score=0.892, content=Python æ˜¯ç®€æ´å¼ºå¤§çš„ç¼–ç¨‹è¯­è¨€ã€‚
   - score=0.765, content=å‘é‡æ•°æ®åº“æ”¯æ’‘è¯­ä¹‰æœç´¢ã€‚

4. MMR æœç´¢...
   MMR ç»“æœæ•°ï¼š 2

5. ä¿å­˜ & åŠ è½½ç´¢å¼•...
   âœ“ ä¿å­˜æˆåŠŸ
   âœ“ åŠ è½½æˆåŠŸï¼Œæ–‡æ¡£æ•°ï¼š 3

6. ç»Ÿè®¡ä¿¡æ¯ï¼š {'documents_count': 3, 'total_vectors': 3, 'dimension': 1536}
   âœ“ å·²æ¸…ç©ºï¼Œæ–‡æ¡£æ•°ï¼š 0

===  åœ¨çº¿æ¨¡å‹æµ‹è¯•å…¨éƒ¨é€šè¿‡  ===
```

### 6.2 æºä»£ç å¯¹æ¯”éªŒè¯

ç¡®ä¿æ•™ç¨‹ä»£ç ä¸é¡¹ç›®æºä»£ç ä¸€è‡´ï¼š

```bash
# å¯¹æ¯”æ–‡ä»¶è¡Œæ•°
wc -l services/vector_store.py
# è¾“å‡ºï¼š476 services/vector_store.py

# æ£€æŸ¥å…³é”®æ–¹æ³•æ˜¯å¦å­˜åœ¨
grep -n "def create_vector_store" services/vector_store.py
grep -n "def search" services/vector_store.py
grep -n "def _sanitize_documents" services/vector_store.py
```

### 6.3 é›†æˆæµ‹è¯•

åœ¨å®Œæ•´é¡¹ç›®ä¸­æµ‹è¯•å‘é‡å­˜å‚¨ï¼š

```python
# test_vector_integration.py
from services.vector_store import VectorStoreService
from langchain.schema import Document

# 1. åˆ›å»ºæœåŠ¡å®ä¾‹
vs = VectorStoreService()

# 2. å‡†å¤‡æµ‹è¯•æ–‡æ¡£
docs = [
    Document(page_content="RAGç³»ç»Ÿç»“åˆæ£€ç´¢å’Œç”Ÿæˆ", metadata={"source": "test.txt"}),
    Document(page_content="FAISSé€‚ç”¨äºä¸­å°è§„æ¨¡å‘é‡æ£€ç´¢", metadata={"source": "test.txt"})
]

# 3. åˆ›å»ºç´¢å¼•
vs.create_vector_store(docs)

# 4. æœç´¢æµ‹è¯•
results = vs.search("ä»€ä¹ˆæ˜¯RAG", top_k=1)
print("æœç´¢ç»“æœï¼š", results[0]["content"])

# 5. ä¿å­˜ç´¢å¼•
vs.save_index("data/test_index")

# 6. æ¸…ç†
vs.clear()
print("æµ‹è¯•é€šè¿‡ï¼")
```

---

## ä¸ƒã€æœ¬ç« æ€»ç»“

### 7.1 æ ¸å¿ƒè¦ç‚¹å›é¡¾

âœ… **ç†è®ºåŸºç¡€**ï¼š
- å‘é‡æ£€ç´¢é€šè¿‡è¯­ä¹‰ç›¸ä¼¼åº¦è§£å†³ä¼ ç»Ÿå…³é”®è¯æœç´¢çš„å±€é™
- FAISSæä¾›æœ¬åœ°é«˜æ€§èƒ½å‘é‡æ£€ç´¢èƒ½åŠ›

âœ… **æ¶æ„è®¾è®¡**ï¼š
- `VectorStoreService` å°è£…å®Œæ•´çš„å‘é‡å­˜å‚¨ç”Ÿå‘½å‘¨æœŸ
- æ”¯æŒåˆå§‹åŒ–ã€åˆ›å»ºã€æœç´¢ã€æŒä¹…åŒ–ã€ç®¡ç†ç­‰å…¨æµç¨‹

âœ… **å…³é”®å®ç°**ï¼š
- **æœç´¢ç­–ç•¥**ï¼šsimilarityï¼ˆç²¾å‡†ï¼‰vs MMRï¼ˆå¤šæ ·æ€§ï¼‰
- **æ–‡æ¡£åˆ†å‰²**ï¼šRecursiveCharacterTextSplitter + chunk_overlap
- **æ•°æ®æ¸…æ´—**ï¼š_sanitize_documents é¿å…è¶…é•¿å†…å®¹
- **æŒä¹…åŒ–**ï¼šä¸‰æ–‡ä»¶ç»“æ„ï¼ˆindex + docs + metadataï¼‰

âœ… **ç”Ÿäº§ç»éªŒ**ï¼š
- è‡ªåŠ¨åŠ è½½æœºåˆ¶é¿å…Streamlité‡å¯ä¸¢å¤±ç´¢å¼•
- è‡ªåŠ¨å›é€€ç­–ç•¥æé«˜å¬å›ç‡
- å…ƒæ•°æ®å¢å¼ºä¾¿äºè°ƒè¯•æº¯æº

### 7.2 ä¸å‰ä¸‰ç« çš„å…³è”

| ç« èŠ‚ | æ ¸å¿ƒç»„ä»¶ | åœ¨ç¬¬04ç« ä¸­çš„åº”ç”¨ |
|------|----------|------------------|
| ç¬¬01ç«  | é¡¹ç›®æ¶æ„ | VectorStoreServiceæ˜¯RAGç³»ç»Ÿæ ¸å¿ƒç»„ä»¶ |
| ç¬¬02ç«  | Settingsé…ç½® | è¯»å–CHUNK_SIZEã€VECTOR_STORE_PATHç­‰é…ç½® |
| ç¬¬03ç«  | UnifiedEmbeddingClient | åˆå§‹åŒ–æ—¶æ³¨å…¥ç»Ÿä¸€çš„Embeddingå®¢æˆ·ç«¯ |

---

## å…«ã€ä¸‹ä¸€ç« é¢„å‘Š

**ç¬¬05ç« ï¼šè¾…åŠ©å·¥å…·ç±» - è£…é¥°å™¨ä¸æ–‡æ¡£å¤„ç†å™¨çš„å·¥ç¨‹åŒ–å®è·µ**

åœ¨ç¬¬05ç« ä¸­ï¼Œæˆ‘ä»¬å°†å®ç°ï¼š
- âœ¨ **è£…é¥°å™¨æ¨¡å—** (`utils/decorators.py`)ï¼šé”™è¯¯å¤„ç†ã€æ—¥å¿—è®°å½•ã€æ€§èƒ½ç›‘æ§
- ğŸ“„ **æ–‡æ¡£å¤„ç†å™¨** (`utils/document_processor.py`)ï¼šæ”¯æŒPDFã€Wordã€Markdownç­‰å¤šæ ¼å¼è§£æ
- ğŸ§ª **å®Œæ•´æµ‹è¯•**ï¼šæ¯ä¸ªå·¥å…·ç±»çš„ç‹¬ç«‹æµ‹è¯•ç”¨ä¾‹

è¿™äº›å·¥å…·ç±»è™½ç„¶"ä¸èµ·çœ¼"ï¼Œä½†å¯¹ç³»ç»Ÿç¨³å®šæ€§å’Œå¯ç»´æŠ¤æ€§è‡³å…³é‡è¦ã€‚

---

**ç‰ˆæœ¬ä¿¡æ¯**ï¼š
- æ•™ç¨‹ç‰ˆæœ¬ï¼šv1.0
- å¯¹åº”æºç ï¼š`services/vector_store.py`ï¼ˆ476è¡Œï¼‰
- æœ€åæ›´æ–°ï¼š2024-01-15
